{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "from z_sae import ZSAE\n",
    "from mlp_transcoder import SparseTranscoder\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from typing import List, Dict, TypedDict, Any, Union, Tuple, Optional\n",
    "from tqdm import trange\n",
    "from plotly_utils import imshow\n",
    "from pprint import pprint\n",
    "from transformer_lens.utils import get_act_name, to_numpy\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "ATOL = 1e-4\n",
    "\n",
    "\n",
    "class LayerKey(TypedDict):\n",
    "    mlp: int\n",
    "    attn: int\n",
    "\n",
    "\n",
    "NUM_ATTN_AUXILARY_FEATURES = 3\n",
    "NUM_MLP_AUXILARY_FEATURES = 2\n",
    "\n",
    "NUM_AUXILARY_FEATURES = NUM_ATTN_AUXILARY_FEATURES + NUM_MLP_AUXILARY_FEATURES\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ActiveFeatures:\n",
    "    vectors: Float[Tensor, \"comp d_model\"]\n",
    "    values: Float[Tensor, \"comp\"]\n",
    "    features: Int[Tensor, \"comp\"]\n",
    "    keys: List[LayerKey]\n",
    "\n",
    "    def get_total_active_features(self):\n",
    "        total = 0\n",
    "\n",
    "        for key in self.keys:\n",
    "            total += key[\"mlp\"] + key[\"attn\"]\n",
    "\n",
    "        return total\n",
    "\n",
    "    def get_total_auxilary_features(self):\n",
    "        return 2 + (NUM_AUXILARY_FEATURES * len(self.keys))\n",
    "\n",
    "    def get_vectors_before_comp(self, kind: str, layer: int):\n",
    "        max_index = 2  # include embed and pos_embed\n",
    "\n",
    "        for i in range(layer):\n",
    "            max_index += (\n",
    "                self.keys[i][\"mlp\"] + self.keys[i][\"attn\"] + NUM_AUXILARY_FEATURES\n",
    "            )  # include error and bias terms\n",
    "\n",
    "        if kind == \"mlp\":\n",
    "            max_index += self.keys[layer][\"attn\"] + NUM_ATTN_AUXILARY_FEATURES\n",
    "\n",
    "        return self.vectors[:max_index] * self.values[:max_index].unsqueeze(-1)\n",
    "\n",
    "    @property\n",
    "    def max_active_features(self):\n",
    "        lens = []\n",
    "\n",
    "        for key in self.keys:\n",
    "            lens.append(key[\"mlp\"])\n",
    "            lens.append(key[\"attn\"])\n",
    "\n",
    "        return max(lens)\n",
    "\n",
    "    def get_attn_start_index(self, layer: int):\n",
    "        start_i = 2\n",
    "\n",
    "        for i, key in enumerate(self.keys):\n",
    "            if i == layer:\n",
    "                break\n",
    "\n",
    "            start_i += key[\"attn\"] + key[\"mlp\"] + NUM_AUXILARY_FEATURES\n",
    "\n",
    "        return start_i\n",
    "\n",
    "    def get_mlp_start_index(self, layer: int):\n",
    "        start_i = 2\n",
    "\n",
    "        for i, key in enumerate(self.keys):\n",
    "            if i == layer:\n",
    "                break\n",
    "\n",
    "            start_i += key[\"attn\"] + key[\"mlp\"] + NUM_AUXILARY_FEATURES\n",
    "\n",
    "        start_i += self.keys[layer][\"attn\"] + NUM_ATTN_AUXILARY_FEATURES\n",
    "\n",
    "        return start_i\n",
    "\n",
    "    def get_reconstructed_attn_out(self, layer: int):\n",
    "        attn_i = self.get_attn_start_index(layer)\n",
    "        num_comps = self.keys[layer][\"attn\"] + NUM_ATTN_AUXILARY_FEATURES\n",
    "\n",
    "        return self.vectors[attn_i : attn_i + num_comps].sum(dim=0)\n",
    "\n",
    "    def get_sae_out_reconstruction(self, layer: int):\n",
    "        attn_i = self.get_attn_start_index(layer)\n",
    "        attn_i += 2\n",
    "\n",
    "        return self.vectors[attn_i : attn_i + self.keys[layer][\"attn\"] + 1].sum(dim=0)\n",
    "\n",
    "    def get_transcoder_reconstruction(self, layer: int):\n",
    "        mlp_i = self.get_mlp_start_index(layer)\n",
    "        mlp_i += 1\n",
    "\n",
    "        return self.vectors[mlp_i : mlp_i + self.keys[layer][\"mlp\"] + 1].sum(dim=0)\n",
    "\n",
    "    def get_attn_feature_vectors(self, layer: int):\n",
    "        attn_i = self.get_attn_start_index(layer)\n",
    "        attn_i += NUM_ATTN_AUXILARY_FEATURES\n",
    "\n",
    "        return self.vectors[attn_i : attn_i + self.keys[layer][\"attn\"]]\n",
    "\n",
    "    def get_mlp_feature_vectors(self, layer: int):\n",
    "        mlp_i = self.get_mlp_start_index(layer)\n",
    "        mlp_i += NUM_MLP_AUXILARY_FEATURES\n",
    "\n",
    "        return self.vectors[mlp_i : mlp_i + self.keys[layer][\"mlp\"]]\n",
    "\n",
    "    def get_reconstructed_mlp_out(self, layer: int):\n",
    "        mlp_i = self.get_mlp_start_index(layer)\n",
    "\n",
    "        num_comps = self.keys[layer][\"mlp\"] + NUM_MLP_AUXILARY_FEATURES\n",
    "\n",
    "        return self.vectors[mlp_i : mlp_i + num_comps].sum(dim=0)\n",
    "\n",
    "    def get_top_k_features(self, activations: Float[Tensor, \"comp\"], k=10):\n",
    "        values, indices = activations.topk(k=k)\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for v, i in zip(values.tolist(), indices.tolist()):\n",
    "            if i == 0:\n",
    "                features.append((\"embed\", 0, self.features[i], v))\n",
    "                break\n",
    "            elif i == 1:\n",
    "                features.append((\"pos_embed\", 0, self.features[i], v))\n",
    "                break\n",
    "\n",
    "            start_i = 2\n",
    "\n",
    "            for l, key in enumerate(self.keys):\n",
    "                if i == start_i:\n",
    "                    features.append((\"O Bias\", l, self.features[i], v))\n",
    "                    break\n",
    "                start_i += 1\n",
    "\n",
    "                if i == start_i:\n",
    "                    features.append((\"Z SAE Error\", l, self.features[i], v))\n",
    "                    break\n",
    "                start_i += 1\n",
    "\n",
    "                if i == start_i:\n",
    "                    features.append((\"Z SAE Bias\", l, self.features[i], v))\n",
    "                    break\n",
    "                start_i += 1\n",
    "\n",
    "                if i < start_i + key[\"attn\"]:\n",
    "                    features.append((\"attn\", l, self.features[i], v))\n",
    "                    break\n",
    "\n",
    "                start_i += key[\"attn\"]\n",
    "\n",
    "                if i == start_i:\n",
    "                    features.append((\"Transcoder Error\", l, self.features[i], v))\n",
    "                    break\n",
    "                start_i += 1\n",
    "\n",
    "                if i == start_i:\n",
    "                    features.append((\"Transcoder Bias\", l, self.features[i], v))\n",
    "                    break\n",
    "                start_i += 1\n",
    "\n",
    "                if i < start_i + key[\"mlp\"]:\n",
    "                    features.append((\"mlp\", l, self.features[i], v))\n",
    "                    break\n",
    "\n",
    "                start_i += key[\"mlp\"]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_top_k_lens_runs(\n",
    "        self,\n",
    "        activations: Float[Tensor, \"comp\"],\n",
    "        web: \"CircuitLens\",\n",
    "        seq_index: int,\n",
    "        k=10,\n",
    "    ):\n",
    "        features = self.get_top_k_features(activations, k=k)\n",
    "\n",
    "        lens_runs = []\n",
    "\n",
    "        for feature in features:\n",
    "            if feature[0] == \"attn\":\n",
    "                run_type = \"z_feature_head_seq\"\n",
    "            elif feature[0] == \"mlp\":\n",
    "                run_type = \"mlp\"\n",
    "            else:\n",
    "                run_type = feature[0]\n",
    "\n",
    "            lens_runs.append(\n",
    "                ComponentLens(\n",
    "                    web,\n",
    "                    run_data={\n",
    "                        \"run_type\": run_type,\n",
    "                        \"layer\": feature[1],\n",
    "                        \"seq_index\": seq_index,\n",
    "                        \"feature\": feature[2],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return lens_runs\n",
    "\n",
    "    def get_top_k_labels(self, activations: Float[Tensor, \"comp\"], k=10):\n",
    "        features = self.get_top_k_features(activations, k=k)\n",
    "\n",
    "        return [\n",
    "            f\"{kind.capitalize()} {layer} | Feature: {feature} | Value: {value:.3g}\"\n",
    "            for kind, layer, feature, value in features\n",
    "        ]\n",
    "\n",
    "    def reshape_activations_for_visualization(\n",
    "        self, activations: Float[Tensor, \"comp 1\"]\n",
    "    ):\n",
    "        min_val = activations.min()\n",
    "\n",
    "        visualization = torch.ones(\n",
    "            ((12 * 4) + 1, self.max_active_features), device=activations.device\n",
    "        ) * (min_val / 2)\n",
    "\n",
    "        a_len = activations.size(0)\n",
    "\n",
    "        visualization[0, :2] = activations[:2]\n",
    "\n",
    "        start_i = 2\n",
    "\n",
    "        for i, key in enumerate(self.keys):\n",
    "            ii = (4 * i) + 1\n",
    "\n",
    "            if start_i + key[\"attn\"] + NUM_ATTN_AUXILARY_FEATURES > a_len:\n",
    "                break\n",
    "\n",
    "            visualization[ii, :NUM_ATTN_AUXILARY_FEATURES] = activations[\n",
    "                start_i : start_i + NUM_ATTN_AUXILARY_FEATURES\n",
    "            ]\n",
    "            start_i += NUM_ATTN_AUXILARY_FEATURES\n",
    "\n",
    "            visualization[ii + 1, : key[\"attn\"]] = activations[\n",
    "                start_i : start_i + key[\"attn\"]\n",
    "            ]\n",
    "            start_i += key[\"attn\"]\n",
    "\n",
    "            if start_i + key[\"mlp\"] + NUM_MLP_AUXILARY_FEATURES > a_len:\n",
    "                break\n",
    "\n",
    "            visualization[ii + 2, :NUM_MLP_AUXILARY_FEATURES] = activations[\n",
    "                start_i : start_i + NUM_MLP_AUXILARY_FEATURES\n",
    "            ]\n",
    "            start_i += NUM_MLP_AUXILARY_FEATURES\n",
    "\n",
    "            visualization[ii + 3, : key[\"mlp\"]] = activations[\n",
    "                start_i : start_i + key[\"mlp\"]\n",
    "            ]\n",
    "            start_i += key[\"mlp\"]\n",
    "\n",
    "        return visualization\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComponentLens:\n",
    "    web: \"CircuitLens\"\n",
    "    run_data: Dict[str, Any]\n",
    "\n",
    "    @property\n",
    "    def run_type(self):\n",
    "        return self.run_data[\"run_type\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.run_type == \"unembed\":\n",
    "            return f\"Unembed | Token: '{self.web.model.tokenizer.decode([self.run_data['token']])}' :: {self.run_data['token']} | Seq Index: {self.run_data['seq_index']}\"\n",
    "        elif self.run_type == \"z_feature_head_seq\":\n",
    "            return f\"Z Feature Head/Seq | Feature: {self.run_data['feature']} |Layer: {self.run_data['layer']} | Seq Index: {self.run_data['seq_index']}\"\n",
    "        elif self.run_type == \"head\":\n",
    "            return f\"Head | Layer: {self.run_data['layer']} | Head: {self.run_data['head']} | Source: {self.run_data['source_index']} | Destination: {self.run_data['destination_index']}\"\n",
    "        elif self.run_type == \"mlp\":\n",
    "            return f\"MLP Lens | Layer: {self.run_data['layer']} | Seq Index: {self.run_data['seq_index']} | Feature: {self.run_data['feature']}\"\n",
    "        else:\n",
    "            return f\"{self.run_type} | Layer: {self.run_data['layer']} | Seq Index: {self.run_data['seq_index']} | Feature: {self.run_data['feature']}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    def __call__(self, head_type=None, **kwargs):\n",
    "        if self.run_type == \"unembed\":\n",
    "            return self.web.get_unembed_lens(\n",
    "                self.run_data[\"token_id\"], self.run_data[\"seq_index\"] ** kwargs\n",
    "            )\n",
    "        elif self.run_type == \"z_feature_head_seq\":\n",
    "            return self.web.get_head_seq_activations_for_z_feature(\n",
    "                self.run_data[\"layer\"],\n",
    "                self.run_data[\"seq_index\"],\n",
    "                self.run_data[\"feature\"],\n",
    "                **kwargs,\n",
    "            )\n",
    "        elif self.run_type == \"head\":\n",
    "            if head_type is None:\n",
    "                head_type = \"q\"\n",
    "\n",
    "            if head_type == \"q\":\n",
    "                return self.web.get_q_lens_on_head_seq(\n",
    "                    self.run_data[\"layer\"],\n",
    "                    self.run_data[\"head\"],\n",
    "                    self.run_data[\"source_index\"],\n",
    "                    self.run_data[\"destination_index\"],\n",
    "                    **kwargs,\n",
    "                )\n",
    "            elif head_type == \"k\":\n",
    "                return self.web.get_k_lens_on_head_seq(\n",
    "                    self.run_data[\"layer\"],\n",
    "                    self.run_data[\"head\"],\n",
    "                    self.run_data[\"source_index\"],\n",
    "                    self.run_data[\"destination_index\"],\n",
    "                    **kwargs,\n",
    "                )\n",
    "            elif head_type == \"v\":\n",
    "                return self.web.get_v_lens_at_seq(\n",
    "                    self.run_data[\"layer\"],\n",
    "                    self.run_data[\"head\"],\n",
    "                    self.run_data[\"source_index\"],\n",
    "                    self.run_data[\"feature\"],\n",
    "                    **kwargs,\n",
    "                )\n",
    "        elif self.run_type == \"mlp\":\n",
    "            return self.web.get_mlp_feature_lens_at_seq(\n",
    "                self.run_data[\"layer\"],\n",
    "                self.run_data[\"seq_index\"],\n",
    "                self.run_data[\"feature\"],\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "\n",
    "model_encoder_cache: Optional[Tuple[HookedTransformer, Any, Any]] = None\n",
    "\n",
    "\n",
    "def get_model_encoders(device):\n",
    "    global model_encoder_cache\n",
    "\n",
    "    if model_encoder_cache is not None:\n",
    "        return model_encoder_cache\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "    z_saes = [ZSAE.load_zsae_for_layer(i) for i in trange(model.cfg.n_layers)]\n",
    "\n",
    "    mlp_transcoders = [\n",
    "        SparseTranscoder.load_from_hugging_face(i) for i in trange(model.cfg.n_layers)\n",
    "    ]\n",
    "\n",
    "    model_encoder_cache = (model, z_saes, mlp_transcoders)\n",
    "\n",
    "    return model_encoder_cache\n",
    "\n",
    "\n",
    "class CircuitLens:\n",
    "    def __init__(self, prompt):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        (model, z_saes, mlp_transcoders) = get_model_encoders(self.device)\n",
    "\n",
    "        self.z_saes = z_saes\n",
    "        self.mlp_transcoders = mlp_transcoders\n",
    "        self.model: HookedTransformer = model\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.tokens = self.model.to_tokens(prompt)\n",
    "\n",
    "        self.logits, self.cache = self.model.run_with_cache(\n",
    "            self.tokens, return_type=\"logits\"\n",
    "        )\n",
    "\n",
    "        self._seq_activations = {}\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self):\n",
    "        return self.tokens.size(1)\n",
    "\n",
    "    def test_compare_sae_out(self, layer, seq_index):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        layer_z = einops.rearrange(\n",
    "            self.cache[\"z\", layer][0, seq_index],\n",
    "            \"n_heads d_head -> (n_heads d_head)\",\n",
    "        )\n",
    "\n",
    "        test_sae_out = self.get_active_features(\n",
    "            seq_index, cache=False\n",
    "        ).get_sae_out_reconstruction(layer)\n",
    "\n",
    "        _, z_recon, _, _, _ = self.z_saes[layer](layer_z)\n",
    "\n",
    "        z_recon = einops.rearrange(\n",
    "            z_recon,\n",
    "            \"(n_head d_head) -> n_head d_head\",\n",
    "            n_head=self.model.cfg.n_heads,\n",
    "        )\n",
    "        z_recon = einops.einsum(\n",
    "            z_recon,\n",
    "            self.model.W_O[layer],\n",
    "            \"n_head d_head, n_head d_head d_model -> d_model\",\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            torch.allclose(z_recon, test_sae_out, atol=ATOL),\n",
    "            (z_recon - test_sae_out).norm().item(),\n",
    "        )\n",
    "\n",
    "    def test_compare_transcoder_out(self, layer, seq_index):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        test_transcoder_out = self.get_active_features(\n",
    "            seq_index, cache=False\n",
    "        ).get_transcoder_reconstruction(layer)\n",
    "\n",
    "        mlp_input = self.cache[\"normalized\", layer, \"ln2\"]\n",
    "        out = self.mlp_transcoders[layer](mlp_input)[0][0, seq_index]\n",
    "\n",
    "        return (\n",
    "            torch.allclose(out, test_transcoder_out, atol=ATOL),\n",
    "            (out - test_transcoder_out).norm().item(),\n",
    "        )\n",
    "\n",
    "    def test_compare_max_attn_features(self, layer: int, seq_index):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "        active_features = self.get_active_features(seq_index, cache=False)\n",
    "\n",
    "        z_sae = self.z_saes[layer]\n",
    "\n",
    "        max_features = active_features.get_attn_feature_vectors(layer)\n",
    "\n",
    "        layer_z = einops.rearrange(\n",
    "            self.cache[\"z\", layer][0, seq_index],\n",
    "            \"n_heads d_head -> (n_heads d_head)\",\n",
    "        )\n",
    "        _, _, z_acts, _, _ = self.z_saes[layer](layer_z)\n",
    "\n",
    "        z_winner_count = z_acts.nonzero().numel()\n",
    "\n",
    "        z_values, z_max_features = z_acts.topk(k=z_winner_count)\n",
    "\n",
    "        z_contributions = z_sae.W_dec[z_max_features.squeeze(0)] * z_values.squeeze(\n",
    "            0\n",
    "        ).unsqueeze(-1)\n",
    "        z_contributions = einops.rearrange(\n",
    "            z_contributions,\n",
    "            \"winners (n_head d_head) -> winners n_head d_head\",\n",
    "            n_head=self.model.cfg.n_heads,\n",
    "        )\n",
    "        z_residual_vectors = einops.einsum(\n",
    "            z_contributions,\n",
    "            self.model.W_O[layer],\n",
    "            \"winners n_head d_head, n_head d_head d_model -> winners d_model\",\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            torch.allclose(z_residual_vectors, max_features, atol=ATOL),\n",
    "            (z_residual_vectors - max_features).norm().item(),\n",
    "        )\n",
    "\n",
    "    def test_compare_max_mlp_features(self, layer: int, seq_index: int):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        mlp_transcoder = self.mlp_transcoders[layer]\n",
    "        mlp_input = self.cache[\"normalized\", layer, \"ln2\"][:, seq_index]\n",
    "\n",
    "        _, mlp_acts, *_ = mlp_transcoder(mlp_input)\n",
    "\n",
    "        mlp_acts = mlp_acts[0]\n",
    "\n",
    "        mlp_winner_count = mlp_acts.nonzero().numel()\n",
    "\n",
    "        mlp_values, mlp_max_features = mlp_acts.topk(k=mlp_winner_count)\n",
    "\n",
    "        mlp_residual_vectors = mlp_transcoder.W_dec[\n",
    "            mlp_max_features.squeeze(0)\n",
    "        ] * mlp_values.squeeze(0).unsqueeze(-1)\n",
    "\n",
    "        test_vectors = self.get_active_features(\n",
    "            seq_index, cache=False\n",
    "        ).get_mlp_feature_vectors(layer)\n",
    "\n",
    "        return (\n",
    "            torch.allclose(mlp_residual_vectors, test_vectors, atol=ATOL),\n",
    "            (mlp_residual_vectors - test_vectors).norm().item(),\n",
    "        )\n",
    "\n",
    "    def test_compare_attn_out(self, layer: int, seq_index: int):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "        active_features = self.get_active_features(seq_index, cache=False)\n",
    "        feature_recon = active_features.get_reconstructed_attn_out(layer)\n",
    "\n",
    "        attn_out = self.cache[\"attn_out\", layer][0, seq_index]\n",
    "\n",
    "        return (\n",
    "            torch.allclose(feature_recon, attn_out, atol=ATOL),\n",
    "            (feature_recon - attn_out).norm().item(),\n",
    "        )\n",
    "\n",
    "    def test_compare_mlp_out(self, layer: int, seq_index: int):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        active_features = self.get_active_features(seq_index, cache=False)\n",
    "        feature_recon = active_features.get_reconstructed_mlp_out(layer)\n",
    "\n",
    "        mlp_out = self.cache[\"mlp_out\", layer][0, seq_index]\n",
    "\n",
    "        return (\n",
    "            torch.allclose(feature_recon, mlp_out, atol=ATOL),\n",
    "            (feature_recon - mlp_out).norm(),\n",
    "        )\n",
    "\n",
    "    def test_compare_final_resid_post(self, seq_index: int):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        cumulative_sum = self.get_active_features(seq_index, cache=False).vectors.sum(\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        resid_post = self.cache[\"resid_post\", self.model.cfg.n_layers - 1][0, seq_index]\n",
    "\n",
    "        return (\n",
    "            torch.allclose(cumulative_sum, resid_post, atol=ATOL),\n",
    "            (cumulative_sum - resid_post).norm(),\n",
    "        )\n",
    "\n",
    "    def get_active_features(self, seq_index: int, cache=True) -> ActiveFeatures:\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        act = self._seq_activations.get(seq_index, None)\n",
    "\n",
    "        if cache and act is not None:\n",
    "            return act\n",
    "\n",
    "        component_keys: List[LayerKey] = []\n",
    "\n",
    "        t1 = torch.tensor(1.0).to(self.cache[\"z\", 0].device)\n",
    "        t0 = torch.tensor(-1).to(self.cache[\"z\", 0].device).int()\n",
    "\n",
    "        # Start with embed and pos embed\n",
    "        vectors = [\n",
    "            torch.stack(\n",
    "                [\n",
    "                    self.cache[\"embed\"][0, seq_index],\n",
    "                    self.cache[\"pos_embed\"][0, seq_index],\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        values = [torch.stack([t1, t1])]\n",
    "        features = [torch.stack([t0, t0])]\n",
    "\n",
    "        for layer in trange(self.model.cfg.n_layers):\n",
    "            # First handle attention\n",
    "            z_sae = self.z_saes[layer]\n",
    "\n",
    "            layer_z = einops.rearrange(\n",
    "                self.cache[\"z\", layer][0, seq_index],\n",
    "                \"n_heads d_head -> (n_heads d_head)\",\n",
    "            )\n",
    "            _, z_recon, z_acts, _, _ = self.z_saes[layer](layer_z)\n",
    "\n",
    "            z_error = layer_z - z_recon\n",
    "            z_bias = self.z_saes[layer].b_dec\n",
    "\n",
    "            z_error_bias = torch.stack([z_error, z_bias])\n",
    "            z_error_bias = einops.rearrange(\n",
    "                z_error_bias,\n",
    "                \"v (n_head d_head) -> v n_head d_head\",\n",
    "                n_head=self.model.cfg.n_heads,\n",
    "            )\n",
    "            z_error_bias = einops.einsum(\n",
    "                z_error_bias,\n",
    "                self.model.W_O[layer],\n",
    "                \"v n_head d_head, n_head d_head d_model -> v d_model\",\n",
    "            )\n",
    "\n",
    "            vectors.append(self.model.b_O[layer].unsqueeze(0))\n",
    "\n",
    "            vectors.append(z_error_bias)\n",
    "            values.append(torch.stack([t1, t1, t1]))\n",
    "            features.append(torch.stack([t0, t0, t0]))\n",
    "\n",
    "            z_winner_count = z_acts.nonzero().numel()\n",
    "\n",
    "            z_values, z_max_features = z_acts.topk(k=z_winner_count)\n",
    "\n",
    "            z_contributions = z_sae.W_dec[z_max_features.squeeze(0)] * z_values.squeeze(\n",
    "                0\n",
    "            ).unsqueeze(-1)\n",
    "            z_contributions = einops.rearrange(\n",
    "                z_contributions,\n",
    "                \"winners (n_head d_head) -> winners n_head d_head\",\n",
    "                n_head=self.model.cfg.n_heads,\n",
    "            )\n",
    "            z_residual_vectors = einops.einsum(\n",
    "                z_contributions,\n",
    "                self.model.W_O[layer],\n",
    "                \"winners n_head d_head, n_head d_head d_model -> winners d_model\",\n",
    "            )\n",
    "\n",
    "            vectors.append(z_residual_vectors)\n",
    "            values.append(z_values)\n",
    "            features.append(z_max_features)\n",
    "\n",
    "            # Now handle the transcoder\n",
    "            mlp_transcoder = self.mlp_transcoders[layer]\n",
    "            mlp_input = self.cache[\"normalized\", layer, \"ln2\"][:, seq_index]\n",
    "            mlp_output = self.cache[\"mlp_out\", layer][:, seq_index]\n",
    "\n",
    "            mlp_recon, mlp_acts, *_ = mlp_transcoder(mlp_input)\n",
    "\n",
    "            mlp_error = mlp_output - mlp_recon\n",
    "            mlp_bias = mlp_transcoder.b_dec_out\n",
    "\n",
    "            vectors.append(torch.stack([mlp_error[0], mlp_bias]))\n",
    "            values.append(torch.stack([t1, t1]))\n",
    "            features.append(torch.stack([t0, t0]))\n",
    "\n",
    "            mlp_winner_count = mlp_acts.nonzero().numel()\n",
    "\n",
    "            mlp_values, mlp_max_features = mlp_acts.topk(k=mlp_winner_count)\n",
    "\n",
    "            mlp_residual_vectors = mlp_transcoder.W_dec[\n",
    "                mlp_max_features[0]\n",
    "            ] * mlp_values[0].unsqueeze(-1)\n",
    "\n",
    "            vectors.append(mlp_residual_vectors)\n",
    "            values.append(mlp_values.squeeze())\n",
    "            features.append(mlp_max_features.squeeze())\n",
    "\n",
    "            component_keys.append({\"mlp\": mlp_winner_count, \"attn\": z_winner_count})\n",
    "\n",
    "        component_vectors = torch.cat(vectors, dim=0)\n",
    "        component_values = torch.cat(values, dim=0)\n",
    "        component_features = torch.cat(features, dim=0)\n",
    "\n",
    "        self._seq_activations[seq_index] = ActiveFeatures(\n",
    "            vectors=component_vectors,\n",
    "            values=component_values,\n",
    "            features=component_features,\n",
    "            keys=component_keys,\n",
    "        )\n",
    "\n",
    "        return self._seq_activations[seq_index]\n",
    "\n",
    "    def visualize_values(self, seq_index: int):\n",
    "        active_features = self.get_active_features(seq_index)\n",
    "\n",
    "        vis = active_features.reshape_activations_for_visualization(\n",
    "            active_features.values\n",
    "        )\n",
    "\n",
    "        imshow(\n",
    "            vis[:, :20],\n",
    "            title=f\"Values for Seq Index {seq_index}\",\n",
    "            y=self.get_imshow_labels()[: vis.size(0)],\n",
    "            height=800,\n",
    "            width=600,\n",
    "        )\n",
    "\n",
    "    def get_next_lens_runs(\n",
    "        self,\n",
    "        active_features,\n",
    "        activations,\n",
    "        seq_index: int,\n",
    "        title: str,\n",
    "        visualize=True,\n",
    "        k=None,\n",
    "    ):\n",
    "        if k is None:\n",
    "            k = 10\n",
    "\n",
    "        if visualize:\n",
    "            vis = active_features.reshape_activations_for_visualization(activations)\n",
    "\n",
    "            imshow(\n",
    "                vis[:, :20],\n",
    "                title=title,\n",
    "                y=self.get_imshow_labels()[: vis.size(0)],\n",
    "                height=800,\n",
    "                width=600,\n",
    "            )\n",
    "\n",
    "            pprint(active_features.get_top_k_labels(activations, k=k))\n",
    "\n",
    "        return active_features.get_top_k_lens_runs(activations, self, seq_index, k=k)\n",
    "\n",
    "    def get_unembed_lens_for_prompt_token(self, seq_index: int, visualize=True, k=None):\n",
    "        \"\"\"\n",
    "        Here `seq_index` refers to the seq position where the next token will be predicted\n",
    "        \"\"\"\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        token_i = self.tokens[0, seq_index + 1].item()\n",
    "\n",
    "        return self.get_unembed_lens(token_i, seq_index, visualize, k)\n",
    "\n",
    "    _labels = None\n",
    "\n",
    "    def get_imshow_labels(self):\n",
    "        if self._labels is not None:\n",
    "            return self._labels\n",
    "\n",
    "        labels = [\"Embed\"]\n",
    "        for i in range(12):\n",
    "            labels.append(f\"Attn {i} Error/Bias\")\n",
    "            labels.append(f\"Attn {i}\")\n",
    "            labels.append(f\"Mlp {i} Error/Bias\")\n",
    "            labels.append(f\"Mlp {i}\")\n",
    "        self._labels = labels\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def process_seq_index(self, seq_index):\n",
    "        if seq_index < 0:\n",
    "            seq_index += self.n_tokens\n",
    "        return seq_index\n",
    "\n",
    "    def get_unembed_lens(\n",
    "        self, token_i: Union[int, str], seq_index: int, visualize=True, k=None\n",
    "    ):\n",
    "        if isinstance(token_i, str):\n",
    "            token_i = self.model.to_tokens(token_i, prepend_bos=False)[0]\n",
    "\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "        active_features = self.get_active_features(seq_index)\n",
    "\n",
    "        activations = (\n",
    "            einops.einsum(\n",
    "                active_features.vectors,\n",
    "                self.model.W_U[:, token_i],\n",
    "                \"comp d_model, d_model -> comp\",\n",
    "            )\n",
    "            / self.cache[\"ln_final.hook_scale\"][0, seq_index]\n",
    "        )\n",
    "\n",
    "        activations /= self.logits[0, seq_index][token_i]\n",
    "\n",
    "        return self.get_next_lens_runs(\n",
    "            active_features=active_features,\n",
    "            activations=activations,\n",
    "            title=f\"Unembed Lens for token '{self.model.tokenizer.decode([token_i])}' at '{self.model.tokenizer.decode([token_i])}\",\n",
    "            seq_index=seq_index,\n",
    "            visualize=visualize,\n",
    "            k=k,\n",
    "        )\n",
    "    \n",
    "\n",
    "    def get_head_seq_activations_for_z_feature(\n",
    "        self, layer: int, seq_index: int, feature: int, visualize=True, k=10\n",
    "    ):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "        v = self.cache[\"v\", layer]\n",
    "        pattern = self.cache[\"pattern\", 9]\n",
    "        encoder = self.z_saes[layer]\n",
    "\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        pre_z = einops.einsum(\n",
    "            v,\n",
    "            pattern,\n",
    "            \"b p_seq n_head d_head, b n_head seq p_seq -> seq b p_seq n_head d_head\",\n",
    "        )[seq_index, 0]\n",
    "\n",
    "        better_w_enc = einops.rearrange(\n",
    "            encoder.W_enc, \"(n_head d_head) feature -> n_head d_head feature\", n_head=12\n",
    "        )[:, :, feature]\n",
    "\n",
    "        feature_act = einops.einsum(\n",
    "            pre_z, better_w_enc, \"seq n_head d_head, n_head d_head -> n_head seq\"\n",
    "        )\n",
    "        feature_act = einops.rearrange(feature_act, \"n_head seq -> (n_head seq)\")\n",
    "\n",
    "        values, indices = feature_act.topk(k=k)\n",
    "\n",
    "        lens_runs = []\n",
    "        vis_list = []\n",
    "\n",
    "        for index, value in zip(indices, values):\n",
    "            head = index // self.n_tokens\n",
    "            source = index % self.n_tokens\n",
    "\n",
    "            lens_runs.append(\n",
    "                ComponentLens(\n",
    "                    web=self,\n",
    "                    run_data={\n",
    "                        \"run_type\": \"head\",\n",
    "                        \"layer\": layer,\n",
    "                        \"head\": head,\n",
    "                        \"source_index\": source,\n",
    "                        \"feature\": feature,\n",
    "                        \"destination_index\": seq_index,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if visualize:\n",
    "                vis_list.append((head, source, seq_index, value))\n",
    "\n",
    "        if visualize:\n",
    "            vis = einops.rearrange(\n",
    "                feature_act, \"(n_head seq) -> n_head seq\", n_head=self.model.cfg.n_heads\n",
    "            )\n",
    "\n",
    "            imshow(\n",
    "                vis,\n",
    "                x=[\n",
    "                    f\"{token}/{i}\"\n",
    "                    for (i, token) in enumerate(self.model.to_str_tokens(self.prompt))\n",
    "                ],\n",
    "                title=f\"Layer {layer} Head/Seq Feature {feature} at '{self.get_str_token_at_seq(seq_index)}'::{seq_index}\",\n",
    "                labels={\"x\": \"Token\", \"y\": \"Head\"},\n",
    "            )\n",
    "\n",
    "            pprint(\n",
    "                [\n",
    "                    f\"Head {head} \"\n",
    "                    + f\"| Source: {self.model.tokenizer.decode([self.tokens[0, source]])}::{source} \"\n",
    "                    + f\"| Destination: {self.model.tokenizer.decode([self.tokens[0, dest]])}::{dest} \"\n",
    "                    + f\"| Value: {value:.3g}\"\n",
    "                    for (head, source, dest, value) in vis_list\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return lens_runs\n",
    "\n",
    "    def get_mlp_feature_lens_at_seq(\n",
    "        self,\n",
    "        layer: int,\n",
    "        seq_index: int,\n",
    "        feature: int,\n",
    "        visualize=True,\n",
    "        k=None,\n",
    "    ):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "\n",
    "        active_features = self.get_active_features(seq_index)\n",
    "        transcoder = self.mlp_transcoders[layer]\n",
    "\n",
    "        vectors = active_features.get_vectors_before_comp(\"mlp\", layer)\n",
    "\n",
    "        activation = einops.einsum(\n",
    "            vectors, transcoder.W_enc[:, feature], \"comp d_model, d_model -> comp\"\n",
    "        )\n",
    "\n",
    "        return self.get_next_lens_runs(\n",
    "            active_features,\n",
    "            activation,\n",
    "            seq_index,\n",
    "            title=f\"MLP Lens for Feature {feature} at '{self.get_str_token_at_seq(seq_index)}' :: {seq_index}\",\n",
    "            visualize=visualize,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "    def get_str_token_at_seq(self, seq_index: int):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "        return self.model.tokenizer.decode([self.tokens[0, seq_index]])\n",
    "\n",
    "    def get_v_lens_at_seq(\n",
    "        self,\n",
    "        layer: int,\n",
    "        head: int,\n",
    "        seq_index: int,\n",
    "        feature: int,\n",
    "        visualize=True,\n",
    "        k=None,\n",
    "    ):\n",
    "        seq_index = self.process_seq_index(seq_index)\n",
    "        active_features = self.get_active_features(seq_index)\n",
    "        z_sae = self.z_saes[layer]\n",
    "\n",
    "        vectors = active_features.get_vectors_before_comp(\"attn\", layer)\n",
    "\n",
    "        effective_v = einops.einsum(\n",
    "            vectors,\n",
    "            self.model.W_V[layer, head],\n",
    "            \"comp d_model, d_model d_head -> comp d_head\",\n",
    "        )\n",
    "\n",
    "        effective_feature = einops.rearrange(\n",
    "            z_sae.W_enc[:, feature],\n",
    "            \"(n_head d_head) -> n_head d_head\",\n",
    "            n_head=self.model.cfg.n_heads,\n",
    "        )[head]\n",
    "\n",
    "        activation = einops.einsum(\n",
    "            effective_v, effective_feature, \"comp d_head, d_head -> comp\"\n",
    "        )\n",
    "\n",
    "        return self.get_next_lens_runs(\n",
    "            active_features,\n",
    "            activation,\n",
    "            seq_index,\n",
    "            title=f\"V Lens | Layer {layer} | Head {head} | Feature {feature} at  '{self.get_str_token_at_seq(seq_index)}'::{seq_index}\",\n",
    "            visualize=visualize,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "    def get_q_lens_on_head_seq(\n",
    "        self,\n",
    "        layer: int,\n",
    "        head: int,\n",
    "        source_index,\n",
    "        destination_index,\n",
    "        visualize=True,\n",
    "        k=None,\n",
    "    ):\n",
    "        source_index = self.process_seq_index(source_index)\n",
    "        destination_index = self.process_seq_index(destination_index)\n",
    "\n",
    "        active_features = self.get_active_features(destination_index)\n",
    "\n",
    "        vectors = active_features.get_vectors_before_comp(\"attn\", layer)\n",
    "\n",
    "        effective_q = einops.einsum(\n",
    "            vectors,\n",
    "            self.model.W_Q[layer, head],\n",
    "            \"comp d_model, d_model d_head -> comp d_head\",\n",
    "        )\n",
    "\n",
    "        effective_k = self.cache[\"k\", layer][0, source_index, head]\n",
    "\n",
    "        bias_q = self.model.b_Q[layer, head]\n",
    "        real_q = self.cache[\"q\", layer][0, destination_index, head]\n",
    "        bias_contrib = einops.einsum(bias_q, effective_k, \"d_head, d_head -> \")\n",
    "        qk = einops.einsum(real_q, effective_k, \"d_head, d_head -> \")\n",
    "        ln_scale = self.cache[\"scale\", layer, \"ln1\"][0, destination_index]\n",
    "\n",
    "        print(\"qk \", qk, \"bias\", bias_contrib, \"ln-scale\", ln_scale)\n",
    "\n",
    "        activation = einops.einsum(\n",
    "            effective_q, effective_k, \"comp d_head, d_head -> comp\"\n",
    "        ) / (ln_scale * (qk - bias_contrib))\n",
    "\n",
    "        return self.get_next_lens_runs(\n",
    "            active_features,\n",
    "            activation,\n",
    "            destination_index,\n",
    "            title=f\"Q Lens | Layer {layer} | Head {head} | '{self.get_str_token_at_seq(source_index)}'::{source_index} -> '{self.get_str_token_at_seq(destination_index)}'::{destination_index}\",\n",
    "            visualize=visualize,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "    def get_k_lens_on_head_seq(\n",
    "        self,\n",
    "        layer: int,\n",
    "        head: int,\n",
    "        source_index,\n",
    "        destination_index,\n",
    "        visualize=True,\n",
    "        k=None,\n",
    "    ):\n",
    "        active_features = self.get_active_features(source_index)\n",
    "\n",
    "        vectors = active_features.get_vectors_before_comp(\"attn\", layer)\n",
    "\n",
    "        effective_k = einops.einsum(\n",
    "            vectors,\n",
    "            self.model.W_K[layer, head],\n",
    "            \"comp d_model, d_model d_head -> comp d_head\",\n",
    "        )\n",
    "\n",
    "        effective_q = self.cache[\"q\", layer][0, destination_index, head]\n",
    "\n",
    "        bias_k = self.model.b_K[layer, head]\n",
    "        real_k = self.cache[\"k\", layer][0, source_index, head]\n",
    "\n",
    "        bias_contrib = einops.einsum(bias_k, effective_q, \"d_head, d_head -> \")\n",
    "        qk = einops.einsum(real_k, effective_q, \"d_head, d_head -> \")\n",
    "        ln_scale = self.cache[\"scale\", layer, \"ln1\"][0, source_index]\n",
    "\n",
    "        activation = einops.einsum(\n",
    "            effective_k, effective_q, \"comp d_head, d_head -> comp\"\n",
    "        ) / (ln_scale * (qk - bias_contrib))\n",
    "\n",
    "        return self.get_next_lens_runs(\n",
    "            active_features,\n",
    "            activation,\n",
    "            destination_index,\n",
    "            title=f\"K Lens | Layer {layer} | Head {head} | '{self.get_str_token_at_seq(source_index)}'::{source_index} -> '{self.get_str_token_at_seq(destination_index)}'::{destination_index}\",\n",
    "            visualize=visualize,\n",
    "            k=k,\n",
    "        )\n",
    "    \n",
    "    def get_transcoder_ixg(self, transcoder, layer, seq_index, feature_vector, is_transcoder_post_ln=True, return_feature_activs=True):\n",
    "        \"\"\"\n",
    "        Pull back the contributions from the transcoder's output to the inputs.\n",
    "        \"\"\"\n",
    "        # Perform the matrix multiplication with the decoder weights\n",
    "        pulledback_feature = transcoder.W_dec @ feature_vector\n",
    "        \n",
    "        # Determine the correct activation name based on whether layer normalization is applied\n",
    "        act_name = (\"normalized\", layer, \"ln2\") if is_transcoder_post_ln else (\"resid_mid\", layer)\n",
    "        \n",
    "        # Retrieve the activations from the cache\n",
    "        feature_activs = transcoder(self.cache[act_name])[1][0, seq_index]\n",
    "        \n",
    "        # Multiply pulledback_feature by the feature activations\n",
    "        pulledback_feature *= feature_activs\n",
    "        \n",
    "        # Return the pulledback_feature and feature_activs\n",
    "        if not return_feature_activs:\n",
    "            return pulledback_feature\n",
    "        else:\n",
    "            return pulledback_feature, feature_activs\n",
    "        \n",
    "\n",
    "    def get_ln_constant(self, vector, layer, token, is_ln2=False, recip=False):\n",
    "        x_act_name = (\"resid_mid\", layer) if is_ln2 else (\"resid_pre\", layer)\n",
    "        y_act_name = (\"normalized\", layer, \"ln2\") if is_ln2 else (\"normalized\", layer, \"ln1\")\n",
    "        \n",
    "        x = self.cache[x_act_name][0, token]\n",
    "        y = self.cache[y_act_name][0, token]\n",
    "        \n",
    "        if torch.dot(vector, x) == 0:\n",
    "            return torch.tensor(0.0, device=vector.device)\n",
    "        return torch.dot(vector, y) / torch.dot(vector, x) if not recip else torch.dot(vector, x) / torch.dot(vector, y)\n",
    "\n",
    "    \n",
    "    def get_attn_head_contribs(self, layer: int, seq_index: int, feature_vector: torch.Tensor):\n",
    "        z_sae = self.z_saes[layer]\n",
    "        layer_z = einops.rearrange(\n",
    "            self.cache[\"z\", layer][0, seq_index],\n",
    "            \"n_heads d_head -> (n_heads d_head)\"\n",
    "        )\n",
    "        \n",
    "        # Get z_acts similar to how it's done in your current code\n",
    "        _, z_recon, z_acts, _, _ = z_sae(layer_z)\n",
    "        \n",
    "        # Compute z_error and z_bias\n",
    "        z_error = layer_z - z_recon\n",
    "        z_bias = z_sae.b_dec\n",
    "\n",
    "        # Stack z_error and z_bias, then rearrange\n",
    "        z_error_bias = torch.stack([z_error, z_bias])\n",
    "        z_error_bias = einops.rearrange(\n",
    "            z_error_bias,\n",
    "            \"v (n_head d_head) -> v n_head d_head\",\n",
    "            n_head=self.model.cfg.n_heads,\n",
    "        )\n",
    "        z_error_bias = einops.einsum(\n",
    "            z_error_bias,\n",
    "            self.model.W_O[layer],\n",
    "            \"v n_head d_head, n_head d_head d_model -> v d_model\",\n",
    "        )\n",
    "        \n",
    "        z_winner_count = z_acts.nonzero().numel()\n",
    "        z_values, z_max_features = z_acts.topk(k=z_winner_count)\n",
    "        \n",
    "        z_contributions = z_sae.W_dec[z_max_features.squeeze(0)] * z_values.squeeze(0).unsqueeze(-1)\n",
    "        z_contributions = einops.rearrange(\n",
    "            z_contributions,\n",
    "            \"winners (n_head d_head) -> winners n_head d_head\",\n",
    "            n_head=self.model.cfg.n_heads,\n",
    "        )\n",
    "        z_residual_vectors = einops.einsum(\n",
    "            z_contributions,\n",
    "            self.model.W_O[layer],\n",
    "            \"winners n_head d_head, n_head d_head d_model -> winners d_model\",\n",
    "        )\n",
    "        return z_residual_vectors\n",
    "\n",
    "    def get_transcoder_contribs(self, layer: int, seq_index: int, feature_vector: torch.Tensor, k=5):\n",
    "        transcoder = self.mlp_transcoders[layer]\n",
    "        is_transcoder_post_ln = 'ln2' in transcoder.cfg.hook_point\n",
    "        act_name = (\"normalized\", layer, \"ln2\") if is_transcoder_post_ln else (\"resid_mid\", layer)\n",
    "        \n",
    "        transcoder_out = transcoder(self.cache[act_name])[0][0, seq_index]\n",
    "        mlp_out = self.model.blocks[layer].mlp(self.cache[act_name])[0, seq_index]\n",
    "        \n",
    "        # Reshape feature_vector to match mlp_out and transcoder_out for dot product\n",
    "        if feature_vector.dim() == 2:  # When feature_vector is 2D (e.g., attention heads)\n",
    "            feature_vector = feature_vector.view(-1)\n",
    "        \n",
    "        error = torch.dot(feature_vector, mlp_out - transcoder_out) / torch.dot(feature_vector, mlp_out)\n",
    "        \n",
    "        pulledback_feature, feature_activs = self.get_transcoder_ixg(transcoder, layer, seq_index, feature_vector)\n",
    "        top_contribs, top_indices = torch.topk(pulledback_feature, k=k)\n",
    "\n",
    "        top_contribs_list = []\n",
    "        for contrib, index in zip(top_contribs, top_indices):\n",
    "            vector = transcoder.W_enc[:, index]\n",
    "            vector = vector * (transcoder.W_dec @ feature_vector)[index]\n",
    "            if is_transcoder_post_ln:\n",
    "                vector *= self.get_ln_constant(vector, layer, seq_index)\n",
    "\n",
    "            top_contribs_list.append((vector, layer, seq_index, index.item(), contrib.item()))\n",
    "        return top_contribs_list\n",
    "\n",
    "    def get_top_contribs(self, feature_vector: torch.Tensor, layer: int, seq_index: int, k=5):\n",
    "        all_mlp_contribs = []\n",
    "        for cur_layer in range(layer + 1):\n",
    "            all_mlp_contribs += self.get_transcoder_contribs(cur_layer, seq_index, feature_vector, k=k)\n",
    "\n",
    "        all_attn_contribs = []\n",
    "        for cur_layer in range(layer + 1):\n",
    "            attn_contribs = self.get_attn_head_contribs(cur_layer, seq_index, feature_vector)\n",
    "            top_attn_contribs_flattened, top_attn_contrib_indices_flattened = torch.topk(attn_contribs.flatten(), k=min(k, len(attn_contribs)))\n",
    "            top_attn_contrib_indices = torch.unravel_index(top_attn_contrib_indices_flattened, attn_contribs.shape)\n",
    "\n",
    "            print(f\"Top attn contribs flattened: {top_attn_contribs_flattened}\")\n",
    "            print(f\"Top attn contrib indices: {top_attn_contrib_indices}\")\n",
    "\n",
    "            for contrib, (winner, head, src_token) in zip(top_attn_contribs_flattened, zip(*top_attn_contrib_indices)):\n",
    "                vector = self.model.OV[cur_layer, head] @ feature_vector\n",
    "                attn_pattern = self.cache[\"pattern\", cur_layer]\n",
    "                vector *= attn_pattern[0, head, seq_index, src_token]\n",
    "                vector *= self.get_ln_constant(vector, cur_layer, src_token)\n",
    "\n",
    "                all_attn_contribs.append((vector, cur_layer, src_token, head, contrib.item()))\n",
    "\n",
    "        all_contribs = all_mlp_contribs + all_attn_contribs\n",
    "        all_contrib_scores = torch.tensor([x[4] for x in all_contribs])\n",
    "        _, top_contrib_indices = torch.topk(all_contrib_scores, k=min(k, len(all_contrib_scores)))\n",
    "        return [all_contribs[i.item()] for i in top_contrib_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:09<00:00,  1.32it/s]\n",
      "100%|| 12/12 [00:05<00:00,  2.07it/s]\n",
      "100%|| 12/12 [00:01<00:00,  9.08it/s]\n"
     ]
    }
   ],
   "source": [
    "circuit_lens = CircuitLens(\"Mary and Jeff went to the store, and Mary gave an apple to Jeff\")\n",
    "unembed_children = circuit_lens.get_unembed_lens_for_prompt_token(-2, visualize=False)\n",
    "top_feature = unembed_children[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Z Feature Head/Seq | Feature: 15647 |Layer: 9 | Seq Index: 14"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the feature vector\n",
    "def get_feature_vector(circuit_lens, top_feature):\n",
    "    layer = top_feature.run_data[\"layer\"]\n",
    "    feature_index = top_feature.run_data[\"feature\"]\n",
    "    seq_index = top_feature.run_data[\"seq_index\"]\n",
    "    run_type = top_feature.run_data[\"run_type\"]\n",
    "\n",
    "    if run_type == \"mlp\":\n",
    "        feature_vector = circuit_lens.mlp_transcoders[layer].W_enc[:, feature_index]\n",
    "    elif run_type == \"z_feature_head_seq\":\n",
    "        head = top_feature.run_data.get(\"head\")\n",
    "        feature_vector = circuit_lens.z_saes[layer].W_enc[:, feature_index]\n",
    "        feature_vector = feature_vector.view(circuit_lens.model.cfg.n_heads, -1)[head]\n",
    "    elif run_type == \"unembed\":\n",
    "        # For unembed, get the W_U column corresponding to the feature index (token ID)\n",
    "        feature_vector = circuit_lens.model.W_U[:, feature_index]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported run type: {run_type}\")\n",
    "\n",
    "    return feature_vector.squeeze()\n",
    "\n",
    "# Get the correct feature vector\n",
    "feature_vector = get_feature_vector(circuit_lens, top_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 64])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top attn contribs flattened: tensor([3.5339, 1.4171, 1.2563, 1.2059, 1.0530], grad_fn=<TopkBackward0>)\n",
      "Top attn contrib indices: (tensor([0, 0, 0, 0, 1]), tensor([480, 745, 393, 640, 480]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now call get_top_contribs with the feature vector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m top_contribs \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit_lens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_contribs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_feature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_feature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the top contributors to the feature\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n",
      "Cell \u001b[0;32mIn[117], line 1118\u001b[0m, in \u001b[0;36mCircuitLens.get_top_contribs\u001b[0;34m(self, feature_vector, layer, seq_index, k)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop attn contribs flattened: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_attn_contribs_flattened\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop attn contrib indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_attn_contrib_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m contrib, (winner, head, src_token) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(top_attn_contribs_flattened, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtop_attn_contrib_indices)):\n\u001b[1;32m   1119\u001b[0m     vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mOV[cur_layer, head] \u001b[38;5;241m@\u001b[39m feature_vector\n\u001b[1;32m   1120\u001b[0m     attn_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m, cur_layer]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Now call get_top_contribs with the feature vector\n",
    "top_contribs = circuit_lens.get_top_contribs(feature_vector, top_feature.run_data[\"layer\"], top_feature.run_data[\"seq_index\"], k=10)\n",
    "\n",
    "# Print the top contributors to the feature\n",
    "from pprint import pprint\n",
    "pprint(top_contribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
