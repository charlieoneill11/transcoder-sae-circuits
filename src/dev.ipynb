{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit autointerpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stuff just sets up everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint=config[\"base_url\"],\n",
    "    api_key=config[\"azure_api_key\"],\n",
    "    api_version=config[\"api_version\"],\n",
    ")\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800 * 2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you can specify the features you want to examine, in each layer, and just pass in either the relevant ZSAE or MLP transcoder depending on what component you want to look at. The `get_feature_scores` function will handle the differences. Let's have a look at the max-activating examples on Danny's features he wanted to check out (note you can slice `owt_tokens_torch` to run for shorter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [16513, 7861]\n",
    "sae = z_saes[8]\n",
    "feature_scores = get_feature_scores(model, sae, owt_tokens_torch, features, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature scores are a tensor of shape `(batch, feature, seq_pos)`, and so I've got a function to help extract the max-activating examples for each feature. You need to specify the feature index, which is why it's helpful to know from above the features in your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 0 # corresponding to 16513\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, feature_idx, :], owt_tokens_torch, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens, top_logits = get_top_k_tokens(model, sae, features[feature_idx], k=20, act_strength=3)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing\n",
    "feature_idx = 1\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, feature_idx, :], owt_tokens_torch, k=15, display_html=False)\n",
    "top_tokens, top_logits = get_top_k_tokens(model, sae, features[feature_idx], k=20, act_strength=5)\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)\n",
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in and boost logits for multiple features at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens, top_logits = get_top_k_tokens(model, sae, features, k=10, act_strength=5)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)\n",
    "\n",
    "\n",
    "examples_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens, [0, 1], k=5, show_score=True)\n",
    "\n",
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can just pass it off to GPT-4 to interpret what's going on. Note that I haven't got access to `GPT-4o` with my credits yet, so this will have to wait a few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pass in multiple features at once to see the max activating examples for features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens, [0, 1], k=5, show_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, instead of passing in individual features for specific components in specific layers, I created an object called `CircuitPrediction` to basically store all this stuff for you. I'll quickly illustrate how to use it in conjunction with the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = get_circuit_prediction(task='ioi', N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing you'll want to do with this is get features from certain components to look at on a specific task. The features for each component are stored in the circuit hypergraph. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.circuit_hypergraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to look at MLP 3, all you have to do is access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.circuit_hypergraph['MLP3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just repeat what we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(cp.circuit_hypergraph['MLP3']['features']))\n",
    "transcoder = transcoders[3]\n",
    "feature_scores = get_feature_scores(model, transcoder, owt_tokens_torch, features, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 0 # corresponding to 16513\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, 0, :], owt_tokens_torch, k=5, show_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few other methods, but you probably don't need to bother with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cp.unique_feature_array(visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "What do we want to actually look for?\n",
    "* We could take specific components, and look at all their features across the circuit hypergraph, then get some sort of \"mass autointerpretation\" of what this feature is doing. I think for this you'd need to also feed in information from where it activates on the actual circuit. Might seem a bit soft and qualitative, but if you do it principled enough, it could be useful. Also try weighting the cluster max-act examples + logits by how often the feature shows up.\n",
    "* Look at what features co-occur together in examples. Should give more signal than just looking at features that activate heaps. (Also look at features that activate strongly across all examples as well though.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature cluster interpretation of model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi', N=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi', N=20)\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint=config[\"base_url\"],\n",
    "    api_key=config[\"azure_api_key\"],\n",
    "    api_version=config[\"api_version\"],\n",
    ")\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800 * 2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.co_occurrence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_cooccurrences(\"MLP0\", \"L9H9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_cooccurrences(\"MLP0\", \"L9H6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.visualize_co_occurrences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_top_k_feature_tuples(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_top_k_feature_tuples_for_component(co_occurrence_dict, component_str, k=5):\n",
    "    # Parse the component string to get the appropriate tuple key\n",
    "    if component_str.startswith(\"MLP\"):\n",
    "        layer = int(component_str[3:])\n",
    "        component = ('mlp_feature', layer)\n",
    "    elif component_str.startswith(\"L\") and \"H\" in component_str:\n",
    "        layer, head = map(int, component_str[1:].split(\"H\"))\n",
    "        component = ('attn_head', layer, head)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid component format: {component_str}\")\n",
    "\n",
    "    # Use a Counter to count the occurrences of each tuple\n",
    "    global_counter = Counter()\n",
    "\n",
    "    # Iterate through the co-occurrence dictionary\n",
    "    for comp_pair, co_occurrences in co_occurrence_dict.items():\n",
    "        comp1, comp2 = comp_pair\n",
    "\n",
    "        if comp1 == component or comp2 == component:\n",
    "            for feature_tuple in co_occurrences:\n",
    "                global_counter[(comp_pair, feature_tuple)] += 1\n",
    "\n",
    "    # Get the top-k tuples by count\n",
    "    top_k_tuples = global_counter.most_common(k)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    top_k_dict = defaultdict(dict)\n",
    "    \n",
    "    for (comp_pair, feature_tuple), count in top_k_tuples:\n",
    "        top_k_dict[comp_pair][feature_tuple] = count\n",
    "\n",
    "    return top_k_dict\n",
    "\n",
    "get_top_k_feature_tuples_for_component(cp.co_occurrence_dict, \"L8H6\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_feature_tuples_for_component(cp.co_occurrence_dict, \"L9H9\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [20101]\n",
    "sae = z_saes[9]\n",
    "feature_scores = get_feature_scores(model, sae, owt_tokens_torch, features, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top k activating examples\n",
    "feature_idx = 0 # corresponding to 16109\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, feature_idx, :], owt_tokens_torch, k=5)\n",
    "\n",
    "encoder_feature_pairs = [(sae, [20101])]\n",
    "\n",
    "# top_tokens, top_logits = get_top_k_tokens(model, sae, features[feature_idx], k=20, act_strength=5)\n",
    "\n",
    "top_tokens, top_logits = get_top_k_tokens(model, encoder_feature_pairs, k=20, act_strength=5)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)\n",
    "\n",
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [6798]\n",
    "transcoder = transcoders[0]\n",
    "feature_scores = get_feature_scores(model, transcoder, owt_tokens_torch, features, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top k activating examples\n",
    "feature_idx = 0 # corresponding to 16109\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, feature_idx, :], owt_tokens_torch, k=5)\n",
    "\n",
    "encoder_feature_pairs = [(transcoder, [6798])]\n",
    "\n",
    "# top_tokens, top_logits = get_top_k_tokens(model, sae, features[feature_idx], k=20, act_strength=5)\n",
    "\n",
    "top_tokens, top_logits = get_top_k_tokens(model, encoder_feature_pairs, k=20, act_strength=5)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)\n",
    "\n",
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_feature_tuples_for_component(cp.co_occurrence_dict, \"L9H9\", k=20)\n",
    "\n",
    "encoder_feature_pairs = [(transcoders[0], [6798]), (z_saes[9], [20101])]\n",
    "top_tokens, top_logits = get_top_k_tokens(model, encoder_feature_pairs, k=20)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = get_feature_scores_across_layers(model, encoder_feature_pairs, owt_tokens_torch, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens, [0, 1], k=25, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_feature_tuples_for_component(cp.co_occurrence_dict, \"L4H11\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 8\n",
    "head = 6\n",
    "feature = 16513\n",
    "\n",
    "features = [feature]\n",
    "sae = z_saes[layer]\n",
    "\n",
    "feature_scores = get_feature_scores(model, sae, owt_tokens_torch[:1024*4], features, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, 0, :], owt_tokens_torch[:1024*4], k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens, top_logits = get_top_k_tokens(model, [(sae, [feature])], k=10, act_strength=5)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerp_prompts import get_opening_prompt\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def new_get_response(llm_client, examples_clean_text, top_tokens):\n",
    "    opening_prompt = get_opening_prompt(examples_clean_text, top_tokens)\n",
    "    messages = [{\"role\": \"user\", \"content\": opening_prompt}]\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt4_large\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return f\"{response.choices[0].message.content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_interpretation = new_get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autointerp over clusters of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through co-occurrence dict and print any tuples that have a feature over 24576\n",
    "for k, v in cp.co_occurrence_dict.items():\n",
    "    for feature_tuple in v:\n",
    "        if any([f > 24576 for f in feature_tuple]):\n",
    "            print(k, feature_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = z_saes[5]\n",
    "sae.W_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scores_for_component_cluster(component_name: str, layer: int):\n",
    "    features = [x for x in list(set(cp.circuit_hypergraph[component_name]['features'])) if x!=-1]\n",
    "\n",
    "    sae = z_saes[layer]\n",
    "    feature_scores = get_feature_scores(model, sae, owt_tokens_torch[:1024*4], features, batch_size=128)\n",
    "\n",
    "    top_tokens, top_logits = get_top_k_tokens(model, [(sae, features)], k=10, act_strength=5)\n",
    "\n",
    "    return feature_scores, top_tokens, top_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores, top_tokens, top_logits = feature_scores_for_component_cluster(\"L0_H1\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens_torch[:1024*4], feature_indices=[x for x in range(feature_scores.shape[1])], k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores, top_tokens, top_logits = feature_scores_for_component_cluster(\"L5_H5\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens_torch[:1024*4], feature_indices=[x for x in range(feature_scores.shape[1])], k=10)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_interpretation = get_response(llm_client, examples_clean_text, top_tokens)\n",
    "print(feature_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores, top_tokens, top_logits = feature_scores_for_component_cluster(\"L10_H7\", 10)\n",
    "\n",
    "example_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens_torch[:1024*4], feature_indices=[x for x in range(feature_scores.shape[1])], k=10)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores, top_tokens, top_logits = feature_scores_for_component_cluster(\"L8_H6\", 8)\n",
    "\n",
    "example_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens_torch[:1024*4], feature_indices=[x for x in range(feature_scores.shape[1])], k=10)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow for evaluating autointerp of specific method:\n",
    "* Get the interpretation of the feature/family of features using everything we've set up above.\n",
    "* Provide that interpretation, along with the reasoning, to another LLM.\n",
    "* For a bunch of different IOI sequences, and for randomly selected tokens in that sequence, get the LLM to predict the feature activations.\n",
    "* Look at the correlation coefficient between the predicted feature activations and the actual feature activations. This is the evaluation metric.\n",
    "\n",
    "There are a few changes based on if we're looking at a specific feature (i.e. when we're just ablating token importance information added in to autointerp prompt) vs. a family of features. When we have a family of features:\n",
    "* We still get the interpretation as before - everything about this is normal. We look at max-activating examples for all features (not summing though - we need to change this to an argmax type thing for each example). For max-boosted logits, we boost all features at once. \n",
    "* When we get the feature activations, I think we should take the max-activation on each token of _any_ feature in the feature family. In this way, the autointerp is just predicting whether this feature family will fire in general - still gives a good idea of performance.\n",
    "\n",
    "For feature co-occurrence, we do the same correlation score approach as family of features, but:\n",
    "* When getting the interpretation, we look at max-activating examples for both sets of features (max for both on an example, divided by 2; same setup as feature families but now there's two features). Logits are boosted by both simultaeneously. \n",
    "\n",
    "\n",
    "Will have to figure out a way to combine token importances with feature families/family co-occurrences when the time comes.\n",
    "\n",
    "Finally, we need a baseline to compare against.\n",
    "* For token importances, this is easy, since we're only looking at one feature at a time. Just compare the correlation score with and without the token importance information.\n",
    "* For feature families, we can compare to the average correlation score of running the autointerp on a given feature in that family. Whilst not exactly the same thing, it gives an idea about how feature families help us to generalise.\n",
    "* For feature co-occurrences, we should also...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "from max_act_analysis import MaxActAnalysis,open_web_text_tokens\n",
    "from aug_interp_prompts import main_aug_interp_prompt, main_aug_interp_prompt_v2\n",
    "from openai_utils import gen_openai_completion, get_response\n",
    "from autointerpretability import *\n",
    "from discovery_strategies import (\n",
    "    create_filter,\n",
    "    create_simple_greedy_strategy,\n",
    "    create_top_contributor_strategy,\n",
    ")\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "feature = 27535\n",
    "# feature = 16401\n",
    "layer = 5\n",
    "# feature = 15647\n",
    "# num_examples = 1000\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "analyze = MaxActAnalysis(\"attn\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy)\n",
    "analyze.show_top_active_examples(num_examples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_examples = analyze.get_context_referenced_prompts_for_range(0, 25)\n",
    "\n",
    "p = main_aug_interp_prompt(mini_examples)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aug_interp_prompts import main_aug_interp_prompt, main_aug_interp_prompt_v2\n",
    "\n",
    "p_base = main_aug_interp_prompt_v2(mini_examples)\n",
    "print(p_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = \"This neuron appears to activate when it encounters an example of 'pair linking' in the text, usually manifested through a conjunction like 'and', activating on the repetition of a pair of words from earlier in the text to later in the text. The activating token is the second of the pair.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_base = get_response(p_base)\n",
    "print(interp_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "device = 'cpu'\n",
    "model, z_saes, transcoders = get_model_encoders(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some IOI examples where it activates (max activation across feature family is the ground-truth)\n",
    "\n",
    "feature = 27535\n",
    "layer = 5\n",
    "component_type = 'attn'\n",
    "\n",
    "# Get the actual prompts\n",
    "n_prompts = 100\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=n_prompts)\n",
    "dataset_prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "dataset_tokens = model.to_tokens(dataset_prompts)\n",
    "\n",
    "# Run the model over the prompts and get the feature activations at each token in each prompt\n",
    "_, cache = model.run_with_cache(dataset_tokens)\n",
    "z = cache[\"z\", layer]\n",
    "b, s, n, d = z.shape\n",
    "del cache\n",
    "z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "\n",
    "# Apply relevant SAE or transcoder to the activations\n",
    "if component_type == 'attn':\n",
    "    encoder = z_saes[layer]\n",
    "else:\n",
    "    encoder = transcoders[layer]\n",
    "\n",
    "z_hidden = encoder.encode(z)\n",
    "z_hidden = einops.rearrange(z_hidden, \"(b s) h -> b s h\", s=s)\n",
    "\n",
    "# Only keep feature indices (last dimension)\n",
    "feature_indices = [feature]\n",
    "z_hidden = z_hidden[:, :, feature_indices]\n",
    "\n",
    "# For each batch (first dimension) and each token in each batch (second dimension), only keep the max activation (third dimension)\n",
    "z_hidden = z_hidden.max(dim=2).values\n",
    "\n",
    "# Print rows which have a non-zero value\n",
    "non_zero_indices = np.where(z_hidden > 0.0)[0].tolist()[:5]\n",
    "print(non_zero_indices)\n",
    "\n",
    "# Keep 3 of these rows plus the row after\n",
    "indices_to_keep = []\n",
    "for i, j in enumerate(non_zero_indices):\n",
    "    indices_to_keep.extend([j])\n",
    "\n",
    "print(indices_to_keep)\n",
    "\n",
    "# Keep z_hidden rows\n",
    "z_hidden = z_hidden[indices_to_keep]\n",
    "\n",
    "z_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new LLM interpreter given interpretation\n",
    "from jinja2 import Template\n",
    "from typing import List\n",
    "\n",
    "def follow_up_activation_prediction_prompt(\n",
    "    interpretation: str, sentence: str\n",
    "):\n",
    "    last_word_in_sentence = sentence.split()[-1]\n",
    "    \n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are an AI researcher continuing an important investigation into a certain neuron in a language model. Your task is to predict whether this neuron will activate on the final word of a given sentence based on a previously provided interpretation of the neuron's behavior. Here's how you will complete this task: #}\n",
    "\n",
    "You are an AI researcher continuing an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to predict whether this neuron will have a zero or non-zero activation on the final word of a given sentence based on the provided interpretation of the neuron's behavior.\n",
    "\n",
    "INTERPRETATION:\n",
    "{{interpretation}}\n",
    "\n",
    "INPUT:\n",
    "The sentence to analyze is:\n",
    "=================================================\n",
    "{{sentence}}\n",
    "=================================================\n",
    "\n",
    "The final word to analyze is: {{last_word_in_sentence}}\n",
    "\n",
    "OUTPUT:\n",
    "Based on the provided interpretation, analyze the sentence and describe your reasoning in two sentences. Then, predict whether the neuron will have a zero or non-zero activation on the final word of the sentence. Provide your answer in the following format:\n",
    "[ANALYSIS]: <two sentences of analysis>\n",
    "[ACTIVATION]: zero or non-zero\n",
    "\n",
    "Guidelines:\n",
    "- Carefully consider the interpretation and apply it to the given sentence.\n",
    "- Your analysis should be concise and relevant to the provided interpretation.\n",
    "- Your prediction should be either \"zero\" or \"non-zero\".\n",
    "\n",
    "EXAMPLE:\n",
    "[ANALYSIS]: The final word in the sentence fits the pattern described in the interpretation. The context provided in the sentence suggests a non-zero activation.\n",
    "[ACTIVATION]: non-zero\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"interpretation\": interpretation, \"sentence\": sentence, \"last_word_in_sentence\": last_word_in_sentence}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each IOI example, predict the feature activation score on a given token\n",
    "accuracies, f1s = [], []\n",
    "for i in range(z_hidden.shape[0]):\n",
    "    sentence = dataset_prompts[i]\n",
    "    sentence_tokens = model.to_tokens(sentence).squeeze()\n",
    "    activations = z_hidden[i]\n",
    "\n",
    "    pred_dict = {}\n",
    "\n",
    "    # positions = [5, 6, 14, 15, 19, 20]\n",
    "    threshold = 1.0\n",
    "    non_zero_indices = np.where(activations > threshold)[0].tolist()\n",
    "    print(non_zero_indices)\n",
    "    # Add one before and after each non-zero index\n",
    "    positions = []\n",
    "    for i in non_zero_indices:\n",
    "        positions.extend([i-1, i, i+1])\n",
    "    positions = [x for x in positions if x > 0 and x < len(sentence_tokens)]\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        # Randomly sample 3 positions\n",
    "        positions = np.random.choice(len(sentence_tokens), 3, replace=False)\n",
    "\n",
    "    print(positions)\n",
    "\n",
    "    for i in tqdm(positions):\n",
    "        sentence_str_example = model.to_string(sentence_tokens[:i+1])\n",
    "        sentence_tokens_example = sentence_tokens[:i+1]\n",
    "        ground_truth = activations[i].item()\n",
    "\n",
    "        # Get the prediction\n",
    "        prompt = follow_up_activation_prediction_prompt(interp, sentence_str_example)\n",
    "        prediction = get_response(prompt) #gen_openai_completion(prompt, visualize_stream=False)\n",
    "        print(prediction)\n",
    "        print(sentence_str_example)\n",
    "        print(ground_truth)\n",
    "        print()\n",
    "        pred_dict[i] = {\"prediction\": prediction, \"sentence\": sentence_str_example, \"ground_truth\": ground_truth}\n",
    "\n",
    "    for k, v in pred_dict.items():\n",
    "        prediction = v[\"prediction\"].split('[ACTIVATION]: ')[-1].strip()\n",
    "        pred_dict[k]['prediction_numeric'] = 1.0 if prediction == 'non-zero' else 0.0\n",
    "        pred_dict[k]['ground_truth_numeric'] = 1.0 if v['ground_truth'] > threshold else 0.0\n",
    "\n",
    "    # Get accuracy and f1 score\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in pred_dict.items():\n",
    "        if v['prediction_numeric'] == v['ground_truth_numeric']:\n",
    "            correct += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 1:\n",
    "            tp += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 0:\n",
    "            fp += 1\n",
    "        if v['prediction_numeric'] == 0 and v['ground_truth_numeric'] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    if tp + fp + fn > 0:\n",
    "        accuracy = correct / len(pred_dict)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "\n",
    "# Means\n",
    "print(np.mean(accuracies))\n",
    "print(np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each IOI example, predict the feature activation score on a given token\n",
    "accuracies, f1s = [], []\n",
    "for i in range(z_hidden.shape[0]):\n",
    "    sentence = dataset_prompts[i]\n",
    "    sentence_tokens = model.to_tokens(sentence).squeeze()\n",
    "    activations = z_hidden[i]\n",
    "\n",
    "    pred_dict = {}\n",
    "\n",
    "    # positions = [5, 6, 14, 15, 19, 20]\n",
    "    threshold = 1.0\n",
    "    non_zero_indices = np.where(activations > threshold)[0].tolist()\n",
    "    print(non_zero_indices)\n",
    "    # Add one before and after each non-zero index\n",
    "    positions = []\n",
    "    for i in non_zero_indices:\n",
    "        positions.extend([i-1, i, i+1])\n",
    "    positions = [x for x in positions if x > 0 and x < len(sentence_tokens)]\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        # Randomly sample 3 positions\n",
    "        positions = np.random.choice(len(sentence_tokens), 3, replace=False)\n",
    "\n",
    "    print(positions)\n",
    "\n",
    "    for i in tqdm(positions):\n",
    "        sentence_str_example = model.to_string(sentence_tokens[:i+1])\n",
    "        sentence_tokens_example = sentence_tokens[:i+1]\n",
    "        ground_truth = activations[i].item()\n",
    "\n",
    "        # Get the prediction\n",
    "        prompt = follow_up_activation_prediction_prompt(interp_base, sentence_str_example)\n",
    "        prediction = get_response(prompt) #gen_openai_completion(prompt, visualize_stream=False)\n",
    "        print(prediction)\n",
    "        print(sentence_str_example)\n",
    "        print(ground_truth)\n",
    "        print()\n",
    "        pred_dict[i] = {\"prediction\": prediction, \"sentence\": sentence_str_example, \"ground_truth\": ground_truth}\n",
    "\n",
    "    for k, v in pred_dict.items():\n",
    "        prediction = v[\"prediction\"].split('[ACTIVATION]: ')[-1].strip()\n",
    "        pred_dict[k]['prediction_numeric'] = 1.0 if prediction == 'non-zero' else 0.0\n",
    "        pred_dict[k]['ground_truth_numeric'] = 1.0 if v['ground_truth'] > threshold else 0.0\n",
    "\n",
    "    # Get accuracy and f1 score\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in pred_dict.items():\n",
    "        if v['prediction_numeric'] == v['ground_truth_numeric']:\n",
    "            correct += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 1:\n",
    "            tp += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 0:\n",
    "            fp += 1\n",
    "        if v['prediction_numeric'] == 0 and v['ground_truth_numeric'] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    if tp + fp + fn > 0:\n",
    "        accuracy = correct / len(pred_dict)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "\n",
    "# Means\n",
    "print(np.mean(accuracies))\n",
    "print(np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"16513_8_attn\": {\n",
    "        \"token_importance\":{\n",
    "            \"accuracies\": 0.83,\n",
    "            \"f1s\": 0.66,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.79,\n",
    "            \"f1s\": 0.54,\n",
    "        }\n",
    "    },\n",
    "    \"24166_2_attn\": {\n",
    "        \"token_importance\":{\n",
    "            \"accuracies\": 0.73,\n",
    "            \"f1s\": 0.2,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.6,\n",
    "            \"f1s\": 0.0,\n",
    "        }\n",
    "    },\n",
    "    \"27535_5_attn\": {\n",
    "        \"token_importance\":{\n",
    "            \"accuracies\": 0.76,\n",
    "            \"f1s\": 0.4,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.36,\n",
    "            \"f1s\": 0.2,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "results_dict = {\n",
    "    \"16513_8_attn\": {\n",
    "        \"token_importance\": {\n",
    "            \"accuracies\": 0.83,\n",
    "            \"f1s\": 0.66,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.79,\n",
    "            \"f1s\": 0.54,\n",
    "        }\n",
    "    },\n",
    "    \"24166_2_attn\": {\n",
    "        \"token_importance\": {\n",
    "            \"accuracies\": 0.73,\n",
    "            \"f1s\": 0.2,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.6,\n",
    "            \"f1s\": 0.0,\n",
    "        }\n",
    "    },\n",
    "    \"27535_5_attn\": {\n",
    "        \"token_importance\": {\n",
    "            \"accuracies\": 0.76,\n",
    "            \"f1s\": 0.4,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.36,\n",
    "            \"f1s\": 0.2,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def create_label(key):\n",
    "    feature_id, layer, attn = key.split(\"_\")\n",
    "    return f\"Feat. {feature_id} (L{layer} Attn.)\"\n",
    "\n",
    "labels = [create_label(key) for key in results_dict.keys()]\n",
    "token_importance_accuracies = [results_dict[key][\"token_importance\"][\"accuracies\"] for key in results_dict.keys()]\n",
    "token_importance_f1s = [results_dict[key][\"token_importance\"][\"f1s\"] for key in results_dict.keys()]\n",
    "base_accuracies = [results_dict[key][\"base\"][\"accuracies\"] for key in results_dict.keys()]\n",
    "base_f1s = [results_dict[key][\"base\"][\"f1s\"] for key in results_dict.keys()]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Token Importance Accuracies', x=labels, y=token_importance_accuracies, marker_color='#ADD8E6'),  # Light blue\n",
    "    go.Bar(name='Token Importance F1s', x=labels, y=token_importance_f1s, marker_color='#00008B'),  # Dark blue\n",
    "    go.Bar(name='Base Accuracies', x=labels, y=base_accuracies, marker_color='#90EE90'),  # Light green\n",
    "    go.Bar(name='Base F1s', x=labels, y=base_f1s, marker_color='#006400')  # Dark green\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Token Importance vs. Base Performance',\n",
    "    xaxis_title='Feature',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    legend_title_text='Metrics',\n",
    "    font=dict(size=14),\n",
    "    template='plotly_white',\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi', N=20)\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800 * 2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_feature_tuples_for_component(co_occurrence_dict, component_str, k=5):\n",
    "    # Parse the component string to get the appropriate tuple key\n",
    "    if component_str.startswith(\"MLP\"):\n",
    "        layer = int(component_str[3:])\n",
    "        component = ('mlp_feature', layer)\n",
    "    elif component_str.startswith(\"L\") and \"H\" in component_str:\n",
    "        layer, head = map(int, component_str[1:].split(\"H\"))\n",
    "        component = ('attn_head', layer, head)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid component format: {component_str}\")\n",
    "\n",
    "    # Use a Counter to count the occurrences of each tuple\n",
    "    global_counter = Counter()\n",
    "\n",
    "    # Iterate through the co-occurrence dictionary\n",
    "    for comp_pair, co_occurrences in co_occurrence_dict.items():\n",
    "        comp1, comp2 = comp_pair\n",
    "\n",
    "        if comp1 == component or comp2 == component:\n",
    "            for feature_tuple in co_occurrences:\n",
    "                global_counter[(comp_pair, feature_tuple)] += 1\n",
    "\n",
    "    # Get the top-k tuples by count\n",
    "    top_k_tuples = global_counter.most_common(k)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    top_k_dict = defaultdict(dict)\n",
    "    \n",
    "    for (comp_pair, feature_tuple), count in top_k_tuples:\n",
    "        top_k_dict[comp_pair][feature_tuple] = count\n",
    "\n",
    "    return top_k_dict\n",
    "\n",
    "get_top_k_feature_tuples_for_component(cp.co_occurrence_dict, \"L5H5\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_top_k_feature_tuples(k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_name = 'L5_H5'\n",
    "layer = 5\n",
    "\n",
    "features = [x for x in list(set(cp.circuit_hypergraph[component_name]['features'])) if x!=-1]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_utils import get_response\n",
    "\n",
    "def get_interpretation_old(examples_clean_text, top_tokens):\n",
    "    opening_prompt = get_opening_prompt(examples_clean_text, top_tokens)\n",
    "    return get_response(opening_prompt) #gen_openai_completion(opening_prompt, visualize_stream=False)\n",
    "\n",
    "def feature_scores_for_component_cluster(component_name: str, layer: int):\n",
    "    features = [x for x in list(set(cp.circuit_hypergraph[component_name]['features'])) if x!=-1]\n",
    "\n",
    "    sae = z_saes[layer]\n",
    "    feature_scores = get_feature_scores(model, sae, owt_tokens_torch[:1024*4], features, batch_size=128)\n",
    "\n",
    "    top_tokens, top_logits = get_top_k_tokens(model, [(sae, features)], k=10, act_strength=5)\n",
    "\n",
    "    return feature_scores, top_tokens, top_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the interpretation\n",
    "\n",
    "feature_scores, top_tokens, top_logits = feature_scores_for_component_cluster(component_name, layer)\n",
    "\n",
    "example_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens_torch[:1024*4], \n",
    "                                                                          feature_indices=[x for x in range(feature_scores.shape[1])], k=25, display_html=False)\n",
    "\n",
    "top_tokens, top_logits = get_top_k_tokens(model, [(z_saes[layer], [x for x in list(set(cp.circuit_hypergraph[component_name]['features'])) if x!=-1])], k=10, act_strength=5)\n",
    "\n",
    "interpretation = get_interpretation_old(examples_clean_text, top_tokens)\n",
    "\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some IOI examples where it activates (max activation across feature family is the ground-truth)\n",
    "\n",
    "# Get the actual prompts\n",
    "n_prompts = 100\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=n_prompts)\n",
    "dataset_prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "dataset_tokens = model.to_tokens(dataset_prompts)\n",
    "\n",
    "# Run the model over the prompts and get the feature activations at each token in each prompt\n",
    "_, cache = model.run_with_cache(dataset_tokens)\n",
    "z = cache[\"z\", layer]\n",
    "b, s, n, d = z.shape\n",
    "del cache\n",
    "z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "\n",
    "# Apply relevant SAE or transcoder to the activations\n",
    "if component_name.startswith(\"L\"):\n",
    "    encoder = z_saes[layer]\n",
    "else:\n",
    "    encoder = transcoders[layer]\n",
    "\n",
    "z_hidden = encoder.encode(z)\n",
    "z_hidden = einops.rearrange(z_hidden, \"(b s) h -> b s h\", s=s)\n",
    "\n",
    "# Only keep feature indices (last dimension)\n",
    "feature_indices = [x for x in list(set(cp.circuit_hypergraph[component_name]['features'])) if x!=-1]\n",
    "print(feature_indices)\n",
    "z_hidden = z_hidden[:, :, feature_indices]\n",
    "\n",
    "# For each batch (first dimension) and each token in each batch (second dimension), only keep the max activation (third dimension)\n",
    "z_hidden = z_hidden.max(dim=2).values\n",
    "\n",
    "# Set first entry in each batch to 0\n",
    "z_hidden[:, 0] = 0\n",
    "\n",
    "# Print rows which have a non-zero value\n",
    "non_zero_indices = np.where(z_hidden > 0.0)[0].tolist()[:3]\n",
    "print(non_zero_indices)\n",
    "\n",
    "# Keep 3 of these rows plus the row after\n",
    "indices_to_keep = []\n",
    "for i, j in enumerate(non_zero_indices):\n",
    "    indices_to_keep.extend([j, j+1])\n",
    "\n",
    "print(indices_to_keep)\n",
    "\n",
    "# Keep z_hidden rows\n",
    "z_hidden = z_hidden[indices_to_keep]\n",
    "\n",
    "z_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new LLM interpreter given interpretation\n",
    "from jinja2 import Template\n",
    "from typing import List\n",
    "\n",
    "def follow_up_activation_prediction_prompt(\n",
    "    interpretation: str, sentence: str\n",
    "):\n",
    "    last_word_in_sentence = sentence.split()[-1]\n",
    "    \n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are an AI researcher continuing an important investigation into a certain neuron in a language model. Your task is to predict whether this neuron will activate on the final word of a given sentence based on a previously provided interpretation of the neuron's behavior. Here's how you will complete this task: #}\n",
    "\n",
    "You are an AI researcher continuing an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to predict whether this neuron will have a zero or non-zero activation on the final word of a given sentence based on the provided interpretation of the neuron's behavior.\n",
    "\n",
    "INTERPRETATION:\n",
    "{{interpretation}}\n",
    "\n",
    "INPUT:\n",
    "The sentence to analyze is:\n",
    "=================================================\n",
    "{{sentence}}\n",
    "=================================================\n",
    "\n",
    "The final word to analyze is: {{last_word_in_sentence}}\n",
    "\n",
    "OUTPUT:\n",
    "Based on the provided interpretation, analyze the sentence and describe your reasoning in two sentences. Then, predict whether the neuron will have a zero or non-zero activation on the final word of the sentence. Provide your answer in the following format:\n",
    "[ANALYSIS]: <two sentences of analysis>\n",
    "[ACTIVATION]: zero or non-zero\n",
    "\n",
    "Guidelines:\n",
    "- Carefully consider the interpretation and apply it to the given sentence.\n",
    "- Your analysis should be concise and relevant to the provided interpretation.\n",
    "- Do not be too rigid; if the interpretation provides an example of an activating token don't assume that specific token always has to be present - follow the pattern instead.\n",
    "- For instance, if the interpretation suggests the neuron activates on names and provides an example name 'David', whilst suggesting other names can activate it, don't just predict zero if David isn't present.\n",
    "- Your prediction should be either \"zero\" or \"non-zero\".\n",
    "\n",
    "EXAMPLE:\n",
    "[ANALYSIS]: The final word in the sentence fits the pattern described in the interpretation. The context provided in the sentence suggests a non-zero activation.\n",
    "[ACTIVATION]: non-zero\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"interpretation\": interpretation, \"sentence\": sentence, \"last_word_in_sentence\": last_word_in_sentence}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each IOI example, predict the feature activation score on a given token\n",
    "accuracies, f1s = [], []\n",
    "for i in range(z_hidden.shape[0]):\n",
    "    sentence = dataset_prompts[i]\n",
    "    sentence_tokens = model.to_tokens(sentence).squeeze()\n",
    "    activations = z_hidden[i]\n",
    "\n",
    "    pred_dict = {}\n",
    "\n",
    "    # positions = [5, 6, 14, 15, 19, 20]\n",
    "    threshold = 1.0\n",
    "    non_zero_indices = np.where(activations > threshold)[0].tolist()\n",
    "    print(non_zero_indices)\n",
    "    # Add one before and after each non-zero index\n",
    "    positions = []\n",
    "    for i in non_zero_indices:\n",
    "        positions.extend([i-1, i, i+1])\n",
    "    positions = [x for x in positions if x > 0 and x < len(sentence_tokens)]\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        # Randomly sample 3 positions\n",
    "        positions = np.random.choice(len(sentence_tokens), 3, replace=False)\n",
    "\n",
    "    print(positions)\n",
    "\n",
    "    for i in tqdm(positions):\n",
    "        sentence_str_example = model.to_string(sentence_tokens[:i+1])\n",
    "        sentence_tokens_example = sentence_tokens[:i+1]\n",
    "        ground_truth = activations[i].item()\n",
    "\n",
    "        # Get the prediction\n",
    "        prompt = follow_up_activation_prediction_prompt(interpretation, sentence_str_example)\n",
    "        prediction = get_response(prompt) #gen_openai_completion(prompt, visualize_stream=False)\n",
    "        print(prediction)\n",
    "        print(sentence_str_example)\n",
    "        print(ground_truth)\n",
    "        print()\n",
    "        pred_dict[i] = {\"prediction\": prediction, \"sentence\": sentence_str_example, \"ground_truth\": ground_truth}\n",
    "\n",
    "    for k, v in pred_dict.items():\n",
    "        prediction = v[\"prediction\"].split('[ACTIVATION]: ')[-1].strip()\n",
    "        pred_dict[k]['prediction_numeric'] = 1.0 if prediction == 'non-zero' else 0.0\n",
    "        pred_dict[k]['ground_truth_numeric'] = 1.0 if v['ground_truth'] > threshold else 0.0\n",
    "\n",
    "    # Get accuracy and f1 score\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in pred_dict.items():\n",
    "        if v['prediction_numeric'] == v['ground_truth_numeric']:\n",
    "            correct += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 1:\n",
    "            tp += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 0:\n",
    "            fp += 1\n",
    "        if v['prediction_numeric'] == 0 and v['ground_truth_numeric'] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    if tp + fp + fn > 0:\n",
    "        accuracy = correct / len(pred_dict)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import AzureOpenAI\n",
    "\n",
    "# config = yaml.safe_load(open(\"config.yaml\"))\n",
    "# azure_client = AzureOpenAI(\n",
    "#     azure_endpoint=config[\"base_url\"],\n",
    "#     api_key=config[\"azure_api_key\"],\n",
    "#     api_version=config[\"api_version\"],\n",
    "# )\n",
    "\n",
    "# opening_prompt = get_opening_prompt(examples_clean_text, top_tokens)\n",
    "# print(opening_prompt)\n",
    "# messages = [{\"role\": \"user\", \"content\": opening_prompt}]\n",
    "# response = azure_client.chat.completions.create(\n",
    "#     model=\"gpt4_large\",\n",
    "#     messages=messages,\n",
    "# )\n",
    "# interpretation = f\"{response.choices[0].message.content}\"\n",
    "\n",
    "interpretation = \"\"\" \n",
    "[EXPLANATION]: The neuron activates on names, especially in possessive or direct reference contexts, and activates in texts involving repeated mentions of specific names or entities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of accuracies and f1 scores\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For each individual feature in our feature family, rerun the autointerp and get the scores for that individual feature\n",
    "# features = [x for x in list(set(cp.circuit_hypergraph[component_name]['features'])) if x!=-1]\n",
    "\n",
    "# feature_scores = get_feature_scores(model, z_saes[layer], owt_tokens_torch[:1024*4], feature_indices=[x for x in range(len(features))], batch_size=128)\n",
    "\n",
    "feat_idx = 1\n",
    "\n",
    "examples_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, feat_idx, :], owt_tokens_torch[:1024*4], k=10, display_html=False)\n",
    "\n",
    "top_tokens, top_logits = get_top_k_tokens(model, [(z_saes[layer], [features[feat_idx]])], k=10, act_strength=5)\n",
    "\n",
    "interpretation = get_interpretation(examples_clean_text, top_tokens)\n",
    "\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some IOI examples where it activates (max activation across feature family is the ground-truth)\n",
    "\n",
    "# Get the actual prompts\n",
    "n_prompts = 100\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=n_prompts)\n",
    "dataset_prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "dataset_tokens = model.to_tokens(dataset_prompts)\n",
    "\n",
    "# Run the model over the prompts and get the feature activations at each token in each prompt\n",
    "_, cache = model.run_with_cache(dataset_tokens)\n",
    "z = cache[\"z\", layer]\n",
    "b, s, n, d = z.shape\n",
    "del cache\n",
    "z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "\n",
    "# Apply relevant SAE or transcoder to the activations\n",
    "if component_name.startswith(\"L\"):\n",
    "    encoder = z_saes[layer]\n",
    "else:\n",
    "    encoder = transcoders[layer]\n",
    "\n",
    "z_hidden = encoder.encode(z)\n",
    "z_hidden = einops.rearrange(z_hidden, \"(b s) h -> b s h\", s=s)\n",
    "\n",
    "# Only keep feature indices (last dimension)\n",
    "feature_indices = [features[feat_idx]]\n",
    "z_hidden = z_hidden[:, :, feature_indices]\n",
    "\n",
    "# For each batch (first dimension) and each token in each batch (second dimension), only keep the max activation (third dimension)\n",
    "z_hidden = z_hidden.max(dim=2).values\n",
    "\n",
    "# Print rows which have a non-zero value\n",
    "non_zero_indices = np.where(z_hidden > 0.0)[0].tolist()[:3]\n",
    "print(non_zero_indices)\n",
    "\n",
    "# Keep 3 of these rows plus the row after\n",
    "indices_to_keep = []\n",
    "for i, j in enumerate(non_zero_indices):\n",
    "    indices_to_keep.extend([j, j+1])\n",
    "\n",
    "print(indices_to_keep)\n",
    "\n",
    "# Keep z_hidden rows\n",
    "z_hidden = z_hidden[indices_to_keep]\n",
    "\n",
    "z_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_utils import get_response\n",
    "\n",
    "# For each IOI example, predict the feature activation score on a given token\n",
    "accuracies, f1s = [], []\n",
    "for i in range(z_hidden.shape[0]):\n",
    "    sentence = dataset_prompts[i]\n",
    "    sentence_tokens = model.to_tokens(sentence).squeeze()\n",
    "    activations = z_hidden[i]\n",
    "\n",
    "    pred_dict = {}\n",
    "\n",
    "    threshold = 0.0\n",
    "    non_zero_indices = np.where(activations > threshold)[0].tolist()\n",
    "    # Add one before and after each non-zero index\n",
    "    positions = []\n",
    "    for i in non_zero_indices:\n",
    "        positions.extend([i-1, i, i+1])\n",
    "    positions = [x for x in positions if x >= 0 and x < len(sentence_tokens)]\n",
    "    print(positions)\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        # Radomly sample 3 positions\n",
    "        positions = np.random.choice(len(sentence_tokens), 3, replace=False)\n",
    "\n",
    "    for i in tqdm(positions):\n",
    "        sentence_str_example = model.to_string(sentence_tokens[:i+1])\n",
    "        sentence_tokens_example = sentence_tokens[:i+1]\n",
    "        ground_truth = activations[i].item()\n",
    "\n",
    "        # Get the prediction\n",
    "        prompt = follow_up_activation_prediction_prompt(interpretation, sentence_str_example)\n",
    "        prediction = get_response(prompt) #gen_openai_completion(prompt, visualize_stream=False)\n",
    "        pred_dict[i] = {\"prediction\": prediction, \"sentence\": sentence_str_example, \"ground_truth\": ground_truth}\n",
    "\n",
    "    for k, v in pred_dict.items():\n",
    "        prediction = v[\"prediction\"].split('[ACTIVATION]: ')[-1].strip()\n",
    "        pred_dict[k]['prediction_numeric'] = 1.0 if prediction == 'non-zero' else 0.0\n",
    "        pred_dict[k]['ground_truth_numeric'] = 1.0 if v['ground_truth'] > 0.0 else 0.0\n",
    "\n",
    "    # Get accuracy and f1 score\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in pred_dict.items():\n",
    "        if v['prediction_numeric'] == v['ground_truth_numeric']:\n",
    "            correct += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 1:\n",
    "            tp += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 0:\n",
    "            fp += 1\n",
    "        if v['prediction_numeric'] == 0 and v['ground_truth_numeric'] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    print(pred_dict)\n",
    "\n",
    "    if tp + fp + fn > 0:\n",
    "        accuracy = correct / len(pred_dict)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"L5H5\": {\"families\": {\"f1\": 1.0, \"accuracy\": 1.0}, \"individual\": {\"f1\": 0.0, \"accuracy\": 0.0}},\n",
    "    \"L8H6\": {},\n",
    "    \"L0H1\": {\"families\": {\"f1\": 0.59, \"accuracy\": 0.81}, \"individual\": {\"f1\": 0.05, \"accuracy\": 0.75}},\n",
    "    \"L2H2\": {\"families\": {\"f1\": 0.75, \"accuracy\": 0.91}, \"individual\": {\"f1\": 0.06, \"accuracy\": 0.49}},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "results_dict = {\n",
    "    \"L5H5\": {\"families\": {\"f1\": 1.0, \"accuracy\": 1.0}, \"individual\": {\"f1\": 0.02, \"accuracy\": 0.4}},\n",
    "    \"L0H1\": {\"families\": {\"f1\": 0.59, \"accuracy\": 0.81}, \"individual\": {\"f1\": 0.05, \"accuracy\": 0.75}},\n",
    "    \"L2H2\": {\"families\": {\"f1\": 0.75, \"accuracy\": 0.91}, \"individual\": {\"f1\": 0.06, \"accuracy\": 0.49}},\n",
    "}\n",
    "\n",
    "labels = list(results_dict.keys())\n",
    "families_f1 = [results_dict[key][\"families\"][\"f1\"] if \"families\" in results_dict[key] else None for key in labels]\n",
    "families_accuracy = [results_dict[key][\"families\"][\"accuracy\"] if \"families\" in results_dict[key] else None for key in labels]\n",
    "individual_f1 = [results_dict[key][\"individual\"][\"f1\"] if \"individual\" in results_dict[key] else None for key in labels]\n",
    "individual_accuracy = [results_dict[key][\"individual\"][\"accuracy\"] if \"individual\" in results_dict[key] else None for key in labels]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Families F1', x=labels, y=families_f1, marker_color='#ADD8E6'),  # Light blue\n",
    "    go.Bar(name='Families Accuracy', x=labels, y=families_accuracy, marker_color='#00008B'),  # Dark blue\n",
    "    go.Bar(name='Individual F1', x=labels, y=individual_f1, marker_color='#90EE90'),  # Light green\n",
    "    go.Bar(name='Individual Accuracy', x=labels, y=individual_accuracy, marker_color='#006400')  # Dark green\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Family vs. individual feature autointerp - IOI performance',\n",
    "    xaxis_title='Attention Head',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    legend_title_text='Metrics',\n",
    "    font=dict(size=14),\n",
    "    template='plotly_white',\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "dataset_prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "\n",
    "names = ['David', 'Elizabeth', 'Paul', 'Sarah']\n",
    "\n",
    "# Count occurrence of each name in dataset prompts\n",
    "name_counts = {name: 0 for name in names}\n",
    "\n",
    "for prompt in dataset_prompts:\n",
    "    for name in names:\n",
    "        if name in prompt:\n",
    "            name_counts[name] += 1\n",
    "\n",
    "name_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import linregress\n",
    "\n",
    "dataset_occurrence = np.array([23, 22, 18, 19])\n",
    "dataset_occurrence = np.divide(dataset_occurrence, np.sum(dataset_occurrence))\n",
    "co_occurrence = np.array([13, 13, 11, 11]) / np.sum([13, 13, 11, 11])\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = linregress(co_occurrence, dataset_occurrence)\n",
    "\n",
    "# Create the line of best fit\n",
    "line_of_best_fit = slope * co_occurrence + intercept\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=co_occurrence, y=dataset_occurrence,\n",
    "                         mode='markers',\n",
    "                         name='Names',\n",
    "                         marker=dict(color='blue', size=10)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=co_occurrence, y=line_of_best_fit,\n",
    "                         mode='lines',\n",
    "                         name='LS Fit',\n",
    "                         line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Co-occurrence of name feature (MLP0 + L2H2) vs. Dataset Occurrence',\n",
    "    xaxis_title='Co-occurrence',\n",
    "    yaxis_title='Dataset Occurrence',\n",
    "    font=dict(size=14),\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "freq = pd.read_csv(\"data/unigram_freq.csv\")\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[freq.word == 'elizabeth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_feature_tuples_for_component(cp.co_occurrence_dict, \"L2H2\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_feature_pairs = [(transcoders[0], [12965]), (z_saes[2], [14186])]\n",
    "top_tokens, top_logits = get_top_k_tokens(model, encoder_feature_pairs, k=20)\n",
    "\n",
    "pretty_print_tokens_logits(top_tokens, top_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = get_feature_scores_across_layers(model, encoder_feature_pairs, owt_tokens_torch[:1024*2], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_html, examples_clean_text = display_top_k_activating_examples_sum(model, feature_scores, owt_tokens_torch[:1024*8], feature_indices=[0, 1], k=20, display_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_prompt = get_opening_prompt(examples_clean_text, top_tokens)\n",
    "\n",
    "# interpret\n",
    "interpretation = get_response(opening_prompt)\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = \"The neuron is activated by the use of many different common names. These names include 'David', as well as other names and words in the context of preceding names earlier in the sentences.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some IOI examples where it activates (max activation across feature family is the ground-truth)\n",
    "layer = 2\n",
    "feature = 24166\n",
    "component_name = 'L2_H2'\n",
    "\n",
    "# Get the actual prompts\n",
    "n_prompts = 100\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=n_prompts)\n",
    "dataset_prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "dataset_tokens = model.to_tokens(dataset_prompts)\n",
    "\n",
    "# Run the model over the prompts and get the feature activations at each token in each prompt\n",
    "_, cache = model.run_with_cache(dataset_tokens)\n",
    "z = cache[\"z\", layer]\n",
    "b, s, n, d = z.shape\n",
    "del cache\n",
    "z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "\n",
    "# Apply relevant SAE or transcoder to the activations\n",
    "if component_name.startswith(\"L\"):\n",
    "    encoder = z_saes[layer]\n",
    "else:\n",
    "    encoder = transcoders[layer]\n",
    "\n",
    "z_hidden = encoder.encode(z)\n",
    "z_hidden = einops.rearrange(z_hidden, \"(b s) h -> b s h\", s=s)\n",
    "\n",
    "# Only keep feature indices (last dimension)\n",
    "feature_indices = [feature]\n",
    "z_hidden = z_hidden[:, :, feature_indices]\n",
    "\n",
    "# For each batch (first dimension) and each token in each batch (second dimension), only keep the max activation (third dimension)\n",
    "z_hidden = z_hidden.max(dim=2).values\n",
    "\n",
    "# Print rows which have a non-zero value\n",
    "non_zero_indices = list(set(np.where(z_hidden > 1.0)[0].tolist()))[:5]\n",
    "print(non_zero_indices)\n",
    "\n",
    "# Keep 3 of these rows plus the row after\n",
    "indices_to_keep = []\n",
    "for i, j in enumerate(non_zero_indices):\n",
    "    indices_to_keep.extend([j])\n",
    "\n",
    "print(indices_to_keep)\n",
    "\n",
    "# Keep z_hidden rows\n",
    "z_hidden = z_hidden[indices_to_keep]\n",
    "\n",
    "z_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each IOI example, predict the feature activation score on a given token\n",
    "accuracies, f1s = [], []\n",
    "for i in range(z_hidden.shape[0]):\n",
    "    sentence = dataset_prompts[i]\n",
    "    sentence_tokens = model.to_tokens(sentence).squeeze()\n",
    "    activations = z_hidden[i]\n",
    "\n",
    "    pred_dict = {}\n",
    "\n",
    "    threshold = 0.5\n",
    "    non_zero_indices = np.where(activations > threshold)[0].tolist()\n",
    "    # Add one before and after each non-zero index\n",
    "    positions = []\n",
    "    for i in non_zero_indices:\n",
    "        positions.extend([i-1, i, i+1])\n",
    "    positions = [x for x in positions if x >= 0 and x < len(sentence_tokens)]\n",
    "    print(positions)\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        # Radomly sample 3 positions\n",
    "        positions = np.random.choice(len(sentence_tokens), 3, replace=False)\n",
    "\n",
    "    for i in tqdm(positions):\n",
    "        sentence_str_example = model.to_string(sentence_tokens[:i+1])\n",
    "        sentence_tokens_example = sentence_tokens[:i+1]\n",
    "        ground_truth = activations[i].item()\n",
    "\n",
    "        # Get the prediction\n",
    "        prompt = follow_up_activation_prediction_prompt(interpretation, sentence_str_example)\n",
    "        prediction = get_response(prompt) #gen_openai_completion(prompt, visualize_stream=False)\n",
    "        pred_dict[i] = {\"prediction\": prediction, \"sentence\": sentence_str_example, \"ground_truth\": ground_truth}\n",
    "        print(prediction)\n",
    "        print(sentence_str_example)\n",
    "        print(ground_truth)\n",
    "\n",
    "    for k, v in pred_dict.items():\n",
    "        prediction = v[\"prediction\"].split('[ACTIVATION]: ')[-1].strip()\n",
    "        pred_dict[k]['prediction_numeric'] = 1.0 if prediction == 'non-zero' else 0.0\n",
    "        pred_dict[k]['ground_truth_numeric'] = 1.0 if v['ground_truth'] > 0.0 else 0.0\n",
    "\n",
    "    # Get accuracy and f1 score\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in pred_dict.items():\n",
    "        if v['prediction_numeric'] == v['ground_truth_numeric']:\n",
    "            correct += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 1:\n",
    "            tp += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 0:\n",
    "            fp += 1\n",
    "        if v['prediction_numeric'] == 0 and v['ground_truth_numeric'] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    if tp + fp + fn > 0:\n",
    "        accuracy = correct / len(pred_dict)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "\n",
    "# Means\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat with just attn head component\n",
    "encoder_feature_pairs = [(z_saes[2], [24166])]\n",
    "top_tokens, top_logits = get_top_k_tokens(model, encoder_feature_pairs, k=20)\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = get_feature_scores_across_layers(model, encoder_feature_pairs, owt_tokens_torch[:8*1024], batch_size=128)\n",
    "\n",
    "examples_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores, owt_tokens_torch[:8*1024], k=10, display_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_prompt = get_opening_prompt(examples_clean_text, top_tokens)\n",
    "base_interp = get_response(opening_prompt)\n",
    "print(base_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each IOI example, predict the feature activation score on a given token\n",
    "accuracies, f1s = [], []\n",
    "for i in range(z_hidden.shape[0]):\n",
    "    sentence = dataset_prompts[i]\n",
    "    sentence_tokens = model.to_tokens(sentence).squeeze()\n",
    "    activations = z_hidden[i]\n",
    "\n",
    "    pred_dict = {}\n",
    "\n",
    "    threshold = 0.0\n",
    "    non_zero_indices = np.where(activations > threshold)[0].tolist()\n",
    "    # Add one before and after each non-zero index\n",
    "    positions = []\n",
    "    for i in non_zero_indices:\n",
    "        positions.extend([i-1, i, i+1])\n",
    "    positions = [x for x in positions if x >= 0 and x < len(sentence_tokens)]\n",
    "    print(positions)\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        # Radomly sample 3 positions\n",
    "        positions = np.random.choice(len(sentence_tokens), 3, replace=False)\n",
    "\n",
    "    for i in tqdm(positions):\n",
    "        sentence_str_example = model.to_string(sentence_tokens[:i+1])\n",
    "        sentence_tokens_example = sentence_tokens[:i+1]\n",
    "        ground_truth = activations[i].item()\n",
    "\n",
    "        # Get the prediction\n",
    "        prompt = follow_up_activation_prediction_prompt(base_interp, sentence_str_example)\n",
    "        prediction = get_response(prompt) #gen_openai_completion(prompt, visualize_stream=False)\n",
    "        pred_dict[i] = {\"prediction\": prediction, \"sentence\": sentence_str_example, \"ground_truth\": ground_truth}\n",
    "\n",
    "        print(prediction)\n",
    "        print(sentence_str_example)\n",
    "        print(ground_truth)\n",
    "\n",
    "    for k, v in pred_dict.items():\n",
    "        prediction = v[\"prediction\"].split('[ACTIVATION]: ')[-1].strip()\n",
    "        pred_dict[k]['prediction_numeric'] = 1.0 if prediction == 'non-zero' else 0.0\n",
    "        pred_dict[k]['ground_truth_numeric'] = 1.0 if v['ground_truth'] > 0.0 else 0.0\n",
    "\n",
    "    # Get accuracy and f1 score\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for k, v in pred_dict.items():\n",
    "        if v['prediction_numeric'] == v['ground_truth_numeric']:\n",
    "            correct += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 1:\n",
    "            tp += 1\n",
    "        if v['prediction_numeric'] == 1 and v['ground_truth_numeric'] == 0:\n",
    "            fp += 1\n",
    "        if v['prediction_numeric'] == 0 and v['ground_truth_numeric'] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    if tp + fp + fn > 0:\n",
    "        accuracy = correct / len(pred_dict)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "\n",
    "# Means\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"15404_L9H9_20546_MLP0\": {\n",
    "        \"co_occurrence\": {\n",
    "            \"accuracies\": 0.68,\n",
    "            \"f1s\": 0.54,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.6,\n",
    "            \"f1s\": 0.0,\n",
    "        }\n",
    "    },\n",
    "    \"16513_L8H6_10461_MLP0\": {\n",
    "        \"co_occurrence\": {\n",
    "            \"accuracies\": 0.82,\n",
    "            \"f1s\": 0.68,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.7,\n",
    "            \"f1s\": 0.5,\n",
    "        }\n",
    "    },\n",
    "    \"24166_L2H2_4522_MLP0\": {\n",
    "        \"co_occurrence\": {\n",
    "            \"accuracies\": 0.52,\n",
    "            \"f1s\": 0.55,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.6,\n",
    "            \"f1s\": 0.0\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "results_dict = {\n",
    "    \"15404_L9H9_20546_MLP0\": {\n",
    "        \"co_occurrence\": {\n",
    "            \"accuracies\": 0.68,\n",
    "            \"f1s\": 0.54,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.6,\n",
    "            \"f1s\": 0.01,\n",
    "        }\n",
    "    },\n",
    "    \"16513_L8H6_10461_MLP0\": {\n",
    "        \"co_occurrence\": {\n",
    "            \"accuracies\": 0.82,\n",
    "            \"f1s\": 0.68,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.7,\n",
    "            \"f1s\": 0.5,\n",
    "        }\n",
    "    },\n",
    "    \"24166_L2H2_4522_MLP0\": {\n",
    "        \"co_occurrence\": {\n",
    "            \"accuracies\": 0.62,\n",
    "            \"f1s\": 0.55,\n",
    "        },\n",
    "        \"base\": {\n",
    "            \"accuracies\": 0.6,\n",
    "            \"f1s\": 0.01\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_label(key):\n",
    "    feature1, layer_head, feature2, mlp = key.split(\"_\")\n",
    "    layer, head = layer_head[1:].split(\"H\")\n",
    "    return f\"{feature1} & {feature2} (L{layer}H{head} MLP{mlp[-1]})\"\n",
    "\n",
    "labels = [create_label(key) for key in results_dict.keys()]\n",
    "co_occurrence_accuracies = [results_dict[key][\"co_occurrence\"][\"accuracies\"] for key in results_dict.keys()]\n",
    "co_occurrence_f1s = [results_dict[key][\"co_occurrence\"][\"f1s\"] for key in results_dict.keys()]\n",
    "base_accuracies = [results_dict[key][\"base\"][\"accuracies\"] for key in results_dict.keys()]\n",
    "base_f1s = [results_dict[key][\"base\"][\"f1s\"] for key in results_dict.keys()]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Co-occurrence Accuracies', x=labels, y=co_occurrence_accuracies, marker_color='#ADD8E6'),  # Light blue\n",
    "    go.Bar(name='Co-occurrence F1s', x=labels, y=co_occurrence_f1s, marker_color='#00008B'),  # Dark blue\n",
    "    go.Bar(name='Base Accuracies', x=labels, y=base_accuracies, marker_color='#90EE90'),  # Light green\n",
    "    go.Bar(name='Base F1s', x=labels, y=base_f1s, marker_color='#006400')  # Dark green\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Co-occurrence vs. Base Performance',\n",
    "    xaxis_title='Features',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    legend_title_text='Metrics',\n",
    "    font=dict(size=14),\n",
    "    template='plotly_white',\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
