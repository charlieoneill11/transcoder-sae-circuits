{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from circuit_lens import CircuitLens\n",
    "from circuit_discovery import CircuitDiscovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When John and Mary went to the store, John gave the bag to Mary\"\n",
    "cd = CircuitDiscovery(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.build_directed_graph(contributors_per_node=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directed_graph_edges_with_weights(cd):\n",
    "    edges_with_weights = []\n",
    "    for receiver, contributors in cd.transformer_model.edge_tracker._reciever_to_contributors.items():\n",
    "        for contributor, weight in contributors.items():\n",
    "            edges_with_weights.append((contributor, receiver, weight))\n",
    "    return edges_with_weights\n",
    "\n",
    "# Access the edges with weights\n",
    "edges_with_weights = get_directed_graph_edges_with_weights(cd)\n",
    "print(len(edges_with_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique names (first thing in tuple) across edges with weights\n",
    "unique_names_src = set([x[0][0] for x in edges_with_weights])\n",
    "# Add unique names for trgt\n",
    "unique_names_trgt = set([x[1][0] for x in edges_with_weights])\n",
    "unique_names = unique_names_src.union(unique_names_trgt)\n",
    "print(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_connectivity_stats(edges_with_weights):\n",
    "    attn_to_attn = 0\n",
    "    attn_to_ff = 0\n",
    "    ff_to_attn = 0\n",
    "    ff_to_ff = 0\n",
    "    other = 0\n",
    "\n",
    "    for edge in edges_with_weights:\n",
    "        if edge[0][0] == \"attn_head\" and edge[1][0] == \"attn_head\":\n",
    "            attn_to_attn += 1\n",
    "        elif edge[0][0] == \"attn_head\" and edge[1][0] == \"mlp_feature\":\n",
    "            attn_to_ff += 1\n",
    "        elif edge[0][0] == \"mlp_feature\" and edge[1][0] == \"attn_head\":\n",
    "            ff_to_attn += 1\n",
    "        elif edge[0][0] == \"mlp_feature\" and edge[1][0] == \"mlp_feature\":\n",
    "            ff_to_ff += 1\n",
    "        else:\n",
    "            other += 1\n",
    "\n",
    "    print(f\"attn_to_attn: {attn_to_attn}\")\n",
    "    print(f\"attn_to_ff: {attn_to_ff}\")\n",
    "    print(f\"ff_to_attn: {ff_to_attn}\")\n",
    "    print(f\"ff_to_ff: {ff_to_ff}\")\n",
    "    print(f\"other: {other}\")\n",
    "\n",
    "print_connectivity_stats(edges_with_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "def filter_and_aggregate_edges(edges_with_weights):\n",
    "    filtered_edges = []\n",
    "    \n",
    "    # Filter edges and prepare the graph\n",
    "    graph = nx.DiGraph()\n",
    "    for src, dst, weight in edges_with_weights:\n",
    "        if (src[0] in ['attn_head', 'mlp_feature'] and dst[0] in ['attn_head', 'mlp_feature']):\n",
    "            #print(f\"Found a direct path between {src} and {dst} with weight {weight}\")\n",
    "            graph.add_edge(src, dst, weight=weight)\n",
    "    \n",
    "    # Aggregation dictionaries\n",
    "    attn_head_aggregation = defaultdict(lambda: defaultdict(list))\n",
    "    mlp_aggregation = defaultdict(list)\n",
    "    \n",
    "    # Aggregate contributions for attention heads and MLPs\n",
    "    for node in graph.nodes:\n",
    "        if node[0] == 'attn_head':\n",
    "            layer, head = node[1], node[2]\n",
    "            attn_head_aggregation[layer][head].append(node)\n",
    "        elif node[0] == 'mlp_feature':\n",
    "            layer = node[1]\n",
    "            mlp_aggregation[layer].append(node)\n",
    "    \n",
    "    # New graph to store aggregated edges\n",
    "    aggregated_graph = nx.DiGraph()\n",
    "    \n",
    "    # Aggregate the edges with at most one intermediate node\n",
    "    for node in graph.nodes:\n",
    "        for neighbor in graph.successors(node):\n",
    "            if graph[node][neighbor]['weight'] is not None:\n",
    "                aggregated_graph.add_edge(node, neighbor, weight=graph[node][neighbor]['weight'])\n",
    "            for next_neighbor in graph.successors(neighbor):\n",
    "                if graph[neighbor][next_neighbor]['weight'] is not None:\n",
    "                    combined_weight = graph[node][neighbor]['weight'] * graph[neighbor][next_neighbor]['weight']\n",
    "                    aggregated_graph.add_edge(node, next_neighbor, weight=combined_weight)\n",
    "    \n",
    "    return aggregated_graph.edges(data=True)\n",
    "\n",
    "aggregated_edges = filter_and_aggregate_edges(edges_with_weights)\n",
    "print(len(aggregated_edges))\n",
    "\n",
    "for edge in aggregated_edges:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through aggregated edges and only include information in tuples we care about\n",
    "cleaned_edges = []\n",
    "for src, dst, data in aggregated_edges:\n",
    "    # If src is an attention head\n",
    "    if src[0] == 'attn_head':\n",
    "        src = (src[0], src[1], src[2])\n",
    "    # If src is an MLP\n",
    "    elif src[0] == 'mlp_feature':\n",
    "        src = (src[0], src[1])\n",
    "    \n",
    "    # If dst is an attention head\n",
    "    if dst[0] == 'attn_head':\n",
    "        dst = (dst[0], dst[1], dst[2])\n",
    "    # If dst is an MLP\n",
    "    elif dst[0] == 'mlp_feature':\n",
    "        dst = (dst[0], dst[1])\n",
    "\n",
    "    cleaned_edges.append((src, dst, data['weight']))\n",
    "\n",
    "print(f\"Length of cleaned edges: {len(cleaned_edges)}\")\n",
    "for edge in cleaned_edges:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to sum the weights for duplicate src and trg nodes\n",
    "cleaned_edges_dict = {}\n",
    "for src, dst, weight in cleaned_edges:\n",
    "    if (src, dst) in cleaned_edges_dict:\n",
    "        cleaned_edges_dict[(src, dst)] += weight\n",
    "    else:\n",
    "        cleaned_edges_dict[(src, dst)] = weight\n",
    "\n",
    "# Print length of cleaned edges\n",
    "print(f\"Length of cleaned edges: {len(cleaned_edges_dict)}\")\n",
    "\n",
    "cleaned_edges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of times we have attn to attn\n",
    "attn_to_attn = 0\n",
    "attn_to_mlp = 0\n",
    "mlp_to_attn = 0\n",
    "mlp_to_mlp = 0\n",
    "\n",
    "for (src, dst), weight in cleaned_edges_dict.items():\n",
    "    if src[0] == 'attn_head' and dst[0] == 'attn_head':\n",
    "        attn_to_attn += 1\n",
    "    elif src[0] == 'attn_head' and dst[0] == 'mlp_feature':\n",
    "        attn_to_mlp += 1\n",
    "    elif src[0] == 'mlp_feature' and dst[0] == 'attn_head':\n",
    "        mlp_to_attn += 1\n",
    "    elif src[0] == 'mlp_feature' and dst[0] == 'mlp_feature':\n",
    "        mlp_to_mlp += 1\n",
    "\n",
    "print(f\"Attn to Attn: {attn_to_attn}\")\n",
    "print(f\"Attn to MLP: {attn_to_mlp}\")\n",
    "print(f\"MLP to Attn: {mlp_to_attn}\")\n",
    "print(f\"MLP to MLP: {mlp_to_mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy paths to build adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_discovery import CircuitDiscovery, CircuitDiscoveryHeadNode, CircuitDiscoveryRegularNode\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When John and Mary went to the store, John gave the bag to Mary\"\n",
    "cd = CircuitDiscovery(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume cd is an instance of the CircuitDiscovery class\n",
    "M = 100  # Number of times to call the greedy function\n",
    "k = 5  # Top k contributors at each step\n",
    "\n",
    "for _ in range(M):\n",
    "    cd.greedily_add_top_contributors(k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to collect nodes and edges\n",
    "def collect_nodes_and_edges(cd):\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "\n",
    "    def visit_node(node):\n",
    "        nodes.add(node.tuple_id)\n",
    "        if isinstance(node, CircuitDiscoveryRegularNode):\n",
    "            for contributor in node.contributors_in_graph:\n",
    "                edges.append((contributor.tuple_id, node.tuple_id, cd.transformer_model.edge_tracker._reciever_to_contributors[node.tuple_id][contributor.tuple_id]))\n",
    "        elif isinstance(node, CircuitDiscoveryHeadNode):\n",
    "            for head_type in [\"q\", \"k\", \"v\"]:\n",
    "                for contributor in node.contributors_in_graph(head_type):\n",
    "                    edges.append((contributor.tuple_id, node.tuple_id, cd.transformer_model.edge_tracker._reciever_to_contributors[node.tuple_id_for_head_type(head_type)][contributor.tuple_id]))\n",
    "\n",
    "    cd.traverse_graph(visit_node)\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "# Collect nodes and edges\n",
    "nodes, edges = collect_nodes_and_edges(cd)\n",
    "\n",
    "# Print the nodes and edges\n",
    "print(\"Nodes:\")\n",
    "for node in nodes:\n",
    "    print(node)\n",
    "\n",
    "print(\"\\nEdges (with weights):\")\n",
    "for edge in edges:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_edges = []\n",
    "for src, dst, weight in edges:\n",
    "    if src[0] == 'attn_head':\n",
    "        src = (src[0], src[1], src[2])\n",
    "    elif src[0] == 'mlp_feature':\n",
    "        src = (src[0], src[1])\n",
    "    \n",
    "    if dst[0] == 'attn_head':\n",
    "        dst = (dst[0], dst[1], dst[2])\n",
    "    elif dst[0] == 'mlp_feature':\n",
    "        dst = (dst[0], dst[1])\n",
    "\n",
    "    cleaned_edges.append((src, dst, weight))\n",
    "\n",
    "print(f\"Length of cleaned edges: {len(cleaned_edges)}\")\n",
    "for edge in cleaned_edges:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_keep_type(node):\n",
    "    \"\"\"Check if a node is a keep type (attention head or MLP feature).\"\"\"\n",
    "    return node[0] in {'attn_head', 'mlp_feature'}\n",
    "\n",
    "def get_layer(node):\n",
    "    \"\"\"Extract the layer number from a node.\"\"\"\n",
    "    return node[1]\n",
    "\n",
    "def find_paths(graph, start, path, paths, weight):\n",
    "    \"\"\"Recursively find all valid paths starting from the given node.\"\"\"\n",
    "    current_node = path[-1]\n",
    "    current_layer = get_layer(current_node)\n",
    "    current_type_is_mlp = current_node[0] == 'mlp_feature'\n",
    "\n",
    "    for edge in graph.get(current_node, []):\n",
    "        next_node, edge_weight = edge\n",
    "        next_layer = get_layer(next_node)\n",
    "        \n",
    "        # Skip if not strictly increasing layers\n",
    "        if next_layer <= current_layer:\n",
    "            continue\n",
    "        \n",
    "        # Ensure the correct alternating pattern\n",
    "        if (current_type_is_mlp and next_node[0] != 'attn_head') or (not current_type_is_mlp and next_node[0] != 'mlp_feature'):\n",
    "            continue\n",
    "        \n",
    "        new_weight = weight + edge_weight\n",
    "        \n",
    "        if is_keep_type(next_node):\n",
    "            # If the next node is a keep type, record the path and start a new path\n",
    "            paths.append((path + [next_node], start, next_node, new_weight))\n",
    "        else:\n",
    "            # Continue to search deeper\n",
    "            find_paths(graph, start, path + [next_node], paths, new_weight)\n",
    "\n",
    "def build_graph(edges):\n",
    "    \"\"\"Build a graph representation from the list of edges.\"\"\"\n",
    "    graph = {}\n",
    "    for src, dst, weight in edges:\n",
    "        if src not in graph:\n",
    "            graph[src] = []\n",
    "        graph[src].append((dst, weight))\n",
    "    return graph\n",
    "\n",
    "def aggregate_paths(paths):\n",
    "    \"\"\"Aggregate paths into edges with combined weights.\"\"\"\n",
    "    aggregated_edges = {}\n",
    "    for path, src, dst, weight in paths:\n",
    "        if (src, dst) not in aggregated_edges:\n",
    "            aggregated_edges[(src, dst)] = 0.0\n",
    "        aggregated_edges[(src, dst)] += weight\n",
    "    return aggregated_edges\n",
    "\n",
    "def filter_and_aggregate_edges(edges):\n",
    "    \"\"\"Main function to filter edges and aggregate paths.\"\"\"\n",
    "    # Step 1: Build the graph\n",
    "    graph = build_graph(edges)\n",
    "    \n",
    "    # Step 2: Find all valid paths\n",
    "    paths = []\n",
    "    for node in graph:\n",
    "        if is_keep_type(node):\n",
    "            find_paths(graph, node, [node], paths, 0.0)\n",
    "    \n",
    "    # Step 3: Aggregate paths into edges\n",
    "    aggregated_edges = aggregate_paths(paths)\n",
    "    \n",
    "    # Convert aggregated_edges to list format\n",
    "    final_edges = [(src, dst, weight) for (src, dst), weight in aggregated_edges.items()]\n",
    "    \n",
    "    return final_edges\n",
    "\n",
    "# Filter and aggregate edges\n",
    "final_edges = filter_and_aggregate_edges(cleaned_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max weight\n",
    "max_weight = 0\n",
    "max_edge = None\n",
    "for edge in final_edges:\n",
    "    if edge[2] > max_weight:\n",
    "        max_weight = edge[2]\n",
    "        max_edge = edge\n",
    "\n",
    "print(max_edge)\n",
    "print(max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_node_index(node):\n",
    "    \"\"\"Get the index of the node in the adjacency matrix.\"\"\"\n",
    "    if node[0] == 'attn_head':\n",
    "        return node[1] * 12 + node[2]\n",
    "    elif node[0] == 'mlp_feature':\n",
    "        return 144 + node[1]\n",
    "\n",
    "def create_labels(num_layers, num_heads_per_layer):\n",
    "    \"\"\"Create labels for the nodes in the adjacency matrix.\"\"\"\n",
    "    labels = []\n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads_per_layer):\n",
    "            labels.append(f'attn_head_{layer}_{head}')\n",
    "        labels.append(f'mlp_feature_{layer}')\n",
    "    return labels\n",
    "\n",
    "def build_adjacency_matrix(final_edges, num_layers, num_heads_per_layer):\n",
    "    \"\"\"Build the adjacency matrix from the final edges.\"\"\"\n",
    "    num_nodes = num_layers * (num_heads_per_layer + 1)\n",
    "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    label_to_index = {}\n",
    "    labels = create_labels(num_layers, num_heads_per_layer)\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        label_to_index[label] = idx\n",
    "\n",
    "    for src, dst, weight in final_edges:\n",
    "        src_label = f'{src[0]}_{src[1]}' if src[0] == 'mlp_feature' else f'{src[0]}_{src[1]}_{src[2]}'\n",
    "        dst_label = f'{dst[0]}_{dst[1]}' if dst[0] == 'mlp_feature' else f'{dst[0]}_{dst[1]}_{dst[2]}'\n",
    "        \n",
    "        src_idx = label_to_index[src_label]\n",
    "        dst_idx = label_to_index[dst_label]\n",
    "        \n",
    "        adj_matrix[src_idx, dst_idx] = weight\n",
    "\n",
    "    return adj_matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix, labels = build_adjacency_matrix(final_edges, 12, 12)\n",
    "\n",
    "# Print the adjacency matrix\n",
    "print(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly imshow for adjacency matrix\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(adjacency_matrix, zmax=1, color_continuous_scale='blues', width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_matrix_to_pred(adjacency_matrix):\n",
    "    # Go through the adjacency matrix and add up the total weights for each source\n",
    "    total_weights = np.sum(adjacency_matrix, axis=0)\n",
    "    # If zero set to -inf\n",
    "    total_weights[total_weights == 0] = -np.inf\n",
    "    # Normalise with softmax\n",
    "    total_weights = np.exp(total_weights) / np.sum(np.exp(total_weights))\n",
    "\n",
    "    # Go through total weights and remove every 13th element\n",
    "    y_pred = np.zeros(144)\n",
    "    for i in range(144):\n",
    "        if i % 13 != 0:\n",
    "            y_pred[i] = total_weights[i]\n",
    "\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the adjacency matrix and add up the total weights for each source\n",
    "total_weights = np.sum(adjacency_matrix, axis=1)\n",
    "# total_weights += np.sum(adjacency_matrix, axis=0)\n",
    "# If zero set to -inf\n",
    "total_weights[total_weights == 0] = -np.inf\n",
    "# Normalise with softmax\n",
    "total_weights = np.exp(total_weights) / np.sum(np.exp(total_weights))\n",
    "\n",
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through total weights and remove every 13th element\n",
    "y_pred = np.zeros(144)\n",
    "for i in range(144):\n",
    "    if i % 13 != 0:\n",
    "        y_pred[i] = total_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "\n",
    "IOI_GROUND_TRUTH_HEADS = IOI_GROUND_TRUTH_HEADS.flatten()\n",
    "\n",
    "IOI_GROUND_TRUTH_HEADS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOI_GROUND_TRUTH_HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(IOI_GROUND_TRUTH_HEADS, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cumulative adjacency matrix for a bunch of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.ioi_dataset import NAMES, SINGLE_TOKEN_NAMES, ABBA_TEMPLATES, gen_prompt_uniform, BABA_TEMPLATES, NOUNS_DICT, gen_templated_prompts\n",
    "from transformer_lens import HookedTransformer, utils, ActivationCache\n",
    "import torch\n",
    "\n",
    "\n",
    "p = gen_prompt_uniform(\n",
    "    [BABA_TEMPLATES[0], ABBA_TEMPLATES[0]], SINGLE_TOKEN_NAMES, NOUNS_DICT, 10, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of prompts\n",
    "prompts = []\n",
    "for prompt in p:\n",
    "    prompts.append(prompt['text'] + \" \" + prompt[\"IO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "agg_adj_matrix = np.zeros((156, 156))\n",
    "\n",
    "for prompt in tqdm(prompts):\n",
    "\n",
    "    cd = CircuitDiscovery(prompt=prompt)\n",
    "\n",
    "    # Assume cd is an instance of the CircuitDiscovery class\n",
    "    M = 5  # Number of times to call the greedy function\n",
    "    k = 3  # Top k contributors at each step\n",
    "\n",
    "    for _ in range(M):\n",
    "        cd.greedily_add_top_contributors(k=k)\n",
    "\n",
    "    nodes, edges = collect_nodes_and_edges(cd)\n",
    "\n",
    "    cleaned_edges = []\n",
    "    for src, dst, weight in edges:\n",
    "        if src[0] == 'attn_head':\n",
    "            src = (src[0], src[1], src[2])\n",
    "        elif src[0] == 'mlp_feature':\n",
    "            src = (src[0], src[1])\n",
    "        \n",
    "        if dst[0] == 'attn_head':\n",
    "            dst = (dst[0], dst[1], dst[2])\n",
    "        elif dst[0] == 'mlp_feature':\n",
    "            dst = (dst[0], dst[1])\n",
    "\n",
    "        cleaned_edges.append((src, dst, weight))\n",
    "\n",
    "    \n",
    "    # Filter and aggregate edges\n",
    "    final_edges = filter_and_aggregate_edges(cleaned_edges)\n",
    "\n",
    "    adjacency_matrix, labels = build_adjacency_matrix(final_edges, 12, 12)\n",
    "\n",
    "    # add to agg_adj_matrix\n",
    "    agg_adj_matrix += adjacency_matrix\n",
    "\n",
    "    del adjacency_matrix\n",
    "    del labels\n",
    "    del cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow agg matrix\n",
    "fig = px.imshow(agg_adj_matrix, zmax=1, color_continuous_scale='blues', \n",
    "                labels={'x': 'Destination', 'y': 'Source'}, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_matrix_to_pred(adjacency_matrix):\n",
    "    # Go through the adjacency matrix and add up the total weights for each source\n",
    "    total_weights = np.sum(adjacency_matrix, axis=1)\n",
    "    total_weights += np.sum(adjacency_matrix, axis=0)\n",
    "    # If zero set to -inf\n",
    "    total_weights[total_weights == 0] = -np.inf\n",
    "    # Normalise with softmax\n",
    "    total_weights = np.exp(total_weights) / np.sum(np.exp(total_weights))\n",
    "\n",
    "    # Go through total weights and remove every 13th element\n",
    "    y_pred = np.zeros(144)\n",
    "    for i in range(144):\n",
    "        if i % 13 != 0:\n",
    "            y_pred[i] = total_weights[i]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "y_pred = adj_matrix_to_pred(agg_adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = roc_auc_score(IOI_GROUND_TRUTH_HEADS, y_pred)\n",
    "print(f\"ROC AUC: {roc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from data.greater_than_dataset import generate_greater_than_dataset\n",
    "from circuit_discovery import CircuitDiscovery, only_feature\n",
    "from circuit_lens import CircuitComponent\n",
    "from plotly_utils import *\n",
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "from data.greater_than_dataset import GT_GROUND_TRUTH_HEADS\n",
    "from memory import get_gpu_memory\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import get_attn_head_roc\n",
    "\n",
    "\n",
    "# %%\n",
    "torch.set_grad_enabled(False)\n",
    "# %%\n",
    "#dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "\n",
    "\n",
    "dataset_prompts = generate_greater_than_dataset(N=100)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def component_filter(component: str):\n",
    "    return component in [\n",
    "        CircuitComponent.Z_FEATURE,\n",
    "        CircuitComponent.MLP_FEATURE,\n",
    "        CircuitComponent.ATTN_HEAD,\n",
    "        CircuitComponent.UNEMBED,\n",
    "        # CircuitComponent.UNEMBED_AT_TOKEN,\n",
    "        CircuitComponent.EMBED,\n",
    "        CircuitComponent.POS_EMBED,\n",
    "        # CircuitComponent.BIAS_O,\n",
    "        CircuitComponent.Z_SAE_ERROR,\n",
    "        # CircuitComponent.Z_SAE_BIAS,\n",
    "        # CircuitComponent.TRANSCODER_ERROR,\n",
    "        # CircuitComponent.TRANSCODER_BIAS,\n",
    "    ]\n",
    "\n",
    "\n",
    "pass_based = True\n",
    "\n",
    "passes = 5\n",
    "node_contributors = 1\n",
    "first_pass_minimal = True\n",
    "\n",
    "sub_passes = 3\n",
    "do_sub_pass = False\n",
    "layer_thres = 9\n",
    "minimal = True\n",
    "\n",
    "\n",
    "num_greedy_passes = 20\n",
    "k = 1\n",
    "N = 30\n",
    "\n",
    "thres = 4\n",
    "\n",
    "def strategy(cd: CircuitDiscovery):\n",
    "    if pass_based:\n",
    "        for _ in range(passes):\n",
    "            cd.add_greedy_pass(contributors_per_node=node_contributors, minimal=first_pass_minimal)\n",
    "\n",
    "            if do_sub_pass:\n",
    "                for _ in range(sub_passes):\n",
    "                    cd.add_greedy_pass_against_all_existing_nodes(contributors_per_node=node_contributors, skip_z_features=True, layer_threshold=layer_thres, minimal=minimal)\n",
    "    else:\n",
    "        for _ in range(num_greedy_passes):\n",
    "            cd.greedily_add_top_contributors(k=k, reciever_threshold=thres)\n",
    "\n",
    "\n",
    "\n",
    "task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "N = 100\n",
    "\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "ground_truth = GT_GROUND_TRUTH_HEADS #IOI_GROUND_TRUTH_HEADS\n",
    "\n",
    "# fp, tp, thresh = get_attn_head_roc(ground_truth, a.flatten().softmax(dim=-1), \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")\n",
    "score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"GT\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "dataset_prompts = generate_greater_than_dataset(N=100)\n",
    "N_list = [1, 5, 10, 50, 100]\n",
    "roc_scores_gt = []\n",
    "for N in tqdm(N_list):\n",
    "\n",
    "    task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "    cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "    attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True)\n",
    "    ground_truth = GT_GROUND_TRUTH_HEADS #IOI_GROUND_TRUTH_HEADS\n",
    "    score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"GT\", visualize=False, additional_title=\"(No Counterfactuals)\")\n",
    "    roc_scores_gt.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=100)\n",
    "N_list = [1, 5, 10, 50, 100]\n",
    "roc_scores_ioi = []\n",
    "for N in tqdm(N_list):\n",
    "\n",
    "    task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "    cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "    attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True)\n",
    "    ground_truth = IOI_GROUND_TRUTH_HEADS\n",
    "    score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"GT\", visualize=False, additional_title=\"(No Counterfactuals)\")\n",
    "    roc_scores_ioi.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "trace_ioi = go.Scatter(x=N_list, y=roc_scores_ioi, mode='lines', name='IOI')\n",
    "trace_gt = go.Scatter(x=N_list, y=roc_scores, mode='lines', name='GT')\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(xaxis_title='No. examples', yaxis_title='ROC', width=600)\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=[trace_ioi, trace_gt], layout=layout)\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly line plot for ROC scores\n",
    "fig = px.line(x=N_list[:-1], y=roc_scores, labels={'x': 'No. examples', 'y': 'ROC AUC Score'}, width=600)\n",
    "# \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from data.greater_than_dataset import generate_greater_than_dataset\n",
    "from circuit_discovery import CircuitDiscovery, only_feature\n",
    "from circuit_lens import CircuitComponent\n",
    "from plotly_utils import *\n",
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "# from data.ioi_dataset import GT_GROUND_TRUTH_HEADS\n",
    "from memory import get_gpu_memory\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import get_attn_head_roc\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %%\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# %%\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "\n",
    "# dataset_prompts = generate_greater_than_dataset(N=100)\n",
    "\n",
    "# %%\n",
    "def component_filter(component: str):\n",
    "    return component in [\n",
    "        CircuitComponent.Z_FEATURE,\n",
    "        CircuitComponent.MLP_FEATURE,\n",
    "        CircuitComponent.ATTN_HEAD,\n",
    "        CircuitComponent.UNEMBED,\n",
    "        # CircuitComponent.UNEMBED_AT_TOKEN,\n",
    "        CircuitComponent.EMBED,\n",
    "        CircuitComponent.POS_EMBED,\n",
    "        # CircuitComponent.BIAS_O,\n",
    "        CircuitComponent.Z_SAE_ERROR,\n",
    "        # CircuitComponent.Z_SAE_BIAS,\n",
    "        # CircuitComponent.TRANSCODER_ERROR,\n",
    "        # CircuitComponent.TRANSCODER_BIAS,\n",
    "    ]\n",
    "\n",
    "pass_based = True\n",
    "\n",
    "passes = 5\n",
    "node_contributors = 1\n",
    "first_pass_minimal = True\n",
    "\n",
    "sub_passes = 3\n",
    "do_sub_pass = False\n",
    "layer_thres = 9\n",
    "minimal = True\n",
    "\n",
    "num_greedy_passes = 20\n",
    "k = 1\n",
    "N = 30\n",
    "\n",
    "thres = 4\n",
    "\n",
    "def strategy(cd: CircuitDiscovery):\n",
    "    if pass_based:\n",
    "        for _ in range(passes):\n",
    "            cd.add_greedy_pass(contributors_per_node=node_contributors, minimal=first_pass_minimal)\n",
    "\n",
    "            if do_sub_pass:\n",
    "                for _ in range(sub_passes):\n",
    "                    cd.add_greedy_pass_against_all_existing_nodes(contributors_per_node=node_contributors, skip_z_features=True, layer_threshold=layer_thres, minimal=minimal)\n",
    "    else:\n",
    "        for _ in range(num_greedy_passes):\n",
    "            cd.greedily_add_top_contributors(k=k, reciever_threshold=thres)\n",
    "\n",
    "task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "N = 5\n",
    "\n",
    "attn_freqs = task_eval.get_weighted_attn_head_freqs_over_dataset(N=N, visualize=True, return_freqs=True)\n",
    "\n",
    "print(attn_freqs.shape)\n",
    "\n",
    "# Softmax across row (do not flatten, apply softmax across rows of matrix)\n",
    "# attn_freqs = attn_freqs.softmax(dim=-1)\n",
    "\n",
    "# %%\n",
    "ground_truth = IOI_GROUND_TRUTH_HEADS\n",
    "\n",
    "# fp, tp, thresh = get_attn_head_roc(ground_truth, a.flatten().softmax(dim=-1), \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")\n",
    "score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(attn_freqs, zmax=1, color_continuous_scale='blues', width=600).show()\n",
    "px.imshow(ground_truth, zmax=1, color_continuous_scale='blues', width=600).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
