{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from circuit_discovery import CircuitDiscovery\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from data.greater_than_dataset import generate_greater_than_dataset\n",
    "from circuit_discovery import CircuitDiscovery, only_feature\n",
    "from circuit_lens import CircuitComponent\n",
    "from plotly_utils import *\n",
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "from data.greater_than_dataset import GT_GROUND_TRUTH_HEADS\n",
    "from memory import get_gpu_memory\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import get_attn_head_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "\n",
    "def component_filter(component: str):\n",
    "    return component in [\n",
    "        CircuitComponent.Z_FEATURE,\n",
    "        CircuitComponent.MLP_FEATURE,\n",
    "        CircuitComponent.ATTN_HEAD,\n",
    "        CircuitComponent.UNEMBED,\n",
    "        # CircuitComponent.UNEMBED_AT_TOKEN,\n",
    "        CircuitComponent.EMBED,\n",
    "        CircuitComponent.POS_EMBED,\n",
    "        # CircuitComponent.BIAS_O,\n",
    "        CircuitComponent.Z_SAE_ERROR,\n",
    "        # CircuitComponent.Z_SAE_BIAS,\n",
    "        # CircuitComponent.TRANSCODER_ERROR,\n",
    "        # CircuitComponent.TRANSCODER_BIAS,\n",
    "    ]\n",
    "\n",
    "\n",
    "pass_based = True\n",
    "\n",
    "passes = 5\n",
    "node_contributors = 1\n",
    "first_pass_minimal = True\n",
    "\n",
    "sub_passes = 3\n",
    "do_sub_pass = False\n",
    "layer_thres = 9\n",
    "minimal = True\n",
    "\n",
    "\n",
    "num_greedy_passes = 20\n",
    "k = 1\n",
    "N = 30\n",
    "\n",
    "thres = 4\n",
    "\n",
    "def strategy(cd: CircuitDiscovery):\n",
    "    if pass_based:\n",
    "        for _ in range(passes):\n",
    "            cd.add_greedy_pass(contributors_per_node=node_contributors, minimal=first_pass_minimal)\n",
    "\n",
    "            if do_sub_pass:\n",
    "                for _ in range(sub_passes):\n",
    "                    cd.add_greedy_pass_against_all_existing_nodes(contributors_per_node=node_contributors, skip_z_features=True, layer_threshold=layer_thres, minimal=minimal)\n",
    "    else:\n",
    "        for _ in range(num_greedy_passes):\n",
    "            cd.greedily_add_top_contributors(k=k, reciever_threshold=thres)\n",
    "\n",
    "\n",
    "\n",
    "task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "N = 20\n",
    "\n",
    "features_for_heads = task_eval.get_features_at_heads_over_dataset(N=N, use_set=False)\n",
    "features_for_mlps = task_eval.get_features_at_mlps_over_dataset(N=N, use_set=False)\n",
    "mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True, visualize=False)\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_heads = task_eval.get_features_at_heads_over_dataset(N=N, use_set=False)\n",
    "features_for_mlps = task_eval.get_features_at_mlps_over_dataset(N=N, use_set=False)\n",
    "mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True, visualize=False)\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True, visualize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = IOI_GROUND_TRUTH_HEADS\n",
    "\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True)\n",
    "score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"GT\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CircuitPrediction:\n",
    "\n",
    "    def __init__(self, attn_freqs, mlp_freqs, features_for_heads, features_for_mlps):\n",
    "        self.attn_freqs = attn_freqs\n",
    "        self.mlp_freqs = mlp_freqs\n",
    "        self.features_for_heads = features_for_heads\n",
    "        self.features_for_mlps = features_for_mlps\n",
    "\n",
    "        self.component_labels = self.get_component_labels()\n",
    "\n",
    "        self.circuit_hypergraph = self.create_circuit_hypergraph()\n",
    "\n",
    "    def create_circuit_hypergraph(self):\n",
    "        \"\"\" \n",
    "        Creates the dictionary for each component with frequency of occurrence and list of features.\n",
    "        \"\"\"\n",
    "        # Create circuit hypergraph with keys being the component labels\n",
    "        circuit_hypergraph = {label: {\"freq\": 0, \"features\": []} for label in self.component_labels}\n",
    "\n",
    "        # Add attention heads\n",
    "        for layer, freq in enumerate(self.attn_freqs):\n",
    "            for head, freq_head in enumerate(freq):\n",
    "                label = f\"L{layer}_H{head}\"\n",
    "                circuit_hypergraph[label][\"freq\"] += freq_head.item()\n",
    "                circuit_hypergraph[label][\"features\"].extend(self.features_for_heads[layer][head])\n",
    "\n",
    "        # Add MLPs\n",
    "        for i, freq in enumerate(self.mlp_freqs):\n",
    "            label = f\"MLP{i}\"\n",
    "            circuit_hypergraph[label][\"freq\"] += freq.item()\n",
    "            circuit_hypergraph[label][\"features\"].extend(self.features_for_mlps[i])\n",
    "\n",
    "        return circuit_hypergraph\n",
    "            \n",
    "    def get_component_labels(self):\n",
    "        # Head labels\n",
    "        head_labels = [f\"L{layer}_H{head}\" for layer in range(12) for head in range(12)]\n",
    "\n",
    "        # MLP labels\n",
    "        mlp_labels = [f\"MLP{i}\" for i in range(12)]\n",
    "\n",
    "        # After every 12 head labels, insert the next MLP label\n",
    "        labels = []\n",
    "        for i in range(12):\n",
    "            labels.extend(head_labels[i*12:(i+1)*12])\n",
    "            labels.append(mlp_labels[i])\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_all_features_from_attn_layer(self, layer: int):\n",
    "        \"\"\"\n",
    "        Returns all features in the circuit hypergraph from a given attention layer.\n",
    "        \"\"\" \n",
    "        features = []\n",
    "        for head in range(12):\n",
    "            features.extend(self.features_for_heads[layer][head])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_circuit_at_threshold(self, threshold: float, visualize: bool = False):\n",
    "        \"\"\"\n",
    "        Selects attention heads and MLPs whose frequency is above the threshold.\n",
    "        \"\"\"\n",
    "        circuit = np.zeros(len(self.component_labels))\n",
    "\n",
    "        for i, label in enumerate(self.component_labels):\n",
    "            if self.circuit_hypergraph[label][\"freq\"] > threshold:\n",
    "                circuit[i] = 1\n",
    "\n",
    "        if visualize:\n",
    "            self.visualize_circuit(circuit)\n",
    "\n",
    "        return circuit\n",
    "    \n",
    "    def visualize_circuit(self, circuit: torch.Tensor, additional_title=\"\"):\n",
    "        \"\"\"\n",
    "        Visualizes the circuit.\n",
    "        \"\"\"\n",
    "        # Circuit array\n",
    "        circuit_array = np.zeros((12, 13))\n",
    "\n",
    "        # Labels are A1, ..., A12, MLP\n",
    "        labels = [f\"A{i}\" for i in range(1, 13)] + [\"MLP\"]\n",
    "\n",
    "        # Fill in the circuit array\n",
    "        for i, pred in enumerate(circuit):\n",
    "            layer = i // 13\n",
    "            head = i % 13\n",
    "\n",
    "            circuit_array[layer, head] = pred\n",
    "\n",
    "        # Create the figure with plotly imshow\n",
    "        fig = px.imshow(circuit_array, labels=dict(x=\"Attention Head\", y=\"Layer\"), width=500,\n",
    "                        title=additional_title,\n",
    "                        x=labels, y=[x for x in range(12)], color_continuous_scale=\"blues\")\n",
    "        fig.show()\n",
    "\n",
    "    def component_frequency_array(self, visualize: bool = False):\n",
    "        \"\"\"\n",
    "        Returns the frequency of each component in the circuit.\n",
    "        \"\"\"\n",
    "        frequency_array = np.zeros((12, 13))\n",
    "\n",
    "        # Labels are A1, ..., A12, MLP\n",
    "        labels = [f\"A{i}\" for i in range(1, 13)] + [\"MLP\"]\n",
    "\n",
    "        # Fill in the frequency by looking at the circuit hypergraph\n",
    "        for i, label in enumerate(self.component_labels):\n",
    "            layer = i // 13\n",
    "            head = i % 13\n",
    "\n",
    "            frequency_array[layer, head] = self.circuit_hypergraph[label][\"freq\"]\n",
    "\n",
    "        if visualize:\n",
    "            fig = px.imshow(frequency_array, labels=dict(x=\"Attention Head\", y=\"Layer\"), width=450,\n",
    "                        title=\"Frequency of components\",\n",
    "                        x=labels, y=[x for x in range(12)], color_continuous_scale=\"blues\")\n",
    "            fig.show()\n",
    "\n",
    "        return frequency_array\n",
    "\n",
    "\n",
    "    def unique_feature_array(self, visualize: bool = False):\n",
    "        \"\"\"\n",
    "        Returns the unique features for each component in the circuit.\n",
    "        \"\"\" \n",
    "        unique_features_array = np.zeros((12, 13))\n",
    "\n",
    "        # Labels are A1, ..., A12, MLP\n",
    "        labels = [f\"A{i}\" for i in range(1, 13)] + [\"MLP\"]\n",
    "\n",
    "        # Fill in the num unique features by looking at the circuit hypergraph\n",
    "        for i, label in enumerate(self.component_labels):\n",
    "            layer = i // 13\n",
    "            head = i % 13\n",
    "\n",
    "            unique_features_array[layer, head] = len(set(self.circuit_hypergraph[label][\"features\"]))\n",
    "\n",
    "        if visualize:\n",
    "            fig = px.imshow(unique_features_array, labels=dict(x=\"Attention Head\", y=\"Layer\"), width=450,\n",
    "                        title=\"Unique features\",\n",
    "                        x=labels, y=[x for x in range(12)], color_continuous_scale=\"blues\")\n",
    "            fig.show()\n",
    "\n",
    "        return unique_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CircuitPrediction(attn_freqs, mlp_freqs, features_for_heads, features_for_mlps)\n",
    "_ = cp.unique_feature_array(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_feature_array = cp.unique_feature_array(visualize=False)[:, :-1]\n",
    "ground_truth = IOI_GROUND_TRUTH_HEADS.numpy()\n",
    "unique_feature_array.shape, ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Flatten both\n",
    "unique_feature_array = unique_feature_array.flatten()\n",
    "ground_truth = ground_truth.flatten()\n",
    "\n",
    "roc_auc_score(ground_truth, unique_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual frequency array\n",
    "freq_array = cp.component_frequency_array(visualize=False)[:, :-1]\n",
    "freq_array = torch.tensor(freq_array).flatten().softmax(dim=-1).numpy()\n",
    "print(freq_array.shape)\n",
    "\n",
    "roc_auc_score(ground_truth, freq_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_feature_array = unique_feature_array * freq_array\n",
    "\n",
    "roc_auc_score(ground_truth, prop_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _, _ = get_attn_head_roc(ground_truth, prop_feature_array, \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _, _ = get_attn_head_roc(ground_truth, unique_feature_array, \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _, _ = get_attn_head_roc(ground_truth, freq_array, \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autointerpretability\n",
    "\n",
    "Given a component (i.e. L5H5 or MLP7) and a list of features for that component, we want to use `CircuitLens` to find the max-activating examples for each feature. We will then feed these features to a language model and go through the autointerpretability pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from circuit_lens import CircuitLens\n",
    "from circuit_lens import ComponentLens\n",
    "from torch import Tensor\n",
    "\n",
    "prompt = dataset_prompts[0]['text'] + dataset_prompts[0]['correct']\n",
    "circuit_lens = CircuitLens(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_feature_activation(circuit_lens, seq_index: int, layer: int, feature: int):\n",
    "    # retrieve the active features for the given sequence index\n",
    "    active_features = circuit_lens.get_active_features(seq_index)\n",
    "    \n",
    "    # get the starting index for the MLP features in the specified layer\n",
    "    start_index = active_features.get_mlp_start_index(layer)\n",
    "    \n",
    "    # get the number of MLP features in this layer\n",
    "    num_mlp_features = active_features.keys[layer]['mlp']\n",
    "    \n",
    "    # extract the MLP features and their corresponding values\n",
    "    mlp_features = active_features.features[start_index:start_index + num_mlp_features]\n",
    "    mlp_values = active_features.values[start_index:start_index + num_mlp_features]\n",
    "    \n",
    "    # check if the specified feature is active and return its value\n",
    "    indices = (mlp_features == feature).nonzero(as_tuple=True)[0]\n",
    "    if len(indices) > 0: return mlp_values[indices[0]].item()\n",
    "    \n",
    "    # if the feature is not active, return 0\n",
    "    return 0.0\n",
    "\n",
    "# Example usage\n",
    "layer = 7\n",
    "seq_index = -2\n",
    "feature = 1414\n",
    "activation = get_mlp_feature_activation(circuit_lens, seq_index, layer, feature)\n",
    "print(f\"Activation of feature {feature} in MLP layer {layer}: {activation}\")\n",
    "# for feature in range(24576):\n",
    "#     #feature = 100\n",
    "#     activation = get_mlp_feature_activation(circuit_lens, seq_index, layer, feature)\n",
    "#     if activation > 0:\n",
    "#         print(f\"Activation of feature {feature} in MLP layer {layer}: {activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_head_activation(circuit_lens, seq_index: int, layer: int, feature: int):\n",
    "    # Retrieve the active features for the given sequence index\n",
    "    active_features = circuit_lens.get_active_features(seq_index)\n",
    "\n",
    "    # Delete active features vectors\n",
    "    #del active_features.vectors\n",
    "    print(active_features)\n",
    "    \n",
    "    # Get the starting index for the attention features in the specified layer\n",
    "    start_index = active_features.get_attn_start_index(layer)\n",
    "    \n",
    "    # Get the number of attention features in this layer\n",
    "    num_attn_features = active_features.keys[layer]['attn']\n",
    "    \n",
    "    # Extract the attention features and their corresponding values\n",
    "    attn_features = active_features.features[start_index:start_index + num_attn_features]\n",
    "    attn_values = active_features.values[start_index:start_index + num_attn_features]\n",
    "    \n",
    "    # Check if the specified feature is active and return its value\n",
    "    indices = (attn_features == feature).nonzero(as_tuple=True)[0]\n",
    "    if len(indices) > 0: return attn_values[indices[0]].item()\n",
    "\n",
    "    # If the feature is not active, return 0\n",
    "    return 0.0\n",
    "\n",
    "# Example usage\n",
    "layer = 5\n",
    "seq_index = -1\n",
    "feature = 44256\n",
    "activation = get_attention_head_activation(circuit_lens, seq_index, layer, feature)\n",
    "print(f\"Activation of feature {feature} in attention layer {layer}: {activation}\")\n",
    "# for feature in range(24576):\n",
    "#     activation = get_attention_head_activation(circuit_lens, seq_index, layer, feature)\n",
    "#     if activation > 0:\n",
    "#         print(f\"Activation of feature {feature} in attention layer {layer}: {activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CircuitPrediction(attn_freqs, mlp_freqs, features_for_heads, features_for_mlps)\n",
    "layer_5_features = [x for x in list(set(cp.get_all_features_from_attn_layer(5))) if x != -1]\n",
    "layer_5_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Then, Jose and Eric had a lot of fun at the store. Eric gave a computer to Jose'\n",
    "circuit_lens = CircuitLens(prompt=prompt)\n",
    "\n",
    "layer = 5\n",
    "feats = [x for x in list(set(cp.get_all_features_from_attn_layer(5))) if x != -1]\n",
    "\n",
    "feat_act_dict = {}\n",
    "for feat in feats:\n",
    "    act = get_attention_head_activation(circuit_lens, -1, layer, feat)\n",
    "    feat_act_dict[feat] = act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_act_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so how we are going to do this? What do we need to be able to do?\n",
    "* Specify a list of components, and a list of features for each component.\n",
    "* Over some large corpus of tokens (probably about 1 mill), cache the active features (this is good - Danny has already set up the infrastructure for this)!\n",
    "* Since we have saved these active features in a principled way, they don't take up much space and also they allow us to just lookup the max-activating examples for a specific feature in a specific layer.\n",
    "* We need to save the active features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_lens import ActiveFeatures\n",
    "\n",
    "circuit_lens = CircuitLens(prompt=prompt)\n",
    "active_features = circuit_lens.get_active_features(-2)\n",
    "values = active_features.values\n",
    "features = active_features.features\n",
    "keys = active_features.keys\n",
    "print(values.shape, features.shape)\n",
    "print(keys)\n",
    "\n",
    "test_af = ActiveFeatures(vectors=[], values=values, features=features, keys=keys)\n",
    "test_af.get_attn_start_index(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the starting index for the attention features in the specified layer\n",
    "start_index = active_features.get_attn_start_index(layer)\n",
    "\n",
    "# Get the number of attention features in this layer\n",
    "layer = 5\n",
    "num_attn_features = active_features.keys[layer]['attn']\n",
    "\n",
    "# Extract the attention features and their corresponding values\n",
    "attn_features = active_features.features[start_index:start_index + num_attn_features]\n",
    "attn_values = active_features.values[start_index:start_index + num_attn_features]\n",
    "\n",
    "attn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_lens = circuit_lens.get_head_seq_lens_for_z_feature(layer=layer, seq_index=-2, feature=attn_features[0], visualize=False, k=50)\n",
    "head_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_lens[0][0].run_data['head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert the head_lens into a list of tuples\n",
    "# Each tuple is (head, contribution) to that specific feature\n",
    "head_lens_list = [(head[0].run_data['head'], head[1]) for head in head_lens]\n",
    "# Sum the contributions of each head\n",
    "head_contributions = {}\n",
    "for head, contribution in head_lens_list:\n",
    "    if head in head_contributions:\n",
    "        head_contributions[head] += contribution\n",
    "    else:\n",
    "        head_contributions[head] = contribution\n",
    "head_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Then, Jose and Eric had a lot of fun at the store. Eric gave a computer to Jose'\n",
    "circuit_lens = CircuitLens(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_lens import ActiveFeatures\n",
    "\n",
    "\n",
    "def prompt_to_active_features(circuit_lens: CircuitLens, seq_pos: int):\n",
    "\n",
    "    active_features = circuit_lens.get_active_features(seq_pos)\n",
    "    values = active_features.values\n",
    "    features = active_features.features\n",
    "    keys = active_features.keys\n",
    "\n",
    "    head_feature_contributions = {}\n",
    "\n",
    "    for layer in range(12):\n",
    "        # Get the starting index for the attention features in the specified layer\n",
    "        start_index = active_features.get_attn_start_index(layer)\n",
    "\n",
    "        # Get the number of attention features in this layer\n",
    "        num_attn_features = active_features.keys[layer]['attn']\n",
    "\n",
    "        # Extract the attention features and their corresponding values\n",
    "        attn_features = active_features.features[start_index:start_index + num_attn_features]\n",
    "        \n",
    "        # Get the unique features for the attention heads in this layer\n",
    "\n",
    "        layer_head_feature_contributions = {}\n",
    "\n",
    "        for attn_feature in set(attn_features):\n",
    "            if attn_feature == -1: continue\n",
    "            head_lens = circuit_lens.get_head_seq_lens_for_z_feature(layer=layer, seq_index=seq_pos, feature=attn_feature, visualize=False, k=50)\n",
    "            head_lens_list = [(head[0].run_data['head'], head[1]) for head in head_lens]\n",
    "            head_contributions = {}\n",
    "            for head, contribution in head_lens_list:\n",
    "                if head in head_contributions:\n",
    "                    head_contributions[head] += contribution\n",
    "                else:\n",
    "                    head_contributions[head] = contribution\n",
    "            layer_head_feature_contributions[attn_feature.item()] = head_contributions\n",
    "\n",
    "        head_feature_contributions[layer] = layer_head_feature_contributions\n",
    "\n",
    "    del active_features\n",
    "\n",
    "    # Final dictionary to save\n",
    "    prompt_dict = {\n",
    "        'prompt': prompt,\n",
    "        'values': values,\n",
    "        'features': features,\n",
    "        'keys': keys,\n",
    "        'head_feature_contributions': head_feature_contributions\n",
    "    }\n",
    "\n",
    "    return prompt_dict\n",
    "\n",
    "prompt_dict = prompt_to_active_features(circuit_lens, -2)\n",
    "prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 1\n",
    "# Load the transformer model and activation store\n",
    "hook_point = \"blocks.8.hook_resid_pre\" # this doesn't matter\n",
    "saes, _ = get_gpt2_res_jb_saes(hook_point)\n",
    "sparse_autoencoder = saes[hook_point]\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "sparse_autoencoder.cfg.hook_point = f\"blocks.{layer}.attn.hook_z\"\n",
    "sparse_autoencoder.cfg.store_batch_size = batch_size\n",
    "\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "\n",
    "print(f\"Loader cfg batch size = {sparse_autoencoder.cfg.store_batch_size} (batch size = {batch_size})\")\n",
    "\n",
    "# don't overwrite the sparse autoencoder with the loader's sae (newly initialized)\n",
    "tl_model, _, activation_store = loader.load_sae_training_group_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = activation_store.get_batch_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 1_000\n",
    "seq_len = 128\n",
    "\n",
    "all_active_features = []\n",
    "\n",
    "for i in range(total_tokens // seq_len):\n",
    "    tokens = activation_store.get_batch_tokens()\n",
    "    prompt = tl_model.to_string(tokens)\n",
    "\n",
    "    for seq_pos in trange(seq_len-1):\n",
    "\n",
    "        circuit_lens = CircuitLens(prompt=prompt)\n",
    "\n",
    "        prompt_dict = prompt_to_active_features(circuit_lens, seq_pos)\n",
    "\n",
    "        all_active_features.append(prompt_dict)\n",
    "\n",
    "        del circuit_lens\n",
    "        del prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach - storing ZSAE and transcoder activations directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "def tokenize_and_concatenate(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    streaming = False,\n",
    "    max_length = 1024,\n",
    "    column_name = \"text\",\n",
    "    add_bos_token = True,\n",
    "):\n",
    "    \"\"\"Helper function to tokenizer and concatenate a dataset of text. This converts the text to tokens, concatenates them (separated by EOS tokens) and then reshapes them into a 2D array of shape (____, sequence_length), dropping the last batch. Tokenizers are much faster if parallelised, so we chop the string into 20, feed it into the tokenizer, in parallel with padding, then remove padding at the end.\n",
    "\n",
    "    This tokenization is useful for training language models, as it allows us to efficiently train on a large corpus of text of varying lengths (without, eg, a lot of truncation or padding). Further, for models with absolute positional encodings, this avoids privileging early tokens (eg, news articles often begin with CNN, and models may learn to use early positional encodings to predict these)\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to tokenize, assumed to be a HuggingFace text dataset.\n",
    "        tokenizer (AutoTokenizer): The tokenizer. Assumed to have a bos_token_id and an eos_token_id.\n",
    "        streaming (bool, optional): Whether the dataset is being streamed. If True, avoids using parallelism. Defaults to False.\n",
    "        max_length (int, optional): The length of the context window of the sequence. Defaults to 1024.\n",
    "        column_name (str, optional): The name of the text column in the dataset. Defaults to 'text'.\n",
    "        add_bos_token (bool, optional): . Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Returns the tokenized dataset, as a dataset of tensors, with a single column called \"tokens\"\n",
    "\n",
    "    Note: There is a bug when inputting very small datasets (eg, <1 batch per process) where it just outputs nothing. I'm not super sure why\n",
    "    \"\"\"\n",
    "    for key in dataset.features:\n",
    "        if key != column_name:\n",
    "            dataset = dataset.remove_columns(key)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        # We add a padding token, purely to implement the tokenizer. This will be removed before inputting tokens to the model, so we do not need to increment d_vocab in the model.\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    # Define the length to chop things up into - leaving space for a bos_token if required\n",
    "    if add_bos_token:\n",
    "        seq_len = max_length - 1\n",
    "    else:\n",
    "        seq_len = max_length\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        text = examples[column_name]\n",
    "        # Concatenate it all into an enormous string, separated by eos_tokens\n",
    "        full_text = tokenizer.eos_token.join(text)\n",
    "        # Divide into 20 chunks of ~ equal length\n",
    "        num_chunks = 20\n",
    "        chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "        chunks = [\n",
    "            full_text[i * chunk_length : (i + 1) * chunk_length]\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "        # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned\n",
    "        tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\n",
    "            \"input_ids\"\n",
    "        ].flatten()\n",
    "        # Drop padding tokens\n",
    "        tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "        num_tokens = len(tokens)\n",
    "        num_batches = num_tokens // (seq_len)\n",
    "        # Drop the final tokens if not enough to make a full sequence\n",
    "        tokens = tokens[: seq_len * num_batches]\n",
    "        tokens = einops.rearrange(\n",
    "            tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len\n",
    "        )\n",
    "        if add_bos_token:\n",
    "            prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "            tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "        return {\"tokens\": tokens}\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[column_name],\n",
    "    )\n",
    "    #tokenized_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800*2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokens_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_lens import get_model_encoders\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from transformer_lens.utils import to_numpy\n",
    "\n",
    "# def get_feature_scores_transcoder(model, encoder, tokens_arr, feature_idx, batch_size=64, act_name='resid_pre', \n",
    "# \t\t\t\t\t\t\t\t  use_raw_scores=False, use_decoder=False, feature_post=None, ignore_endoftext=False):\n",
    "# \tact_name = encoder.cfg.hook_point\n",
    "# \tlayer = encoder.cfg.hook_point_layer\n",
    "\t\t\n",
    "# \tscores = []\n",
    "# \tendoftext_token = model.tokenizer.eos_token \n",
    "# \tfor i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "# \t\twith torch.no_grad():\n",
    "# \t\t\t_, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[\n",
    "# \t\t\t\tact_name\n",
    "# \t\t\t])\n",
    "# \t\t\tmlp_acts = cache[act_name]\n",
    "# \t\t\tmlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "# \t\t\tif feature_post is None:\n",
    "# \t\t\t\tfeature_post = encoder.W_enc[:, feature_idx] if not use_decoder else encoder.W_dec[feature_idx]\n",
    "# \t\t\tbias = -(encoder.b_dec @ feature_post) if use_decoder else encoder.b_enc[feature_idx] - (encoder.b_dec @ feature_post)\n",
    "# \t\t\tif use_raw_scores:\n",
    "# \t\t\t\tcur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "# \t\t\telse:\n",
    "# \t\t\t\thidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "# \t\t\t\tcur_scores = hidden_acts[:, feature_idx]\n",
    "# \t\t\t\tdel hidden_acts\n",
    "# \t\t\tif ignore_endoftext:\n",
    "# \t\t\t\t\tcur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "# \t\tscores.append(to_numpy(cur_scores.reshape(-1, tokens_arr.shape[1])).astype(np.float16))\n",
    "# \treturn np.concatenate(scores)\n",
    "\n",
    "\n",
    "# def get_feature_scores_zsae(model, encoder, tokens_arr, feature_idx, batch_size=64, act_name='attn.hook_z',\n",
    "# \t\t\t\t\t\t\tuse_raw_scores=False, feature_post=None, ignore_endoftext=False):\n",
    "# \tprint(encoder.cfg)\n",
    "# \tlayer = encoder.cfg['layer']\n",
    "\n",
    "# \tscores = []\n",
    "# \tendoftext_token = model.tokenizer.eos_token\n",
    "\n",
    "# \tname_filter = f'blocks.{layer}.attn.hook_z'\n",
    "\n",
    "# \tfor i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "# \t\twith torch.no_grad():\n",
    "# \t\t\t_, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[\n",
    "# \t\t\t\tname_filter\n",
    "# \t\t\t])\n",
    "# \t\t\tmlp_acts = cache[name_filter]\n",
    "# \t\t\tmlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "# \t\t\tif feature_post is None:\n",
    "# \t\t\t\tfeature_post = encoder.W_enc[:, feature_idx]\n",
    "# \t\t\tbias = encoder.b_enc[feature_idx] - (encoder.b_dec @ feature_post)\n",
    "# \t\t\tif use_raw_scores:\n",
    "# \t\t\t\tcur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "# \t\t\telse:\n",
    "# \t\t\t\thidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "# \t\t\t\tcur_scores = hidden_acts[:, feature_idx]\n",
    "# \t\t\t\tdel hidden_acts\n",
    "# \t\t\tif ignore_endoftext:\n",
    "# \t\t\t\tcur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "# \t\tscores.append(to_numpy(cur_scores.reshape(-1, tokens_arr.shape[1])).astype(np.float16))\n",
    "\n",
    "# \treturn np.concatenate(scores)\n",
    "\n",
    "def get_feature_scores_transcoder(model, encoder, tokens_arr, feature_indices, batch_size=64, act_name='resid_pre', \n",
    "                                  use_raw_scores=False, use_decoder=False, feature_post=None, ignore_endoftext=False):\n",
    "    act_name = encoder.cfg.hook_point\n",
    "    layer = encoder.cfg.hook_point_layer\n",
    "    \n",
    "    scores = []\n",
    "    endoftext_token = model.tokenizer.eos_token \n",
    "    for i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[act_name])\n",
    "            mlp_acts = cache[act_name]\n",
    "            mlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "            if feature_post is None:\n",
    "                feature_post = encoder.W_enc[:, feature_indices] if not use_decoder else encoder.W_dec[:, feature_indices]\n",
    "            bias = -(encoder.b_dec @ feature_post) if use_decoder else encoder.b_enc[feature_indices] - (encoder.b_dec @ feature_post)\n",
    "            if use_raw_scores:\n",
    "                cur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "            else:\n",
    "                hidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "                cur_scores = hidden_acts[:, feature_indices]\n",
    "                del hidden_acts\n",
    "            if ignore_endoftext:\n",
    "                cur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "        scores.append(to_numpy(cur_scores.reshape(-1, len(feature_indices), tokens_arr.shape[1])).astype(np.float16))\n",
    "    return np.concatenate(scores, axis=0)\n",
    "\n",
    "\n",
    "def get_feature_scores_zsae(model, encoder, tokens_arr, feature_indices, batch_size=64, act_name='attn.hook_z',\n",
    "                            use_raw_scores=False, feature_post=None, ignore_endoftext=False):\n",
    "    layer = encoder.cfg['layer']\n",
    "\n",
    "    scores = []\n",
    "    endoftext_token = model.tokenizer.eos_token\n",
    "\n",
    "    name_filter = f'blocks.{layer}.attn.hook_z'\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[name_filter])\n",
    "            mlp_acts = cache[name_filter]\n",
    "            mlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "            if feature_post is None:\n",
    "                feature_post = encoder.W_enc[:, feature_indices]\n",
    "            bias = encoder.b_enc[feature_indices] - (encoder.b_dec @ feature_post)\n",
    "            if use_raw_scores:\n",
    "                cur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "            else:\n",
    "                hidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "                cur_scores = hidden_acts[:, feature_indices]\n",
    "                del hidden_acts\n",
    "            if ignore_endoftext:\n",
    "                cur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "        scores.append(to_numpy(cur_scores.reshape(-1, len(feature_indices), tokens_arr.shape[1])).astype(np.float16))\n",
    "\n",
    "    return np.concatenate(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = z_saes[8]\n",
    "feature_idx = [20100]\n",
    "feature_scores = get_feature_scores_zsae(model, sae, owt_tokens_torch[:1024], feature_idx, batch_size=4, act_name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = feature_scores[:, 0, :]\n",
    "\n",
    "# Print (batch, seq) where feature scores is not zero\n",
    "for i in range(feature_scores.shape[0]):\n",
    "    for j in range(feature_scores.shape[1]):\n",
    "        if feature_scores[i, j] != 0:\n",
    "            print(f\"Batch {i}, Seq {j}: {feature_scores[i, j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder = transcoders[8]\n",
    "feature_idx = 20104\n",
    "\n",
    "feature_scores = get_feature_scores_transcoder(model, transcoder, owt_tokens_torch[:1024], feature_idx, batch_size=4, act_name='resid_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of non-zero elements\n",
    "np.count_nonzero(feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "# cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# # f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "# N = 50\n",
    "\n",
    "# features_for_heads = task_eval.get_features_at_heads_over_dataset(N=N, use_set=False)\n",
    "# features_for_mlps = task_eval.get_features_at_mlps_over_dataset(N=N, use_set=False)\n",
    "# mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True, visualize=False)\n",
    "# attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True, visualize=False)\n",
    "\n",
    "cp = CircuitPrediction(attn_freqs, mlp_freqs, features_for_heads, features_for_mlps)\n",
    "\n",
    "_ = cp.component_frequency_array(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set([x for x in cp.circuit_hypergraph['MLP3']['features'] if x != -1]))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokens_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder = transcoders[3]\n",
    "feature_scores = get_feature_scores_transcoder(model, transcoder, owt_tokens_torch, features, batch_size=64, act_name='resid_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set([x for x in cp.circuit_hypergraph['L5_H5']['features'] if x != -1]))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set([x for x in cp.circuit_hypergraph['L0_H1']['features'] if x != -1]))\n",
    "print(features)\n",
    "sae = z_saes[0]\n",
    "feature_scores = get_feature_scores_zsae(model, sae, owt_tokens_torch, features, batch_size=64, act_name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML, display\n",
    "import html\n",
    "\n",
    "def get_top_k_activating_examples(feature_scores, tokens, model, k=5):\n",
    "    # Flatten the feature_scores to get the top k scores and their indices\n",
    "    flat_scores = feature_scores.flatten()\n",
    "    top_k_indices = flat_scores.argsort()[-k:][::-1]\n",
    "    top_k_scores = flat_scores[top_k_indices]\n",
    "    \n",
    "    # Convert the flat indices back to the original (batch, seq_len) indices\n",
    "    top_k_batch_indices, top_k_seq_indices = np.unravel_index(top_k_indices, feature_scores.shape)\n",
    "\n",
    "    # Extract the corresponding token sequences and scores\n",
    "    top_k_tokens = [tokens[batch_idx].tolist() for batch_idx in top_k_batch_indices]\n",
    "    top_k_tokens_str = [[model.to_string(x) for x in token_seq] for token_seq in top_k_tokens]\n",
    "    top_k_scores_per_seq = [feature_scores[batch_idx].tolist() for batch_idx in top_k_batch_indices]\n",
    "\n",
    "    return top_k_tokens_str, top_k_scores_per_seq, top_k_seq_indices\n",
    "\n",
    "def highlight_scores_in_html(token_strs, scores, seq_idx, max_color='#ff8c00', zero_color='#ffffff', show_score=True):\n",
    "    if len(token_strs) != len(scores):\n",
    "        print(\"Length mismatch between tokens and scores\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "    scores_min = min(scores)\n",
    "    scores_max = max(scores)\n",
    "    scores_normalized = (np.array(scores) - scores_min) / (scores_max - scores_min)\n",
    "    \n",
    "    max_color_vec = np.array([int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)])\n",
    "    zero_color_vec = np.array([int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)])\n",
    "    \n",
    "    color_vecs = np.einsum('i, j -> ij', scores_normalized, max_color_vec) + np.einsum('i, j -> ij', 1 - scores_normalized, zero_color_vec)\n",
    "    color_strs = [f\"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}\" for x in color_vecs]\n",
    "    \n",
    "    if show_score:\n",
    "        tokens_html = \"\".join([\n",
    "            f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}<span class='feature_val'> ({scores[i]:.2f})</span></span>\"\"\"\n",
    "            for i, token_str in enumerate(token_strs)\n",
    "        ])\n",
    "        clean_text = \" | \".join([\n",
    "            f\"{token_str} ({scores[i]:.2f})\"\n",
    "            for i, token_str in enumerate(token_strs)\n",
    "        ])\n",
    "    else:\n",
    "        tokens_html = \"\".join([\n",
    "            f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}</span>\"\"\"\n",
    "            for i, token_str in enumerate(token_strs)\n",
    "        ])\n",
    "        clean_text = \" | \".join(token_strs)\n",
    "\n",
    "    head = \"\"\"\n",
    "    <style>\n",
    "        span.token {\n",
    "            font-family: monospace;\n",
    "            border-style: solid;\n",
    "            border-width: 1px;\n",
    "            border-color: #dddddd;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return head + tokens_html, clean_text\n",
    "\n",
    "def display_top_k_activating_examples(model, feature_scores, tokens, k=5, show_score=True):\n",
    "    top_k_tokens_str, top_k_scores_per_seq, top_k_seq_indices = get_top_k_activating_examples(feature_scores, tokens, model, k=k)\n",
    "    \n",
    "    examples_html = []\n",
    "    examples_clean_text = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        example_html, clean_text = highlight_scores_in_html(top_k_tokens_str[i], top_k_scores_per_seq[i], top_k_seq_indices[i], show_score=show_score)\n",
    "        display(HTML(example_html))\n",
    "        examples_html.append(example_html)\n",
    "        examples_clean_text.append(clean_text)\n",
    "\n",
    "    return examples_html, examples_clean_text\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have `model`, `feature_scores` (a tensor of shape (batch_size, seq_len)), and `tokens` (a tensor of shape (batch_size, seq_len))\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, 2, :], owt_tokens_torch, k=25, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "\n",
    "llm_client = AzureOpenAI(\n",
    "        azure_endpoint=config[\"base_url\"],\n",
    "        api_key=config[\"azure_api_key\"],\n",
    "        api_version=config[\"api_version\"],\n",
    "    )\n",
    "\n",
    "BASE_PROMPT = \"\"\" \n",
    "We're studying neurons in a neural network, trying to identify their roles. \\\n",
    "Look at the parts/tokens of the document this particular neuron activates \\\n",
    "highly for and summarize in a single sentence what the neuron is \\\n",
    "looking for. Don't list examples of words.We will show short text excerpts, \\\n",
    "followed by a comma separated list of tokens(part of word) that activate \\\n",
    "highly in those text excerpts. The format is word (score). Your task \\\n",
    "is to summarize what the highly activating tokens have in common, \\\n",
    "taking their context into account. \n",
    "\"\"\"\n",
    "\n",
    "def get_response(llm_client, prompt):\n",
    "    # Join prompt together separated by \\n\\n\n",
    "    prompt = \"\\n\\n\".join(prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": BASE_PROMPT + prompt}\n",
    "    ]\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt4_large\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return f\"{response.choices[0].message.content}\"\n",
    "\n",
    "\n",
    "print(BASE_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(llm_client, examples_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint=config[\"base_url\"],\n",
    "    api_key=config[\"azure_api_key\"],\n",
    "    api_version=config[\"api_version\"],\n",
    ")\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800 * 2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokens_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [16513, 7861]\n",
    "sae = z_saes[8]\n",
    "feature_scores = get_feature_scores(model, sae, owt_tokens_torch, features, batch_size=64, act_name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component_name = 'L8_H1'\n",
    "# features = list(set([x for x in cp.circuit_hypergraph[component_name]['features'] if x != -1]))\n",
    "features = [16513, 7861]\n",
    "\n",
    "if component_name[0] == 'L':\n",
    "    sae = z_saes[0]\n",
    "    feature_scores = get_feature_scores(model, sae, owt_tokens_torch, features, batch_size=64, act_name='z')\n",
    "else:\n",
    "    transcoder = transcoders[3]\n",
    "    feature_scores = get_feature_scores(model, transcoder, owt_tokens_torch, features, batch_size=64, act_name='resid_pre')\n",
    "\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, 2, :], owt_tokens_torch, k=8, show_score=True)\n",
    "feature_interpretation = get_response(llm_client, example_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
