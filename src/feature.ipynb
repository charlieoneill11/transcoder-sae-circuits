{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from circuit_discovery import CircuitDiscovery\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from data.greater_than_dataset import generate_greater_than_dataset\n",
    "from circuit_discovery import CircuitDiscovery, only_feature\n",
    "from circuit_lens import CircuitComponent\n",
    "from plotly_utils import *\n",
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "from data.greater_than_dataset import GT_GROUND_TRUTH_HEADS\n",
    "from memory import get_gpu_memory\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import get_attn_head_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "\n",
      "Loading SAEs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:08<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Transcoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6/12 [00:02<00:02,  2.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_greedy_passes):\n\u001b[1;32m     50\u001b[0m             cd\u001b[38;5;241m.\u001b[39mgreedily_add_top_contributors(k\u001b[38;5;241m=\u001b[39mk, reciever_threshold\u001b[38;5;241m=\u001b[39mthres)\n\u001b[0;32m---> 54\u001b[0m task_eval \u001b[38;5;241m=\u001b[39m \u001b[43mTaskEvaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuit_discovery_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_components_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m cd \u001b[38;5;241m=\u001b[39m task_eval\u001b[38;5;241m.\u001b[39mget_circuit_discovery_for_prompt(\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# f = task_eval.get_features_at_heads_over_dataset(N=30)\u001b[39;00m\n",
      "File \u001b[0;32m~/transcoder-sae-circuits/src/task_evaluation.py:108\u001b[0m, in \u001b[0;36mTaskEvaluation.__init__\u001b[0;34m(self, prompts, circuit_discovery_strategy, allowed_components_filter, eval_index)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    102\u001b[0m     prompts: List[EvalItem],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     eval_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    106\u001b[0m ):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_encoders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit_discovery_strategy \u001b[38;5;241m=\u001b[39m circuit_discovery_strategy\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts \u001b[38;5;241m=\u001b[39m prompts\n",
      "File \u001b[0;32m~/transcoder-sae-circuits/src/circuit_lens.py:506\u001b[0m, in \u001b[0;36mget_model_encoders\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Transcoders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    505\u001b[0m mlp_transcoders \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 506\u001b[0m     \u001b[43mSparseTranscoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_hugging_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(model\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_layers)\n\u001b[1;32m    507\u001b[0m ]\n\u001b[1;32m    509\u001b[0m model_encoder_cache \u001b[38;5;241m=\u001b[39m (model, z_saes, mlp_transcoders)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_encoder_cache\n",
      "File \u001b[0;32m~/transcoder-sae-circuits/src/mlp_transcoder.py:598\u001b[0m, in \u001b[0;36mSparseTranscoder.load_from_hugging_face\u001b[0;34m(cls, layer, repo_id, file_template)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_hugging_face\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    595\u001b[0m     file_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_sparse_autoencoder_gpt2-small_blocks.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.ln2.hook_normalized_24576.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    596\u001b[0m ):\n\u001b[1;32m    597\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m file_template\u001b[38;5;241m.\u001b[39mformat(layer)\n\u001b[0;32m--> 598\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_from_pretrained(file_path)\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1642\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:395\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/http/client.py:1419\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1419\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/ssl.py:1253\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1252\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "\n",
    "def component_filter(component: str):\n",
    "    return component in [\n",
    "        CircuitComponent.Z_FEATURE,\n",
    "        CircuitComponent.MLP_FEATURE,\n",
    "        CircuitComponent.ATTN_HEAD,\n",
    "        CircuitComponent.UNEMBED,\n",
    "        # CircuitComponent.UNEMBED_AT_TOKEN,\n",
    "        CircuitComponent.EMBED,\n",
    "        CircuitComponent.POS_EMBED,\n",
    "        # CircuitComponent.BIAS_O,\n",
    "        CircuitComponent.Z_SAE_ERROR,\n",
    "        # CircuitComponent.Z_SAE_BIAS,\n",
    "        # CircuitComponent.TRANSCODER_ERROR,\n",
    "        # CircuitComponent.TRANSCODER_BIAS,\n",
    "    ]\n",
    "\n",
    "\n",
    "pass_based = True\n",
    "\n",
    "passes = 5\n",
    "node_contributors = 1\n",
    "first_pass_minimal = True\n",
    "\n",
    "sub_passes = 3\n",
    "do_sub_pass = False\n",
    "layer_thres = 9\n",
    "minimal = True\n",
    "\n",
    "\n",
    "num_greedy_passes = 20\n",
    "k = 1\n",
    "N = 30\n",
    "\n",
    "thres = 4\n",
    "\n",
    "def strategy(cd: CircuitDiscovery):\n",
    "    if pass_based:\n",
    "        for _ in range(passes):\n",
    "            cd.add_greedy_pass(contributors_per_node=node_contributors, minimal=first_pass_minimal)\n",
    "\n",
    "            if do_sub_pass:\n",
    "                for _ in range(sub_passes):\n",
    "                    cd.add_greedy_pass_against_all_existing_nodes(contributors_per_node=node_contributors, skip_z_features=True, layer_threshold=layer_thres, minimal=minimal)\n",
    "    else:\n",
    "        for _ in range(num_greedy_passes):\n",
    "            cd.greedily_add_top_contributors(k=k, reciever_threshold=thres)\n",
    "\n",
    "\n",
    "\n",
    "task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "N = 20\n",
    "\n",
    "features_for_heads = task_eval.get_features_at_heads_over_dataset(N=N, use_set=False)\n",
    "features_for_mlps = task_eval.get_features_at_mlps_over_dataset(N=N, use_set=False)\n",
    "mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True, visualize=False)\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_heads = task_eval.get_features_at_heads_over_dataset(N=N, use_set=False)\n",
    "features_for_mlps = task_eval.get_features_at_mlps_over_dataset(N=N, use_set=False)\n",
    "mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True, visualize=False)\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True, visualize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = IOI_GROUND_TRUTH_HEADS\n",
    "\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True)\n",
    "score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"GT\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CircuitPrediction:\n",
    "\n",
    "    def __init__(self, attn_freqs, mlp_freqs, features_for_heads, features_for_mlps):\n",
    "        self.attn_freqs = attn_freqs\n",
    "        self.mlp_freqs = mlp_freqs\n",
    "        self.features_for_heads = features_for_heads\n",
    "        self.features_for_mlps = features_for_mlps\n",
    "\n",
    "        self.component_labels = self.get_component_labels()\n",
    "\n",
    "        self.circuit_hypergraph = self.create_circuit_hypergraph()\n",
    "\n",
    "    def create_circuit_hypergraph(self):\n",
    "        \"\"\" \n",
    "        Creates the dictionary for each component with frequency of occurrence and list of features.\n",
    "        \"\"\"\n",
    "        # Create circuit hypergraph with keys being the component labels\n",
    "        circuit_hypergraph = {label: {\"freq\": 0, \"features\": []} for label in self.component_labels}\n",
    "\n",
    "        # Add attention heads\n",
    "        for layer, freq in enumerate(self.attn_freqs):\n",
    "            for head, freq_head in enumerate(freq):\n",
    "                label = f\"L{layer}_H{head}\"\n",
    "                circuit_hypergraph[label][\"freq\"] += freq_head.item()\n",
    "                circuit_hypergraph[label][\"features\"].extend(self.features_for_heads[layer][head])\n",
    "\n",
    "        # Add MLPs\n",
    "        for i, freq in enumerate(self.mlp_freqs):\n",
    "            label = f\"MLP{i}\"\n",
    "            circuit_hypergraph[label][\"freq\"] += freq.item()\n",
    "            circuit_hypergraph[label][\"features\"].extend(self.features_for_mlps[i])\n",
    "\n",
    "        return circuit_hypergraph\n",
    "            \n",
    "    def get_component_labels(self):\n",
    "        # Head labels\n",
    "        head_labels = [f\"L{layer}_H{head}\" for layer in range(12) for head in range(12)]\n",
    "\n",
    "        # MLP labels\n",
    "        mlp_labels = [f\"MLP{i}\" for i in range(12)]\n",
    "\n",
    "        # After every 12 head labels, insert the next MLP label\n",
    "        labels = []\n",
    "        for i in range(12):\n",
    "            labels.extend(head_labels[i*12:(i+1)*12])\n",
    "            labels.append(mlp_labels[i])\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def get_all_features_from_attn_layer(self, layer: int):\n",
    "        \"\"\"\n",
    "        Returns all features in the circuit hypergraph from a given attention layer.\n",
    "        \"\"\" \n",
    "        features = []\n",
    "        for head in range(12):\n",
    "            features.extend(self.features_for_heads[layer][head])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_circuit_at_threshold(self, threshold: float, visualize: bool = False):\n",
    "        \"\"\"\n",
    "        Selects attention heads and MLPs whose frequency is above the threshold.\n",
    "        \"\"\"\n",
    "        circuit = np.zeros(len(self.component_labels))\n",
    "\n",
    "        for i, label in enumerate(self.component_labels):\n",
    "            if self.circuit_hypergraph[label][\"freq\"] > threshold:\n",
    "                circuit[i] = 1\n",
    "\n",
    "        if visualize:\n",
    "            self.visualize_circuit(circuit)\n",
    "\n",
    "        return circuit\n",
    "    \n",
    "    def visualize_circuit(self, circuit: torch.Tensor, additional_title=\"\"):\n",
    "        \"\"\"\n",
    "        Visualizes the circuit.\n",
    "        \"\"\"\n",
    "        # Circuit array\n",
    "        circuit_array = np.zeros((12, 13))\n",
    "\n",
    "        # Labels are A1, ..., A12, MLP\n",
    "        labels = [f\"A{i}\" for i in range(1, 13)] + [\"MLP\"]\n",
    "\n",
    "        # Fill in the circuit array\n",
    "        for i, pred in enumerate(circuit):\n",
    "            layer = i // 13\n",
    "            head = i % 13\n",
    "\n",
    "            circuit_array[layer, head] = pred\n",
    "\n",
    "        # Create the figure with plotly imshow\n",
    "        fig = px.imshow(circuit_array, labels=dict(x=\"Attention Head\", y=\"Layer\"), width=500,\n",
    "                        title=additional_title,\n",
    "                        x=labels, y=[x for x in range(12)], color_continuous_scale=\"blues\")\n",
    "        fig.show()\n",
    "\n",
    "    def component_frequency_array(self, visualize: bool = False):\n",
    "        \"\"\"\n",
    "        Returns the frequency of each component in the circuit.\n",
    "        \"\"\"\n",
    "        frequency_array = np.zeros((12, 13))\n",
    "\n",
    "        # Labels are A1, ..., A12, MLP\n",
    "        labels = [f\"A{i}\" for i in range(1, 13)] + [\"MLP\"]\n",
    "\n",
    "        # Fill in the frequency by looking at the circuit hypergraph\n",
    "        for i, label in enumerate(self.component_labels):\n",
    "            layer = i // 13\n",
    "            head = i % 13\n",
    "\n",
    "            frequency_array[layer, head] = self.circuit_hypergraph[label][\"freq\"]\n",
    "\n",
    "        if visualize:\n",
    "            fig = px.imshow(frequency_array, labels=dict(x=\"Attention Head\", y=\"Layer\"), width=450,\n",
    "                        title=\"Frequency of components\",\n",
    "                        x=labels, y=[x for x in range(12)], color_continuous_scale=\"blues\")\n",
    "            fig.show()\n",
    "\n",
    "        return frequency_array\n",
    "\n",
    "\n",
    "    def unique_feature_array(self, visualize: bool = False):\n",
    "        \"\"\"\n",
    "        Returns the unique features for each component in the circuit.\n",
    "        \"\"\" \n",
    "        unique_features_array = np.zeros((12, 13))\n",
    "\n",
    "        # Labels are A1, ..., A12, MLP\n",
    "        labels = [f\"A{i}\" for i in range(1, 13)] + [\"MLP\"]\n",
    "\n",
    "        # Fill in the num unique features by looking at the circuit hypergraph\n",
    "        for i, label in enumerate(self.component_labels):\n",
    "            layer = i // 13\n",
    "            head = i % 13\n",
    "\n",
    "            unique_features_array[layer, head] = len(set(self.circuit_hypergraph[label][\"features\"]))\n",
    "\n",
    "        if visualize:\n",
    "            fig = px.imshow(unique_features_array, labels=dict(x=\"Attention Head\", y=\"Layer\"), width=450,\n",
    "                        title=\"Unique features\",\n",
    "                        x=labels, y=[x for x in range(12)], color_continuous_scale=\"blues\")\n",
    "            fig.show()\n",
    "\n",
    "        return unique_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CircuitPrediction(attn_freqs, mlp_freqs, features_for_heads, features_for_mlps)\n",
    "_ = cp.unique_feature_array(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_feature_array = cp.unique_feature_array(visualize=False)[:, :-1]\n",
    "ground_truth = IOI_GROUND_TRUTH_HEADS.numpy()\n",
    "unique_feature_array.shape, ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Flatten both\n",
    "unique_feature_array = unique_feature_array.flatten()\n",
    "ground_truth = ground_truth.flatten()\n",
    "\n",
    "roc_auc_score(ground_truth, unique_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual frequency array\n",
    "freq_array = cp.component_frequency_array(visualize=False)[:, :-1]\n",
    "freq_array = torch.tensor(freq_array).flatten().softmax(dim=-1).numpy()\n",
    "print(freq_array.shape)\n",
    "\n",
    "roc_auc_score(ground_truth, freq_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_feature_array = unique_feature_array * freq_array\n",
    "\n",
    "roc_auc_score(ground_truth, prop_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _, _ = get_attn_head_roc(ground_truth, prop_feature_array, \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _, _ = get_attn_head_roc(ground_truth, unique_feature_array, \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _, _ = get_attn_head_roc(ground_truth, freq_array, \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autointerpretability\n",
    "\n",
    "Given a component (i.e. L5H5 or MLP7) and a list of features for that component, we want to use `CircuitLens` to find the max-activating examples for each feature. We will then feed these features to a language model and go through the autointerpretability pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from circuit_lens import CircuitLens\n",
    "from circuit_lens import ComponentLens\n",
    "from torch import Tensor\n",
    "\n",
    "prompt = dataset_prompts[0]['text'] + dataset_prompts[0]['correct']\n",
    "circuit_lens = CircuitLens(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_feature_activation(circuit_lens, seq_index: int, layer: int, feature: int):\n",
    "    # retrieve the active features for the given sequence index\n",
    "    active_features = circuit_lens.get_active_features(seq_index)\n",
    "    \n",
    "    # get the starting index for the MLP features in the specified layer\n",
    "    start_index = active_features.get_mlp_start_index(layer)\n",
    "    \n",
    "    # get the number of MLP features in this layer\n",
    "    num_mlp_features = active_features.keys[layer]['mlp']\n",
    "    \n",
    "    # extract the MLP features and their corresponding values\n",
    "    mlp_features = active_features.features[start_index:start_index + num_mlp_features]\n",
    "    mlp_values = active_features.values[start_index:start_index + num_mlp_features]\n",
    "    \n",
    "    # check if the specified feature is active and return its value\n",
    "    indices = (mlp_features == feature).nonzero(as_tuple=True)[0]\n",
    "    if len(indices) > 0: return mlp_values[indices[0]].item()\n",
    "    \n",
    "    # if the feature is not active, return 0\n",
    "    return 0.0\n",
    "\n",
    "# Example usage\n",
    "layer = 7\n",
    "seq_index = -2\n",
    "feature = 1414\n",
    "activation = get_mlp_feature_activation(circuit_lens, seq_index, layer, feature)\n",
    "print(f\"Activation of feature {feature} in MLP layer {layer}: {activation}\")\n",
    "# for feature in range(24576):\n",
    "#     #feature = 100\n",
    "#     activation = get_mlp_feature_activation(circuit_lens, seq_index, layer, feature)\n",
    "#     if activation > 0:\n",
    "#         print(f\"Activation of feature {feature} in MLP layer {layer}: {activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_head_activation(circuit_lens, seq_index: int, layer: int, feature: int):\n",
    "    # Retrieve the active features for the given sequence index\n",
    "    active_features = circuit_lens.get_active_features(seq_index)\n",
    "\n",
    "    # Delete active features vectors\n",
    "    #del active_features.vectors\n",
    "    print(active_features)\n",
    "    \n",
    "    # Get the starting index for the attention features in the specified layer\n",
    "    start_index = active_features.get_attn_start_index(layer)\n",
    "    \n",
    "    # Get the number of attention features in this layer\n",
    "    num_attn_features = active_features.keys[layer]['attn']\n",
    "    \n",
    "    # Extract the attention features and their corresponding values\n",
    "    attn_features = active_features.features[start_index:start_index + num_attn_features]\n",
    "    attn_values = active_features.values[start_index:start_index + num_attn_features]\n",
    "    \n",
    "    # Check if the specified feature is active and return its value\n",
    "    indices = (attn_features == feature).nonzero(as_tuple=True)[0]\n",
    "    if len(indices) > 0: return attn_values[indices[0]].item()\n",
    "\n",
    "    # If the feature is not active, return 0\n",
    "    return 0.0\n",
    "\n",
    "# Example usage\n",
    "layer = 5\n",
    "seq_index = -1\n",
    "feature = 44256\n",
    "activation = get_attention_head_activation(circuit_lens, seq_index, layer, feature)\n",
    "print(f\"Activation of feature {feature} in attention layer {layer}: {activation}\")\n",
    "# for feature in range(24576):\n",
    "#     activation = get_attention_head_activation(circuit_lens, seq_index, layer, feature)\n",
    "#     if activation > 0:\n",
    "#         print(f\"Activation of feature {feature} in attention layer {layer}: {activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CircuitPrediction(attn_freqs, mlp_freqs, features_for_heads, features_for_mlps)\n",
    "layer_5_features = [x for x in list(set(cp.get_all_features_from_attn_layer(5))) if x != -1]\n",
    "layer_5_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Then, Jose and Eric had a lot of fun at the store. Eric gave a computer to Jose'\n",
    "circuit_lens = CircuitLens(prompt=prompt)\n",
    "\n",
    "layer = 5\n",
    "feats = [x for x in list(set(cp.get_all_features_from_attn_layer(5))) if x != -1]\n",
    "\n",
    "feat_act_dict = {}\n",
    "for feat in feats:\n",
    "    act = get_attention_head_activation(circuit_lens, -1, layer, feat)\n",
    "    feat_act_dict[feat] = act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_act_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so how we are going to do this? What do we need to be able to do?\n",
    "* Specify a list of components, and a list of features for each component.\n",
    "* Over some large corpus of tokens (probably about 1 mill), cache the active features (this is good - Danny has already set up the infrastructure for this)!\n",
    "* Since we have saved these active features in a principled way, they don't take up much space and also they allow us to just lookup the max-activating examples for a specific feature in a specific layer.\n",
    "* We need to save the active features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_lens import ActiveFeatures\n",
    "\n",
    "circuit_lens = CircuitLens(prompt=prompt)\n",
    "active_features = circuit_lens.get_active_features(-2)\n",
    "values = active_features.values\n",
    "features = active_features.features\n",
    "keys = active_features.keys\n",
    "print(values.shape, features.shape)\n",
    "print(keys)\n",
    "\n",
    "test_af = ActiveFeatures(vectors=[], values=values, features=features, keys=keys)\n",
    "test_af.get_attn_start_index(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the starting index for the attention features in the specified layer\n",
    "start_index = active_features.get_attn_start_index(layer)\n",
    "\n",
    "# Get the number of attention features in this layer\n",
    "layer = 5\n",
    "num_attn_features = active_features.keys[layer]['attn']\n",
    "\n",
    "# Extract the attention features and their corresponding values\n",
    "attn_features = active_features.features[start_index:start_index + num_attn_features]\n",
    "attn_values = active_features.values[start_index:start_index + num_attn_features]\n",
    "\n",
    "attn_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_lens = circuit_lens.get_head_seq_lens_for_z_feature(layer=layer, seq_index=-2, feature=attn_features[0], visualize=False, k=50)\n",
    "head_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_lens[0][0].run_data['head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert the head_lens into a list of tuples\n",
    "# Each tuple is (head, contribution) to that specific feature\n",
    "head_lens_list = [(head[0].run_data['head'], head[1]) for head in head_lens]\n",
    "# Sum the contributions of each head\n",
    "head_contributions = {}\n",
    "for head, contribution in head_lens_list:\n",
    "    if head in head_contributions:\n",
    "        head_contributions[head] += contribution\n",
    "    else:\n",
    "        head_contributions[head] = contribution\n",
    "head_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Then, Jose and Eric had a lot of fun at the store. Eric gave a computer to Jose'\n",
    "circuit_lens = CircuitLens(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_lens import ActiveFeatures\n",
    "\n",
    "\n",
    "def prompt_to_active_features(circuit_lens: CircuitLens, seq_pos: int):\n",
    "\n",
    "    active_features = circuit_lens.get_active_features(seq_pos)\n",
    "    values = active_features.values\n",
    "    features = active_features.features\n",
    "    keys = active_features.keys\n",
    "\n",
    "    head_feature_contributions = {}\n",
    "\n",
    "    for layer in range(12):\n",
    "        # Get the starting index for the attention features in the specified layer\n",
    "        start_index = active_features.get_attn_start_index(layer)\n",
    "\n",
    "        # Get the number of attention features in this layer\n",
    "        num_attn_features = active_features.keys[layer]['attn']\n",
    "\n",
    "        # Extract the attention features and their corresponding values\n",
    "        attn_features = active_features.features[start_index:start_index + num_attn_features]\n",
    "        \n",
    "        # Get the unique features for the attention heads in this layer\n",
    "\n",
    "        layer_head_feature_contributions = {}\n",
    "\n",
    "        for attn_feature in set(attn_features):\n",
    "            if attn_feature == -1: continue\n",
    "            head_lens = circuit_lens.get_head_seq_lens_for_z_feature(layer=layer, seq_index=seq_pos, feature=attn_feature, visualize=False, k=50)\n",
    "            head_lens_list = [(head[0].run_data['head'], head[1]) for head in head_lens]\n",
    "            head_contributions = {}\n",
    "            for head, contribution in head_lens_list:\n",
    "                if head in head_contributions:\n",
    "                    head_contributions[head] += contribution\n",
    "                else:\n",
    "                    head_contributions[head] = contribution\n",
    "            layer_head_feature_contributions[attn_feature.item()] = head_contributions\n",
    "\n",
    "        head_feature_contributions[layer] = layer_head_feature_contributions\n",
    "\n",
    "    del active_features\n",
    "\n",
    "    # Final dictionary to save\n",
    "    prompt_dict = {\n",
    "        'prompt': prompt,\n",
    "        'values': values,\n",
    "        'features': features,\n",
    "        'keys': keys,\n",
    "        'head_feature_contributions': head_feature_contributions\n",
    "    }\n",
    "\n",
    "    return prompt_dict\n",
    "\n",
    "prompt_dict = prompt_to_active_features(circuit_lens, -2)\n",
    "prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 1\n",
    "# Load the transformer model and activation store\n",
    "hook_point = \"blocks.8.hook_resid_pre\" # this doesn't matter\n",
    "saes, _ = get_gpt2_res_jb_saes(hook_point)\n",
    "sparse_autoencoder = saes[hook_point]\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "sparse_autoencoder.cfg.hook_point = f\"blocks.{layer}.attn.hook_z\"\n",
    "sparse_autoencoder.cfg.store_batch_size = batch_size\n",
    "\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "\n",
    "print(f\"Loader cfg batch size = {sparse_autoencoder.cfg.store_batch_size} (batch size = {batch_size})\")\n",
    "\n",
    "# don't overwrite the sparse autoencoder with the loader's sae (newly initialized)\n",
    "tl_model, _, activation_store = loader.load_sae_training_group_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = activation_store.get_batch_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 1_000\n",
    "seq_len = 128\n",
    "\n",
    "all_active_features = []\n",
    "\n",
    "for i in range(total_tokens // seq_len):\n",
    "    tokens = activation_store.get_batch_tokens()\n",
    "    prompt = tl_model.to_string(tokens)\n",
    "\n",
    "    for seq_pos in trange(seq_len-1):\n",
    "\n",
    "        circuit_lens = CircuitLens(prompt=prompt)\n",
    "\n",
    "        prompt_dict = prompt_to_active_features(circuit_lens, seq_pos)\n",
    "\n",
    "        all_active_features.append(prompt_dict)\n",
    "\n",
    "        del circuit_lens\n",
    "        del prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach - storing ZSAE and transcoder activations directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "def tokenize_and_concatenate(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    streaming = False,\n",
    "    max_length = 1024,\n",
    "    column_name = \"text\",\n",
    "    add_bos_token = True,\n",
    "):\n",
    "    \"\"\"Helper function to tokenizer and concatenate a dataset of text. This converts the text to tokens, concatenates them (separated by EOS tokens) and then reshapes them into a 2D array of shape (____, sequence_length), dropping the last batch. Tokenizers are much faster if parallelised, so we chop the string into 20, feed it into the tokenizer, in parallel with padding, then remove padding at the end.\n",
    "\n",
    "    This tokenization is useful for training language models, as it allows us to efficiently train on a large corpus of text of varying lengths (without, eg, a lot of truncation or padding). Further, for models with absolute positional encodings, this avoids privileging early tokens (eg, news articles often begin with CNN, and models may learn to use early positional encodings to predict these)\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to tokenize, assumed to be a HuggingFace text dataset.\n",
    "        tokenizer (AutoTokenizer): The tokenizer. Assumed to have a bos_token_id and an eos_token_id.\n",
    "        streaming (bool, optional): Whether the dataset is being streamed. If True, avoids using parallelism. Defaults to False.\n",
    "        max_length (int, optional): The length of the context window of the sequence. Defaults to 1024.\n",
    "        column_name (str, optional): The name of the text column in the dataset. Defaults to 'text'.\n",
    "        add_bos_token (bool, optional): . Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Returns the tokenized dataset, as a dataset of tensors, with a single column called \"tokens\"\n",
    "\n",
    "    Note: There is a bug when inputting very small datasets (eg, <1 batch per process) where it just outputs nothing. I'm not super sure why\n",
    "    \"\"\"\n",
    "    for key in dataset.features:\n",
    "        if key != column_name:\n",
    "            dataset = dataset.remove_columns(key)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        # We add a padding token, purely to implement the tokenizer. This will be removed before inputting tokens to the model, so we do not need to increment d_vocab in the model.\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    # Define the length to chop things up into - leaving space for a bos_token if required\n",
    "    if add_bos_token:\n",
    "        seq_len = max_length - 1\n",
    "    else:\n",
    "        seq_len = max_length\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        text = examples[column_name]\n",
    "        # Concatenate it all into an enormous string, separated by eos_tokens\n",
    "        full_text = tokenizer.eos_token.join(text)\n",
    "        # Divide into 20 chunks of ~ equal length\n",
    "        num_chunks = 20\n",
    "        chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "        chunks = [\n",
    "            full_text[i * chunk_length : (i + 1) * chunk_length]\n",
    "            for i in range(num_chunks)\n",
    "        ]\n",
    "        # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned\n",
    "        tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\n",
    "            \"input_ids\"\n",
    "        ].flatten()\n",
    "        # Drop padding tokens\n",
    "        tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "        num_tokens = len(tokens)\n",
    "        num_batches = num_tokens // (seq_len)\n",
    "        # Drop the final tokens if not enough to make a full sequence\n",
    "        tokens = tokens[: seq_len * num_batches]\n",
    "        tokens = einops.rearrange(\n",
    "            tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len\n",
    "        )\n",
    "        if add_bos_token:\n",
    "            prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "            tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "        return {\"tokens\": tokens}\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[column_name],\n",
    "    )\n",
    "    #tokenized_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800*2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokens_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_lens import get_model_encoders\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from transformer_lens.utils import to_numpy\n",
    "\n",
    "# def get_feature_scores_transcoder(model, encoder, tokens_arr, feature_idx, batch_size=64, act_name='resid_pre', \n",
    "# \t\t\t\t\t\t\t\t  use_raw_scores=False, use_decoder=False, feature_post=None, ignore_endoftext=False):\n",
    "# \tact_name = encoder.cfg.hook_point\n",
    "# \tlayer = encoder.cfg.hook_point_layer\n",
    "\t\t\n",
    "# \tscores = []\n",
    "# \tendoftext_token = model.tokenizer.eos_token \n",
    "# \tfor i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "# \t\twith torch.no_grad():\n",
    "# \t\t\t_, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[\n",
    "# \t\t\t\tact_name\n",
    "# \t\t\t])\n",
    "# \t\t\tmlp_acts = cache[act_name]\n",
    "# \t\t\tmlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "# \t\t\tif feature_post is None:\n",
    "# \t\t\t\tfeature_post = encoder.W_enc[:, feature_idx] if not use_decoder else encoder.W_dec[feature_idx]\n",
    "# \t\t\tbias = -(encoder.b_dec @ feature_post) if use_decoder else encoder.b_enc[feature_idx] - (encoder.b_dec @ feature_post)\n",
    "# \t\t\tif use_raw_scores:\n",
    "# \t\t\t\tcur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "# \t\t\telse:\n",
    "# \t\t\t\thidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "# \t\t\t\tcur_scores = hidden_acts[:, feature_idx]\n",
    "# \t\t\t\tdel hidden_acts\n",
    "# \t\t\tif ignore_endoftext:\n",
    "# \t\t\t\t\tcur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "# \t\tscores.append(to_numpy(cur_scores.reshape(-1, tokens_arr.shape[1])).astype(np.float16))\n",
    "# \treturn np.concatenate(scores)\n",
    "\n",
    "\n",
    "# def get_feature_scores_zsae(model, encoder, tokens_arr, feature_idx, batch_size=64, act_name='attn.hook_z',\n",
    "# \t\t\t\t\t\t\tuse_raw_scores=False, feature_post=None, ignore_endoftext=False):\n",
    "# \tprint(encoder.cfg)\n",
    "# \tlayer = encoder.cfg['layer']\n",
    "\n",
    "# \tscores = []\n",
    "# \tendoftext_token = model.tokenizer.eos_token\n",
    "\n",
    "# \tname_filter = f'blocks.{layer}.attn.hook_z'\n",
    "\n",
    "# \tfor i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "# \t\twith torch.no_grad():\n",
    "# \t\t\t_, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[\n",
    "# \t\t\t\tname_filter\n",
    "# \t\t\t])\n",
    "# \t\t\tmlp_acts = cache[name_filter]\n",
    "# \t\t\tmlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "# \t\t\tif feature_post is None:\n",
    "# \t\t\t\tfeature_post = encoder.W_enc[:, feature_idx]\n",
    "# \t\t\tbias = encoder.b_enc[feature_idx] - (encoder.b_dec @ feature_post)\n",
    "# \t\t\tif use_raw_scores:\n",
    "# \t\t\t\tcur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "# \t\t\telse:\n",
    "# \t\t\t\thidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "# \t\t\t\tcur_scores = hidden_acts[:, feature_idx]\n",
    "# \t\t\t\tdel hidden_acts\n",
    "# \t\t\tif ignore_endoftext:\n",
    "# \t\t\t\tcur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "# \t\tscores.append(to_numpy(cur_scores.reshape(-1, tokens_arr.shape[1])).astype(np.float16))\n",
    "\n",
    "# \treturn np.concatenate(scores)\n",
    "\n",
    "def get_feature_scores_transcoder(model, encoder, tokens_arr, feature_indices, batch_size=64, act_name='resid_pre', \n",
    "                                  use_raw_scores=False, use_decoder=False, feature_post=None, ignore_endoftext=False):\n",
    "    act_name = encoder.cfg.hook_point\n",
    "    layer = encoder.cfg.hook_point_layer\n",
    "    \n",
    "    scores = []\n",
    "    endoftext_token = model.tokenizer.eos_token \n",
    "    for i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[act_name])\n",
    "            mlp_acts = cache[act_name]\n",
    "            mlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "            if feature_post is None:\n",
    "                feature_post = encoder.W_enc[:, feature_indices] if not use_decoder else encoder.W_dec[:, feature_indices]\n",
    "            bias = -(encoder.b_dec @ feature_post) if use_decoder else encoder.b_enc[feature_indices] - (encoder.b_dec @ feature_post)\n",
    "            if use_raw_scores:\n",
    "                cur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "            else:\n",
    "                hidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "                cur_scores = hidden_acts[:, feature_indices]\n",
    "                del hidden_acts\n",
    "            if ignore_endoftext:\n",
    "                cur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "        scores.append(to_numpy(cur_scores.reshape(-1, len(feature_indices), tokens_arr.shape[1])).astype(np.float16))\n",
    "    return np.concatenate(scores, axis=0)\n",
    "\n",
    "\n",
    "def get_feature_scores_zsae(model, encoder, tokens_arr, feature_indices, batch_size=64, act_name='attn.hook_z',\n",
    "                            use_raw_scores=False, feature_post=None, ignore_endoftext=False):\n",
    "    layer = encoder.cfg['layer']\n",
    "\n",
    "    scores = []\n",
    "    endoftext_token = model.tokenizer.eos_token\n",
    "\n",
    "    name_filter = f'blocks.{layer}.attn.hook_z'\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[name_filter])\n",
    "            mlp_acts = cache[name_filter]\n",
    "            mlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])\n",
    "            if feature_post is None:\n",
    "                feature_post = encoder.W_enc[:, feature_indices]\n",
    "            bias = encoder.b_enc[feature_indices] - (encoder.b_dec @ feature_post)\n",
    "            if use_raw_scores:\n",
    "                cur_scores = (mlp_acts_flattened @ feature_post) + bias\n",
    "            else:\n",
    "                hidden_acts = encoder.encode(mlp_acts_flattened)\n",
    "                cur_scores = hidden_acts[:, feature_indices]\n",
    "                del hidden_acts\n",
    "            if ignore_endoftext:\n",
    "                cur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf\n",
    "        scores.append(to_numpy(cur_scores.reshape(-1, len(feature_indices), tokens_arr.shape[1])).astype(np.float16))\n",
    "\n",
    "    return np.concatenate(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = z_saes[8]\n",
    "feature_idx = [20100]\n",
    "feature_scores = get_feature_scores_zsae(model, sae, owt_tokens_torch[:1024], feature_idx, batch_size=4, act_name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = feature_scores[:, 0, :]\n",
    "\n",
    "# Print (batch, seq) where feature scores is not zero\n",
    "for i in range(feature_scores.shape[0]):\n",
    "    for j in range(feature_scores.shape[1]):\n",
    "        if feature_scores[i, j] != 0:\n",
    "            print(f\"Batch {i}, Seq {j}: {feature_scores[i, j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder = transcoders[8]\n",
    "feature_idx = 20104\n",
    "\n",
    "feature_scores = get_feature_scores_transcoder(model, transcoder, owt_tokens_torch[:1024], feature_idx, batch_size=4, act_name='resid_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of non-zero elements\n",
    "np.count_nonzero(feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "# cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# # f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "# N = 50\n",
    "\n",
    "# features_for_heads = task_eval.get_features_at_heads_over_dataset(N=N, use_set=False)\n",
    "# features_for_mlps = task_eval.get_features_at_mlps_over_dataset(N=N, use_set=False)\n",
    "# mlp_freqs = task_eval.get_mlp_freqs_over_dataset(N=N, return_freqs=True, visualize=False)\n",
    "# attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True, visualize=False)\n",
    "\n",
    "cp = CircuitPrediction(attn_freqs, mlp_freqs, features_for_heads, features_for_mlps)\n",
    "\n",
    "_ = cp.component_frequency_array(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set([x for x in cp.circuit_hypergraph['MLP3']['features'] if x != -1]))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokens_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcoder = transcoders[3]\n",
    "feature_scores = get_feature_scores_transcoder(model, transcoder, owt_tokens_torch, features, batch_size=64, act_name='resid_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set([x for x in cp.circuit_hypergraph['L5_H5']['features'] if x != -1]))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set([x for x in cp.circuit_hypergraph['L0_H1']['features'] if x != -1]))\n",
    "print(features)\n",
    "sae = z_saes[0]\n",
    "feature_scores = get_feature_scores_zsae(model, sae, owt_tokens_torch, features, batch_size=64, act_name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML, display\n",
    "import html\n",
    "\n",
    "def get_top_k_activating_examples(feature_scores, tokens, model, k=5):\n",
    "    # Flatten the feature_scores to get the top k scores and their indices\n",
    "    flat_scores = feature_scores.flatten()\n",
    "    top_k_indices = flat_scores.argsort()[-k:][::-1]\n",
    "    top_k_scores = flat_scores[top_k_indices]\n",
    "    \n",
    "    # Convert the flat indices back to the original (batch, seq_len) indices\n",
    "    top_k_batch_indices, top_k_seq_indices = np.unravel_index(top_k_indices, feature_scores.shape)\n",
    "\n",
    "    # Extract the corresponding token sequences and scores\n",
    "    top_k_tokens = [tokens[batch_idx].tolist() for batch_idx in top_k_batch_indices]\n",
    "    top_k_tokens_str = [[model.to_string(x) for x in token_seq] for token_seq in top_k_tokens]\n",
    "    top_k_scores_per_seq = [feature_scores[batch_idx].tolist() for batch_idx in top_k_batch_indices]\n",
    "\n",
    "    return top_k_tokens_str, top_k_scores_per_seq, top_k_seq_indices\n",
    "\n",
    "def highlight_scores_in_html(token_strs, scores, seq_idx, max_color='#ff8c00', zero_color='#ffffff', show_score=True):\n",
    "    if len(token_strs) != len(scores):\n",
    "        print(\"Length mismatch between tokens and scores\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "    scores_min = min(scores)\n",
    "    scores_max = max(scores)\n",
    "    scores_normalized = (np.array(scores) - scores_min) / (scores_max - scores_min)\n",
    "    \n",
    "    max_color_vec = np.array([int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)])\n",
    "    zero_color_vec = np.array([int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)])\n",
    "    \n",
    "    color_vecs = np.einsum('i, j -> ij', scores_normalized, max_color_vec) + np.einsum('i, j -> ij', 1 - scores_normalized, zero_color_vec)\n",
    "    color_strs = [f\"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}\" for x in color_vecs]\n",
    "    \n",
    "    if show_score:\n",
    "        tokens_html = \"\".join([\n",
    "            f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}<span class='feature_val'> ({scores[i]:.2f})</span></span>\"\"\"\n",
    "            for i, token_str in enumerate(token_strs)\n",
    "        ])\n",
    "        clean_text = \" | \".join([\n",
    "            f\"{token_str} ({scores[i]:.2f})\"\n",
    "            for i, token_str in enumerate(token_strs)\n",
    "        ])\n",
    "    else:\n",
    "        tokens_html = \"\".join([\n",
    "            f\"\"\"<span class='token' style='background-color: {color_strs[i]}'>{html.escape(token_str)}</span>\"\"\"\n",
    "            for i, token_str in enumerate(token_strs)\n",
    "        ])\n",
    "        clean_text = \" | \".join(token_strs)\n",
    "\n",
    "    head = \"\"\"\n",
    "    <style>\n",
    "        span.token {\n",
    "            font-family: monospace;\n",
    "            border-style: solid;\n",
    "            border-width: 1px;\n",
    "            border-color: #dddddd;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return head + tokens_html, clean_text\n",
    "\n",
    "def display_top_k_activating_examples(model, feature_scores, tokens, k=5, show_score=True):\n",
    "    top_k_tokens_str, top_k_scores_per_seq, top_k_seq_indices = get_top_k_activating_examples(feature_scores, tokens, model, k=k)\n",
    "    \n",
    "    examples_html = []\n",
    "    examples_clean_text = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        example_html, clean_text = highlight_scores_in_html(top_k_tokens_str[i], top_k_scores_per_seq[i], top_k_seq_indices[i], show_score=show_score)\n",
    "        display(HTML(example_html))\n",
    "        examples_html.append(example_html)\n",
    "        examples_clean_text.append(clean_text)\n",
    "\n",
    "    return examples_html, examples_clean_text\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have `model`, `feature_scores` (a tensor of shape (batch_size, seq_len)), and `tokens` (a tensor of shape (batch_size, seq_len))\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, 2, :], owt_tokens_torch, k=25, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import yaml\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "\n",
    "llm_client = AzureOpenAI(\n",
    "        azure_endpoint=config[\"base_url\"],\n",
    "        api_key=config[\"azure_api_key\"],\n",
    "        api_version=config[\"api_version\"],\n",
    "    )\n",
    "\n",
    "BASE_PROMPT = \"\"\" \n",
    "We're studying neurons in a neural network, trying to identify their roles. \\\n",
    "Look at the parts/tokens of the document this particular neuron activates \\\n",
    "highly for and summarize in a single sentence what the neuron is \\\n",
    "looking for. Don't list examples of words.We will show short text excerpts, \\\n",
    "followed by a comma separated list of tokens(part of word) that activate \\\n",
    "highly in those text excerpts. The format is word (score). Your task \\\n",
    "is to summarize what the highly activating tokens have in common, \\\n",
    "taking their context into account. \n",
    "\"\"\"\n",
    "\n",
    "def get_response(llm_client, prompt):\n",
    "    # Join prompt together separated by \\n\\n\n",
    "    prompt = \"\\n\\n\".join(prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": BASE_PROMPT + prompt}\n",
    "    ]\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt4_large\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return f\"{response.choices[0].message.content}\"\n",
    "\n",
    "\n",
    "print(BASE_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(llm_client, examples_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointerpretability import *\n",
    "\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "llm_client = AzureOpenAI(\n",
    "    azure_endpoint=config[\"base_url\"],\n",
    "    api_key=config[\"azure_api_key\"],\n",
    "    api_version=config[\"api_version\"],\n",
    ")\n",
    "\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "\n",
    "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
    "dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "tokenized_owt = tokenize_and_concatenate(dataset, model.tokenizer, max_length=128, streaming=True)\n",
    "tokenized_owt = tokenized_owt.shuffle(42)\n",
    "tokenized_owt = tokenized_owt.take(12800 * 2)\n",
    "owt_tokens = np.stack([x['tokens'] for x in tokenized_owt])\n",
    "owt_tokens_torch = torch.tensor(owt_tokens)\n",
    "\n",
    "device = 'cpu'\n",
    "tl_model, z_saes, transcoders = get_model_encoders(device=device)\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owt_tokens_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [16513, 7861]\n",
    "sae = z_saes[8]\n",
    "feature_scores = get_feature_scores(model, sae, owt_tokens_torch, features, batch_size=64, act_name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component_name = 'L8_H1'\n",
    "# features = list(set([x for x in cp.circuit_hypergraph[component_name]['features'] if x != -1]))\n",
    "features = [16513, 7861]\n",
    "\n",
    "if component_name[0] == 'L':\n",
    "    sae = z_saes[0]\n",
    "    feature_scores = get_feature_scores(model, sae, owt_tokens_torch, features, batch_size=64, act_name='z')\n",
    "else:\n",
    "    transcoder = transcoders[3]\n",
    "    feature_scores = get_feature_scores(model, transcoder, owt_tokens_torch, features, batch_size=64, act_name='resid_pre')\n",
    "\n",
    "example_html, examples_clean_text = display_top_k_activating_examples(model, feature_scores[:, 2, :], owt_tokens_torch, k=8, show_score=True)\n",
    "feature_interpretation = get_response(llm_client, example_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
