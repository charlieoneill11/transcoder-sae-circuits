{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "from circuit_lens import get_model_encoders\n",
    "from z_sae import ZSAE\n",
    "from mlp_transcoder import SparseTranscoder\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from typing import List, Dict, TypedDict, Any, Union, Tuple, Optional\n",
    "from tqdm import trange\n",
    "from plotly_utils import imshow\n",
    "from pprint import pprint\n",
    "from transformer_lens.utils import get_act_name, to_numpy\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Import plotly stuff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seq_index(tokens, seq_index):\n",
    "    n_tokens = len(tokens)\n",
    "    if seq_index < 0:\n",
    "        seq_index += n_tokens\n",
    "    return seq_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/anu/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 12/12 [00:07<00:00,  1.56it/s]\n",
      "100%|██████████| 12/12 [00:04<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "model, z_saes, transcoders = get_model_encoders(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z_saes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Forest is to tree as sky is to cloud\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "layer = 9\n",
    "seq_index = -2\n",
    "seq_index = process_seq_index(tokens, seq_index)\n",
    "\n",
    "layer_z = einops.rearrange(\n",
    "    cache[\"z\", layer][0, seq_index],\n",
    "    \"n_heads d_head -> (n_heads d_head)\",\n",
    ")\n",
    "\n",
    "print(layer_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, z_recon, z_acts, _, _ = z_saes[layer](layer_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([24576]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_recon.shape, z_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.0157e-02,  7.1703e-02,  2.3363e-01,  3.5861e-01,  2.0213e-01,\n",
       "         1.3408e-01,  3.2548e-01,  2.3148e-01,  3.7720e-01, -2.5025e-01,\n",
       "        -2.1057e-01, -2.3851e-01,  2.3197e-01,  2.4023e-01,  1.4382e-02,\n",
       "        -3.3220e-02,  9.0975e-02,  2.8427e-01, -3.7215e-03,  1.1231e-01,\n",
       "         2.2920e-01, -5.8852e-02,  1.5306e-01, -2.3493e-01, -1.9300e-01,\n",
       "        -1.9187e-01, -2.0395e-01,  2.0610e-01,  2.9720e-02, -3.3919e-02,\n",
       "         2.8115e-02,  2.1890e-01,  1.8478e-01, -7.7957e-02,  1.3496e-01,\n",
       "         3.4570e-01, -1.1305e-01, -1.5915e-01,  8.9999e-03, -1.4639e-01,\n",
       "         1.7077e-01,  4.3786e-02, -4.6952e-01,  2.3977e-01,  3.5124e-03,\n",
       "         2.5602e-01,  1.2772e-01,  8.4909e-02,  2.6550e-01, -1.0964e-01,\n",
       "        -1.3403e-01,  6.5226e-02,  1.6067e-01,  1.7311e-01,  1.1640e-01,\n",
       "        -3.4142e-01, -7.5401e-02, -1.2113e-01, -4.0713e-02,  7.3968e-02,\n",
       "        -2.1001e-01,  1.1112e-01, -1.1536e-01,  2.5154e-02, -8.0788e-02,\n",
       "        -1.5961e-02, -1.0148e-01, -5.4164e-02, -1.8131e-02,  1.3086e-02,\n",
       "        -9.3878e-03, -9.4767e-02,  6.2215e-02,  9.5365e-04, -3.6064e-02,\n",
       "         1.7003e-04, -4.4070e-02, -5.4879e-02,  5.5262e-02, -4.3381e-02,\n",
       "         5.4543e-03,  6.9484e-02, -3.0343e-02,  5.2811e-02,  7.8554e-02,\n",
       "        -4.7091e-03,  7.2170e-02, -3.7168e-02,  1.3843e-02, -8.6109e-02,\n",
       "         6.4880e-02,  1.2301e-01,  2.1286e-02, -3.3376e-02,  4.2404e-02,\n",
       "        -3.8966e-03, -5.8328e-02,  2.8807e-02,  2.6202e-02, -3.9664e-02,\n",
       "        -1.5327e-02, -9.2939e-02,  5.2753e-02,  1.0667e-01, -6.5152e-02,\n",
       "         2.0337e-03, -5.4132e-02, -4.5406e-03, -6.8003e-02, -1.0559e-02,\n",
       "        -1.4997e-01,  5.2665e-02, -5.8431e-03,  1.6382e-02, -9.9316e-02,\n",
       "         3.5879e-02, -1.3893e-03,  1.2696e-01,  2.1281e-01, -3.7287e-02,\n",
       "        -4.3827e-02,  6.7060e-03,  7.6208e-02,  1.8049e-01,  3.8779e-03,\n",
       "         1.0085e-01,  2.9690e-02,  1.2985e-01,  5.0229e-02,  1.0263e-02,\n",
       "         3.2467e-02,  6.0967e-02, -1.8143e-02,  1.0705e-02,  7.1061e-03,\n",
       "        -5.4681e-02,  5.5068e-02, -8.1777e-02,  3.9587e-03, -7.3618e-02,\n",
       "         1.1893e-01,  1.7830e-01, -6.7346e-02,  3.3870e-02, -2.8808e-03,\n",
       "        -4.2556e-03,  3.1167e-02, -3.0807e-02,  6.9058e-02,  4.2156e-02,\n",
       "         7.4363e-02,  1.1742e-03,  4.8773e-02, -4.7341e-02,  2.5841e-02,\n",
       "        -1.3895e-01, -5.5262e-02, -8.4043e-04,  5.8122e-02, -3.8355e-02,\n",
       "         4.5565e-03, -9.1562e-02, -1.1290e-01,  1.9757e-02,  7.1548e-03,\n",
       "        -5.1232e-02, -3.4888e-02, -3.5728e-02, -6.1911e-02, -1.0674e-03,\n",
       "         1.7462e-02, -9.1547e-02, -9.4455e-02,  5.0719e-02,  8.0085e-02,\n",
       "         4.2534e-02,  4.4573e-02, -6.5585e-02,  7.8160e-03, -4.7972e-02,\n",
       "        -2.9813e-02, -1.1434e-02, -3.8827e-02,  6.2111e-02, -6.9968e-03,\n",
       "        -2.6233e-02,  4.1635e-02, -6.0201e-02, -1.0438e-01, -5.0958e-02,\n",
       "         1.2069e-02, -3.3145e-02, -3.6130e-01,  1.4032e-01, -3.5929e-01,\n",
       "        -6.4763e-03, -1.2685e-02, -2.0505e-01, -3.6995e-01,  4.2801e-01,\n",
       "         7.3560e-01, -1.4804e-01, -6.2086e-01,  3.8475e-01, -1.7798e-02,\n",
       "        -2.3264e-01,  2.1078e-01,  3.6794e-01,  1.4795e-01,  2.9200e-01,\n",
       "        -2.7395e-02, -7.5971e-02, -4.4652e-01,  3.3754e-02,  6.3873e-03,\n",
       "        -1.0363e-01, -9.2302e-02,  3.4106e-01, -3.1455e-01, -3.4450e-01,\n",
       "         3.1612e-01, -3.8920e-01, -1.5838e-01,  1.6755e-01,  3.7453e-01,\n",
       "         1.6697e-01, -2.0397e-01, -4.0606e-01, -4.3368e-01,  2.0483e-01,\n",
       "         7.1103e-01, -8.0742e-02,  2.8017e-01, -8.4288e-02,  2.8047e-01,\n",
       "         1.6214e-03,  4.4710e-01,  2.5200e-01,  5.3183e-01,  6.4960e-01,\n",
       "        -1.9791e-01, -6.4606e-02, -8.7633e-02, -4.9241e-01, -1.2761e-01,\n",
       "         4.3007e-01,  1.4902e-02, -3.0021e-02, -2.3203e-02, -6.1289e-01,\n",
       "        -4.9403e-02, -4.5775e-02,  3.5585e-01, -3.1061e-01,  1.5773e-01,\n",
       "        -4.6617e-01,  2.8698e-02,  2.8708e-02,  2.3060e-02,  2.0904e-02,\n",
       "        -2.6989e-02, -3.3013e-03, -2.8654e-02, -6.3337e-03,  2.6673e-02,\n",
       "        -6.1523e-03, -3.5856e-02,  2.2310e-02, -3.4713e-02, -9.2379e-02,\n",
       "         9.7033e-04, -3.4354e-02,  5.0701e-02,  1.9775e-02, -7.3109e-03,\n",
       "         1.4944e-02,  3.4906e-02, -2.8154e-02,  3.1712e-02, -1.0739e-02,\n",
       "         2.0738e-02,  1.7967e-03,  1.1691e-02,  2.3181e-02,  1.6049e-02,\n",
       "         2.5205e-02, -1.2476e-03, -5.0475e-04, -3.3063e-02,  2.6694e-02,\n",
       "         1.7433e-03, -3.3463e-02, -1.2173e-02,  1.8802e-02, -2.3146e-02,\n",
       "        -1.5226e-02,  1.0615e-02,  1.0245e-02, -1.0487e-02, -1.3639e-02,\n",
       "        -2.0456e-02, -1.2435e-03,  3.3245e-02, -3.6762e-02, -4.3020e-03,\n",
       "         1.4924e-03,  1.4476e-02, -2.6116e-02, -7.4910e-03,  1.6373e-03,\n",
       "        -4.3215e-03, -1.0352e-02,  2.7554e-02,  2.5214e-02, -1.0812e-02,\n",
       "        -5.8540e-02,  7.4970e-02,  4.6669e-02, -2.6157e-02, -6.4899e-04,\n",
       "         5.4110e-02,  4.8509e-03,  6.3969e-03, -2.3126e-02,  4.9888e-02,\n",
       "         6.3301e-02,  1.9074e-02,  8.2925e-02, -1.1665e-01, -5.3562e-02,\n",
       "         3.7096e-02, -4.4508e-02,  1.5552e-01, -5.9224e-02, -3.4640e-02,\n",
       "         2.3160e-01,  4.9488e-03, -1.1841e-02,  2.3345e-02, -2.7449e-02,\n",
       "        -1.3757e-01,  5.0586e-03,  7.4319e-02, -3.4921e-02, -2.0266e-02,\n",
       "        -6.9640e-02, -6.6206e-02,  8.5040e-02, -1.1242e-02,  1.3817e-02,\n",
       "         8.6196e-02,  1.2603e-01,  9.5610e-02,  9.8175e-02,  1.1361e-01,\n",
       "        -1.6300e-02, -8.5166e-02,  2.1587e-02,  2.4249e-03, -1.4038e-01,\n",
       "         9.0484e-02,  7.6372e-02, -3.0004e-03,  4.1685e-03, -7.6811e-02,\n",
       "         8.5646e-02, -2.2225e-03, -5.2890e-02, -4.7480e-02,  8.6832e-02,\n",
       "        -2.3431e-02,  6.3335e-02, -4.3932e-02, -3.3747e-02, -2.3001e-02,\n",
       "         5.6578e-03, -4.7381e-02,  1.4210e-01, -2.2845e-02,  1.4716e-01,\n",
       "        -1.1648e-02, -1.4898e-01,  6.8109e-02, -8.5687e-02,  6.9011e-02,\n",
       "         7.6183e-02,  4.6023e-02, -3.5871e-02,  6.8097e-02,  2.4686e-02,\n",
       "         8.2762e-02, -1.1448e-02,  1.5273e-02,  8.1003e-03,  1.0911e-03,\n",
       "        -6.2565e-02, -9.2733e-02, -2.4889e-02,  3.9470e-03,  5.2588e-02,\n",
       "         4.8906e-02,  3.6732e-02,  5.8504e-03, -2.3430e-03, -5.3811e-02,\n",
       "        -3.2451e-02, -3.1400e-03, -7.4319e-02,  1.1229e-02,  1.0499e-01,\n",
       "         8.7561e-02,  9.8856e-02, -4.1466e-03, -1.8557e-02, -4.8169e-02,\n",
       "        -4.4675e-02, -2.7722e-02,  1.3478e-03,  5.3622e-02, -5.5074e-03,\n",
       "        -1.2306e-02,  9.5865e-02,  3.7361e-02, -3.8797e-03,  2.8907e-02,\n",
       "        -3.4626e-02, -1.4057e-02, -6.2505e-02,  6.1714e-02, -7.9677e-05,\n",
       "        -5.1316e-02,  3.8543e-02,  2.2385e-02, -2.2435e-02, -8.5396e-02,\n",
       "        -5.3248e-03, -1.2906e-01,  1.4429e-02, -3.0188e-02,  1.2052e-02,\n",
       "         5.5103e-02, -1.4320e-02,  1.1050e-01,  1.3922e-01,  4.8805e-02,\n",
       "        -1.7855e-02, -8.0544e-02,  1.3443e-02,  1.0889e-02, -3.9141e-02,\n",
       "        -1.1380e-02, -7.4454e-02,  6.7994e-02,  2.1088e-02, -5.8344e-02,\n",
       "        -7.4166e-02, -5.5796e-02,  5.3744e-02, -5.4156e-02,  8.5478e-02,\n",
       "         7.5480e-02,  4.7533e-02,  1.2736e-01,  1.5359e-02, -1.9788e-02,\n",
       "        -7.1859e-02, -7.8337e-02,  1.2474e-01, -1.0748e-01,  3.6559e-02,\n",
       "         7.4522e-02,  2.7045e-02, -3.4227e-02, -4.6336e-02, -1.0507e-01,\n",
       "        -2.7277e-02,  1.2004e-01,  2.4892e-02, -1.4305e-02,  9.0733e-02,\n",
       "        -1.2310e-02, -3.7741e-02, -1.4619e-01, -3.9117e-02,  2.8608e-02,\n",
       "        -1.4440e-01, -1.5376e-02, -4.8008e-02,  6.2425e-02,  5.3946e-02,\n",
       "         1.7137e-01,  2.6287e-03,  9.6329e-02,  4.3074e-02,  5.2763e-03,\n",
       "         3.9812e-03, -2.6839e-01, -6.7405e-02, -4.7178e-02, -7.3666e-02,\n",
       "        -5.4820e-02,  1.6165e-01,  8.3148e-02, -2.7482e-02,  7.8801e-02,\n",
       "        -1.5878e-01, -5.4252e-02,  7.1778e-02,  1.3765e-02, -4.7805e-03,\n",
       "         5.1345e-02,  1.3781e-01,  9.4918e-02,  3.3369e-03,  7.9123e-02,\n",
       "        -1.1041e-01, -2.5011e-02,  1.4947e-02,  2.9510e-02, -1.7711e-02,\n",
       "         2.1666e-02,  1.2793e-01,  9.3630e-03,  1.7442e-01,  3.9132e-02,\n",
       "        -1.7153e-02,  5.1810e-02, -8.4774e-02,  5.6640e-02, -1.8428e-02,\n",
       "         4.2733e-02, -1.2006e-01,  3.0109e-02,  8.4317e-02,  8.2872e-02,\n",
       "         2.2810e-02, -1.7032e-01, -3.8861e-02,  2.1838e-01, -4.6288e-02,\n",
       "        -9.3224e-03, -6.7363e-02, -1.0830e-02,  1.0472e-01,  1.6607e-01,\n",
       "        -2.3909e-02,  3.4345e-02,  1.8059e-02, -3.4484e-02,  3.6948e-02,\n",
       "         2.1337e-02, -1.4088e-03, -1.0836e-01, -3.0135e-02, -3.5445e-02,\n",
       "         5.2426e-02, -6.0407e-02,  2.3432e-02,  8.6283e-02,  1.0387e-01,\n",
       "        -4.3997e-02, -1.5608e-02, -1.0186e-01,  6.5540e-03,  1.2589e-01,\n",
       "        -3.2141e-02, -8.4288e-02,  3.9624e-02, -1.6851e-02, -7.2656e-02,\n",
       "        -1.7076e-01, -7.6897e-02, -1.4372e-02, -1.3334e-01, -7.7742e-02,\n",
       "        -1.0932e-01, -3.6110e-02,  2.4029e-02, -7.2053e-03,  4.1412e-02,\n",
       "        -1.5778e-02,  2.1568e-02,  4.8407e-02,  7.4461e-03, -8.4292e-03,\n",
       "         1.1979e-02, -3.0853e-02,  1.8715e-02, -3.7359e-02, -7.2775e-02,\n",
       "         5.4447e-02, -3.3152e-02, -3.2761e-02, -2.8695e-02, -4.6822e-03,\n",
       "        -1.8630e-02,  8.1851e-03,  2.6670e-02,  2.2539e-02,  2.5751e-02,\n",
       "        -2.5316e-02,  7.1407e-03,  2.4936e-02, -2.8091e-02, -7.6095e-03,\n",
       "         1.9387e-02, -3.9858e-02,  1.6167e-02,  3.3479e-02, -5.7864e-03,\n",
       "         5.4627e-02,  1.7848e-03,  3.3774e-03, -5.1275e-02,  1.8269e-02,\n",
       "        -9.1189e-03,  3.2399e-02, -1.2196e-02, -3.7493e-02, -6.2463e-03,\n",
       "         3.4167e-02, -5.5507e-03,  9.5715e-03,  6.8928e-04,  3.4893e-02,\n",
       "        -1.6117e-02,  2.8454e-03, -4.9257e-02,  2.3418e-02, -4.2629e-02,\n",
       "         1.9421e-02, -1.6930e-02, -7.0657e-02,  3.8802e-03,  5.6653e-03,\n",
       "        -3.8623e-02,  1.2636e-03, -2.0774e-02, -3.2147e-02, -6.1866e-03,\n",
       "        -1.4882e-01, -5.3088e-01,  5.4427e-01, -1.3918e-01,  4.0153e-03,\n",
       "        -7.1180e-03,  2.1383e-01,  7.3727e-02,  6.5781e-02,  9.3130e-02,\n",
       "        -2.2863e-01,  5.2089e-01, -5.6706e-01,  6.6902e-03, -1.0213e-01,\n",
       "         4.9261e-03, -1.7927e-01, -1.1715e-01, -6.5340e-01,  4.2722e-01,\n",
       "         1.7190e-01,  1.5358e-01, -5.4172e-01,  1.5888e-01,  1.6637e-01,\n",
       "        -4.5757e-01, -1.7466e-01, -5.5271e-02,  8.4485e-02, -1.2450e-01,\n",
       "        -6.0554e-01,  3.8523e-02,  3.0446e-01,  3.7435e-01,  1.0779e-01,\n",
       "        -3.1095e-01,  3.6497e-01,  4.8713e-02, -3.4244e-02,  2.9393e-01,\n",
       "        -9.8815e-02,  1.3803e-01,  1.6528e-01,  1.9886e-01,  9.6220e-02,\n",
       "         1.6959e-01, -1.7356e-01, -1.9332e-01, -4.3694e-01, -9.7304e-02,\n",
       "         8.7775e-01,  2.8085e-01, -3.1047e-01, -3.5174e-01, -4.4935e-01,\n",
       "        -8.8855e-01, -2.9411e-01,  1.7154e-01,  2.9034e-01, -1.9783e-01,\n",
       "         5.5114e-01, -7.4897e-02,  1.6201e-01,  6.9254e-01, -2.1384e-02,\n",
       "        -4.6329e-03, -1.9229e-02, -2.3101e-02, -1.7369e-02, -1.7062e-02,\n",
       "         5.0434e-02,  5.4698e-02, -4.6500e-02, -3.3527e-02,  1.1734e-02,\n",
       "         5.8620e-02,  3.0106e-02,  1.0657e-02,  1.8444e-02, -1.0558e-01,\n",
       "         4.2861e-03,  2.2599e-03, -1.2839e-02,  1.0346e-02, -7.7640e-03,\n",
       "         5.7306e-02, -6.3087e-03,  7.5491e-03, -5.4723e-02,  2.3860e-02,\n",
       "        -1.4859e-02, -1.5095e-02, -7.0624e-02, -1.7402e-03,  3.1253e-02,\n",
       "        -5.2236e-02,  2.9162e-03, -9.1015e-03, -2.7691e-02, -5.2190e-02,\n",
       "        -1.5115e-02,  2.0707e-03, -6.8172e-03,  8.1433e-02,  9.3382e-02,\n",
       "         1.3166e-02, -1.4100e-02,  3.1299e-02, -1.7274e-02, -4.8156e-02,\n",
       "        -2.4100e-02,  2.5027e-02, -2.2027e-02, -5.6649e-03,  2.2731e-02,\n",
       "        -2.1031e-03, -1.9559e-02, -1.7592e-02,  1.1844e-02,  8.4728e-03,\n",
       "        -3.1409e-03, -2.2095e-02,  4.2559e-03, -3.0531e-02,  3.0505e-03,\n",
       "         8.2491e-03, -4.9807e-02,  4.7248e-02], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_error = layer_z - z_recon\n",
    "sae_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767
         ],
         "xaxis": "x",
         "y": [
          0.070157185,
          0.071703464,
          0.23363343,
          0.35860986,
          0.20213172,
          0.13407618,
          0.32547545,
          0.23148203,
          0.3771962,
          -0.25025102,
          -0.21057385,
          -0.238509,
          0.2319746,
          0.2402331,
          0.014381502,
          -0.033219516,
          0.090975136,
          0.2842714,
          -0.0037215017,
          0.11231176,
          0.22920284,
          -0.058851615,
          0.15305944,
          -0.23493442,
          -0.19299644,
          -0.19186915,
          -0.20394519,
          0.20610169,
          0.02972048,
          -0.033918902,
          0.028115481,
          0.21890153,
          0.18478362,
          -0.07795742,
          0.13495895,
          0.3456966,
          -0.113054335,
          -0.15914825,
          0.008999908,
          -0.14639357,
          0.1707697,
          0.043786395,
          -0.469524,
          0.23977327,
          0.0035124123,
          0.25602478,
          0.12771574,
          0.0849089,
          0.26550326,
          -0.10964452,
          -0.13403171,
          0.06522607,
          0.16067225,
          0.17310563,
          0.11640328,
          -0.3414193,
          -0.07540128,
          -0.1211255,
          -0.040713295,
          0.0739681,
          -0.21000916,
          0.111119404,
          -0.11536366,
          0.025153637,
          -0.080788106,
          -0.015960611,
          -0.101475805,
          -0.054164417,
          -0.018131286,
          0.013085879,
          -0.009387836,
          -0.09476738,
          0.06221495,
          0.0009536505,
          -0.03606406,
          0.00017003156,
          -0.044069648,
          -0.054878898,
          0.055261828,
          -0.04338114,
          0.0054542907,
          0.06948359,
          -0.030342609,
          0.052810766,
          0.07855397,
          -0.004709075,
          0.07217042,
          -0.037168253,
          0.0138429515,
          -0.08610909,
          0.064880274,
          0.1230129,
          0.021285996,
          -0.03337574,
          0.04240424,
          -0.0038966462,
          -0.058328345,
          0.02880658,
          0.026201747,
          -0.03966359,
          -0.015327022,
          -0.092939496,
          0.052753158,
          0.10666759,
          -0.06515162,
          0.0020336546,
          -0.0541315,
          -0.0045406222,
          -0.06800298,
          -0.010559175,
          -0.14997178,
          0.052665144,
          -0.005843118,
          0.016381875,
          -0.09931649,
          0.035879247,
          -0.0013892613,
          0.12695752,
          0.2128091,
          -0.03728743,
          -0.043827455,
          0.006705964,
          0.076207526,
          0.180489,
          0.0038779303,
          0.10085451,
          0.02969017,
          0.12984827,
          0.05022925,
          0.010262713,
          0.03246741,
          0.06096696,
          -0.018143348,
          0.0107045695,
          0.007106127,
          -0.05468117,
          0.055068363,
          -0.08177699,
          0.0039587393,
          -0.073617935,
          0.1189277,
          0.17829525,
          -0.06734559,
          0.03386967,
          -0.0028807893,
          -0.004255578,
          0.031166866,
          -0.030806754,
          0.06905782,
          0.042155728,
          0.07436323,
          0.0011741631,
          0.048772752,
          -0.047340527,
          0.025840662,
          -0.13894774,
          -0.055261806,
          -0.00084042735,
          0.05812154,
          -0.038355004,
          0.0045564957,
          -0.09156241,
          -0.11289564,
          0.019756868,
          0.0071548037,
          -0.051232062,
          -0.034887955,
          -0.03572753,
          -0.061910976,
          -0.0010674149,
          0.017462194,
          -0.091547355,
          -0.0944551,
          0.05071901,
          0.08008452,
          0.042534117,
          0.04457336,
          -0.06558511,
          0.007815957,
          -0.047971696,
          -0.029812519,
          -0.011433512,
          -0.038827315,
          0.062111072,
          -0.0069967806,
          -0.026233412,
          0.041635215,
          -0.06020081,
          -0.10437739,
          -0.050957814,
          0.012068741,
          -0.0331453,
          -0.3613037,
          0.14032127,
          -0.3592913,
          -0.006476339,
          -0.012685254,
          -0.20505479,
          -0.369947,
          0.428006,
          0.7356038,
          -0.14804317,
          -0.6208635,
          0.38475084,
          -0.01779799,
          -0.23263612,
          0.21078403,
          0.36793616,
          0.1479517,
          0.2919992,
          -0.027395241,
          -0.0759715,
          -0.4465245,
          0.03375426,
          0.006387338,
          -0.103634715,
          -0.09230173,
          0.34106106,
          -0.31455392,
          -0.34450242,
          0.31612307,
          -0.3891977,
          -0.15837815,
          0.16754776,
          0.37453198,
          0.16697448,
          -0.203973,
          -0.4060619,
          -0.4336787,
          0.20483021,
          0.7110308,
          -0.0807417,
          0.2801736,
          -0.08428816,
          0.280475,
          0.0016213655,
          0.4471001,
          0.25200397,
          0.5318348,
          0.64959776,
          -0.19791062,
          -0.064605534,
          -0.08763254,
          -0.4924141,
          -0.12761384,
          0.43007228,
          0.014901744,
          -0.030021116,
          -0.02320294,
          -0.6128949,
          -0.049402855,
          -0.04577513,
          0.35585177,
          -0.3106052,
          0.15773085,
          -0.46617076,
          0.028698271,
          0.028707527,
          0.023059912,
          0.020903813,
          -0.026988555,
          -0.0033012945,
          -0.028654143,
          -0.006333679,
          0.026672699,
          -0.0061523393,
          -0.035855506,
          0.022309855,
          -0.034713086,
          -0.09237932,
          0.0009703338,
          -0.0343541,
          0.050701186,
          0.019775018,
          -0.007310887,
          0.014944104,
          0.034905743,
          -0.028154053,
          0.031711593,
          -0.01073887,
          0.020737823,
          0.0017967448,
          0.0116913915,
          0.023181358,
          0.016049411,
          0.025204875,
          -0.0012475848,
          -0.0005047545,
          -0.033063054,
          0.026694112,
          0.0017432887,
          -0.03346254,
          -0.012173414,
          0.01880166,
          -0.023146093,
          -0.015225852,
          0.010614738,
          0.010244639,
          -0.010487184,
          -0.0136390105,
          -0.020456202,
          -0.001243487,
          0.03324522,
          -0.036762394,
          -0.0043019727,
          0.0014924402,
          0.014475845,
          -0.026115924,
          -0.007490972,
          0.0016373098,
          -0.004321538,
          -0.010352191,
          0.027554385,
          0.025213758,
          -0.010811601,
          -0.05854044,
          0.074969724,
          0.04666935,
          -0.026157245,
          -0.0006489903,
          0.05411046,
          0.0048508644,
          0.0063969046,
          -0.023126483,
          0.049888067,
          0.06330094,
          0.019074291,
          0.082925156,
          -0.116645865,
          -0.053562146,
          0.037096128,
          -0.044507995,
          0.1555219,
          -0.059223816,
          -0.034640387,
          0.23159611,
          0.004948847,
          -0.011841461,
          0.023344744,
          -0.027449109,
          -0.1375674,
          0.0050585717,
          0.07431924,
          -0.03492134,
          -0.020266011,
          -0.0696401,
          -0.06620553,
          0.08504039,
          -0.011241987,
          0.013817001,
          0.08619602,
          0.12602833,
          0.095610105,
          0.09817476,
          0.11361003,
          -0.01630047,
          -0.085165955,
          0.02158676,
          0.0024248604,
          -0.14038126,
          0.090483904,
          0.07637165,
          -0.0030004168,
          0.0041684806,
          -0.07681096,
          0.08564572,
          -0.0022224914,
          -0.052890062,
          -0.04748018,
          0.08683244,
          -0.023431223,
          0.0633345,
          -0.043931603,
          -0.033746794,
          -0.023000762,
          0.005657822,
          -0.047381222,
          0.14209883,
          -0.02284516,
          0.14716116,
          -0.011648141,
          -0.14897764,
          0.06810872,
          -0.085687146,
          0.06901146,
          0.0761827,
          0.046023466,
          -0.03587103,
          0.06809695,
          0.024686217,
          0.08276228,
          -0.011447698,
          0.015273057,
          0.008100294,
          0.0010911189,
          -0.062564746,
          -0.09273311,
          -0.024888689,
          0.00394696,
          0.052588288,
          0.048906326,
          0.036731772,
          0.005850427,
          -0.0023429915,
          -0.053810813,
          -0.032450825,
          -0.0031400248,
          -0.07431946,
          0.01122937,
          0.10499043,
          0.08756145,
          0.098856494,
          -0.0041465834,
          -0.018556569,
          -0.048168793,
          -0.044674665,
          -0.02772168,
          0.0013478473,
          0.05362224,
          -0.005507365,
          -0.012305617,
          0.095865384,
          0.037360974,
          -0.0038796943,
          0.028907336,
          -0.034626354,
          -0.01405707,
          -0.062504895,
          0.061714195,
          -0.00007967651,
          -0.05131636,
          0.038543284,
          0.022385322,
          -0.022434674,
          -0.08539595,
          -0.0053248303,
          -0.12906057,
          0.014429166,
          -0.030187868,
          0.012051791,
          0.055102833,
          -0.014320491,
          0.110499024,
          0.13922366,
          0.04880511,
          -0.017854594,
          -0.08054355,
          0.013442911,
          0.0108891465,
          -0.03914064,
          -0.011380363,
          -0.0744537,
          0.06799398,
          0.021088205,
          -0.058343593,
          -0.0741663,
          -0.055795714,
          0.0537436,
          -0.05415611,
          0.08547759,
          0.07547976,
          0.0475329,
          0.12735747,
          0.0153594185,
          -0.01978764,
          -0.07185927,
          -0.07833719,
          0.12473898,
          -0.107476026,
          0.0365586,
          0.0745217,
          0.02704484,
          -0.034226775,
          -0.046336308,
          -0.10507475,
          -0.027277485,
          0.12003619,
          0.024892136,
          -0.014305353,
          0.09073324,
          -0.012310181,
          -0.03774106,
          -0.14618741,
          -0.03911723,
          0.028608013,
          -0.14439683,
          -0.015376465,
          -0.04800823,
          0.06242496,
          0.05394619,
          0.17137119,
          0.0026287436,
          0.096329264,
          0.043073803,
          0.005276317,
          0.003981175,
          -0.2683925,
          -0.06740497,
          -0.04717763,
          -0.07366636,
          -0.05482019,
          0.16164917,
          0.083147526,
          -0.027481858,
          0.07880071,
          -0.15878348,
          -0.05425217,
          0.07177827,
          0.013764568,
          -0.004780516,
          0.05134499,
          0.13780941,
          0.09491773,
          0.0033369064,
          0.07912263,
          -0.11040898,
          -0.025011294,
          0.01494699,
          0.029509814,
          -0.017711401,
          0.021666236,
          0.12792556,
          0.009363017,
          0.17442413,
          0.039132226,
          -0.017153211,
          0.0518104,
          -0.08477365,
          0.056640208,
          -0.018428398,
          0.04273282,
          -0.12006411,
          0.03010901,
          0.08431715,
          0.08287157,
          0.022809517,
          -0.17031592,
          -0.038860533,
          0.2183766,
          -0.046287775,
          -0.00932236,
          -0.067362644,
          -0.01083019,
          0.1047153,
          0.16607243,
          -0.02390917,
          0.034344837,
          0.018059313,
          -0.034484453,
          0.0369476,
          0.021337308,
          -0.0014088377,
          -0.10836038,
          -0.030135259,
          -0.035445116,
          0.05242638,
          -0.060406752,
          0.023431938,
          0.08628325,
          0.10387358,
          -0.043996923,
          -0.015608102,
          -0.10185549,
          0.006554018,
          0.12589496,
          -0.032140683,
          -0.084287934,
          0.039624453,
          -0.0168508,
          -0.072656155,
          -0.17076387,
          -0.07689749,
          -0.014372252,
          -0.1333406,
          -0.07774167,
          -0.109315924,
          -0.036109857,
          0.024029067,
          -0.007205317,
          0.04141166,
          -0.015778393,
          0.021567553,
          0.048407182,
          0.0074461326,
          -0.00842924,
          0.011979371,
          -0.030852519,
          0.018715292,
          -0.03735934,
          -0.072775096,
          0.054446675,
          -0.033152074,
          -0.032760836,
          -0.028695304,
          -0.004682243,
          -0.018629909,
          0.008185096,
          0.026669698,
          0.022539273,
          0.025751092,
          -0.02531571,
          0.0071406737,
          0.024936497,
          -0.028091017,
          -0.0076095164,
          0.019387282,
          -0.039857782,
          0.016166747,
          0.033479143,
          -0.0057863705,
          0.05462669,
          0.001784817,
          0.003377363,
          -0.051274575,
          0.018269375,
          -0.009118915,
          0.032399252,
          -0.012196317,
          -0.03749326,
          -0.0062463433,
          0.03416743,
          -0.005550742,
          0.0095714815,
          0.0006892793,
          0.034892865,
          -0.016116852,
          0.002845399,
          -0.049257454,
          0.023418434,
          -0.04262862,
          0.019420946,
          -0.016930342,
          -0.070656925,
          0.0038801525,
          0.005665306,
          -0.038623445,
          0.0012635849,
          -0.020774387,
          -0.032147285,
          -0.0061865747,
          -0.14882389,
          -0.5308794,
          0.54427433,
          -0.13917722,
          0.0040153116,
          -0.007117985,
          0.21383321,
          0.073726825,
          0.06578128,
          0.093130365,
          -0.22862786,
          0.52088916,
          -0.5670623,
          0.006690193,
          -0.10212731,
          0.004926145,
          -0.17926842,
          -0.117145084,
          -0.6534019,
          0.42722404,
          0.17189871,
          0.15358073,
          -0.54171646,
          0.15887688,
          0.16636896,
          -0.45757204,
          -0.1746554,
          -0.055270918,
          0.08448525,
          -0.124504775,
          -0.6055395,
          0.038522802,
          0.30446365,
          0.37434667,
          0.10779241,
          -0.31094915,
          0.3649682,
          0.04871277,
          -0.034243554,
          0.29392612,
          -0.098814845,
          0.13803332,
          0.16528147,
          0.19886015,
          0.09621999,
          0.16958785,
          -0.17356268,
          -0.19331704,
          -0.43693572,
          -0.0973039,
          0.8777532,
          0.2808532,
          -0.3104695,
          -0.3517381,
          -0.44935226,
          -0.88854545,
          -0.2941098,
          0.17154112,
          0.2903442,
          -0.19782627,
          0.55114055,
          -0.07489701,
          0.16201456,
          0.69254136,
          -0.021384029,
          -0.004632855,
          -0.019228982,
          -0.023100756,
          -0.017369017,
          -0.017062336,
          0.0504337,
          0.05469816,
          -0.04650048,
          -0.033526585,
          0.0117337,
          0.058620345,
          0.030105572,
          0.010657296,
          0.018444104,
          -0.105575144,
          0.00428614,
          0.0022598896,
          -0.01283904,
          0.010346486,
          -0.0077640377,
          0.057305988,
          -0.0063087046,
          0.0075491094,
          -0.054723367,
          0.023860306,
          -0.014859125,
          -0.01509472,
          -0.07062356,
          -0.001740165,
          0.03125298,
          -0.052236453,
          0.0029161796,
          -0.00910148,
          -0.027690912,
          -0.05218961,
          -0.01511538,
          0.002070725,
          -0.0068171583,
          0.081432976,
          0.09338166,
          0.013165578,
          -0.014100432,
          0.031299308,
          -0.017274298,
          -0.048155878,
          -0.024099842,
          0.02502735,
          -0.022027478,
          -0.0056649446,
          0.022731267,
          -0.0021031126,
          -0.01955947,
          -0.017592207,
          0.011843741,
          0.0084728105,
          -0.0031409413,
          -0.022094816,
          0.004255902,
          -0.03053088,
          0.0030505434,
          0.0082491115,
          -0.049807142,
          0.0472484
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(to_numpy(sae_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Found cached dataset parquet (/Users/charlesoneill/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adaa90f4fe348388624a2f7ca7a451a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"NeelNanda/pile-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24280/24280 [00:30<00:00, 786.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the huggingface dataset up into seq_len text\n",
    "seq_len = 128\n",
    "batch_size = 4096\n",
    "model_name = 'gpt2-small'\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device='cpu')\n",
    "\n",
    "tokenized_dataset = []\n",
    "# Concat all the text together\n",
    "text = \" \".join(dataset['train']['text'])\n",
    "\n",
    "# Tokenize the text\n",
    "for i in trange(0, len(text), 2500):\n",
    "    tokens = model.to_tokens(text[i:i+2500]).squeeze()\n",
    "    # Split into seq_len chunks\n",
    "    for j in range(0, len(tokens), seq_len):\n",
    "        tokenized_dataset.append(tokens[j:j+seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only examples with seq_len 128\n",
    "tokenized_dataset = [x for x in tokenized_dataset if len(x) == seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118061"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert all tensors have shape seq_length\n",
    "for i, tokens in enumerate(tokenized_dataset):\n",
    "    assert tokens.shape[0] == seq_len, f\"Token {i} has shape {tokens.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# Turn tokenized_dataset (a list of tensors) into a Pytorch Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_dataset[idx]\n",
    "    \n",
    "dataset = TokenizedDataset(tokenized_dataset)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(next(iter(dataloader)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2bbf23470>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable torch grad\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1845 [01:07<2:38:35,  5.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m z_acts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m----> 6\u001b[0m     logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     z \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer] \u001b[38;5;66;03m# batch_size x seq_len x n_heads x d_head\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m logits\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:634\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    619\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    627\u001b[0m ]:\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    638\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    639\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    640\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:555\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    552\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    553\u001b[0m         )\n\u001b[0;32m--> 555\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/components.py:1210\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1204\u001b[0m         resid_mid\n\u001b[1;32m   1205\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[1;32m   1208\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n\u001b[1;32m   1209\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_out(\n\u001b[0;32m-> 1210\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid_mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_post(\n\u001b[1;32m   1213\u001b[0m         resid_mid \u001b[38;5;241m+\u001b[39m mlp_out\n\u001b[1;32m   1214\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/components.py:965\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    960\u001b[0m pre_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_pre(\n\u001b[1;32m    961\u001b[0m     einsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model, d_model d_mlp -> batch pos d_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_in)\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_in\n\u001b[1;32m    963\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mact_fn\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ln\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 965\u001b[0m     post_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_post(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_act\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     mid_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(pre_act))  \u001b[38;5;66;03m# [batch, pos, d_mlp]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/transformer_lens/utils.py:171\u001b[0m, in \u001b[0;36mgelu_new\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgelu_new\u001b[39m(\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28minput\u001b[39m: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    164\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Implementation of GeLU used by GPT2 - subtly different from PyTorch's\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 171\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sae = z_saes[9]\n",
    "\n",
    "# Get all z activations\n",
    "z_acts = []\n",
    "for batch in tqdm(dataloader):\n",
    "    logits, cache = model.run_with_cache(batch)\n",
    "    z = cache[\"z\", layer] # batch_size x seq_len x n_heads x d_head\n",
    "    del logits\n",
    "    del cache\n",
    "    z = einops.rearrange(\n",
    "        z, \n",
    "        \"b s n d -> (b s) (n d)\"\n",
    "    )\n",
    "    z_acts.append(z)\n",
    "\n",
    "# Stack all z activations along first dimension\n",
    "z_acts = torch.cat(z_acts, dim=0)\n",
    "z_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensors\n",
    "torch.save(z_acts, \"z_acts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load z_acts\n",
    "z_acts = torch.load(\"z_acts.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([709632, 64])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SAE dataset\n",
    "class SAEDataset(Dataset):\n",
    "    def __init__(self, z_acts):\n",
    "        self.z_acts = z_acts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.z_acts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z_acts[idx]\n",
    "    \n",
    "sae_dataset = SAEDataset(z_acts)\n",
    "\n",
    "# Create SAE dataloader\n",
    "sae_dataloader = DataLoader(sae_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/709632 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (768) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sae_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m tqdm(z_acts):\n\u001b[0;32m----> 4\u001b[0m     _, z_recon, z_acts, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43msae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape, z_recon\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m     sae_error \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m-\u001b[39m z_recon\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anu/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/transcoder-sae-circuits/src/z_sae.py:60\u001b[0m, in \u001b[0;36mZSAE.forward\u001b[0;34m(self, x, per_token)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, per_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Move x to cfg device\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 60\u001b[0m     x_cent \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_dec\u001b[49m\n\u001b[1;32m     61\u001b[0m     acts \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x_cent \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_enc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_enc)  \u001b[38;5;66;03m# [batch_size, d_hidden]\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     x_reconstruct \u001b[38;5;241m=\u001b[39m acts \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_dec \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_dec  \u001b[38;5;66;03m# [batch_size, act_size]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (768) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Get SAE errors on each z_acts - we need to store the errors, and the original z_acts\n",
    "sae_errors = []\n",
    "for z in tqdm(z_acts):\n",
    "    _, z_recon, z_acts, _, _ = sae(z)\n",
    "    print(z.shape, z_recon.shape)\n",
    "    sae_error = z - z_recon\n",
    "    sae_errors.append(sae_error)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 49,\n",
       " 'batch_size': 4096,\n",
       " 'buffer_mult': 384,\n",
       " 'lr': 0.0012,\n",
       " 'num_tokens': 2000000000,\n",
       " 'l1_coeff': 1.2,\n",
       " 'beta1': 0.9,\n",
       " 'beta2': 0.99,\n",
       " 'dict_mult': 32,\n",
       " 'seq_len': 128,\n",
       " 'enc_dtype': 'fp32',\n",
       " 'model_name': 'gpt2-small',\n",
       " 'site': 'z',\n",
       " 'layer': 9,\n",
       " 'device': 'cpu',\n",
       " 'reinit': 'reinit',\n",
       " 'head': 'cat',\n",
       " 'concat_heads': True,\n",
       " 'resample_scheme': 'anthropic',\n",
       " 'anthropic_neuron_resample_scale': 0.2,\n",
       " 'dead_direction_cutoff': 1e-06,\n",
       " 're_init_every': 25000,\n",
       " 'anthropic_resample_last': 12500,\n",
       " 'resample_factor': 0.01,\n",
       " 'num_resamples': 4,\n",
       " 'wandb_project_name': 'gpt2-L9-20240117',\n",
       " 'wandb_entity': 'ckkissane',\n",
       " 'save_state_dict_every': 50000,\n",
       " 'b_dec_init': 'zeros',\n",
       " 'sched_type': 'cosine_warmup',\n",
       " 'sched_epochs': 1000,\n",
       " 'sched_lr_factor': 0.1,\n",
       " 'sched_warmup_epochs': 1000,\n",
       " 'sched_finish': True,\n",
       " 'anthropic_resample_batches': 100,\n",
       " 'eval_every': 1000,\n",
       " 'model_batch_size': 512,\n",
       " 'buffer_size': 1572864,\n",
       " 'buffer_batches': 12288,\n",
       " 'act_name': 'blocks.9.attn.hook_z',\n",
       " 'act_size': 768,\n",
       " 'dict_size': 24576,\n",
       " 'name': 'gpt2-small_9_24576_z'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_saes[9].cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
