{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from data.greater_than_dataset import generate_greater_than_dataset\n",
    "from circuit_discovery import CircuitDiscovery, only_feature\n",
    "from circuit_lens import CircuitComponent\n",
    "from plotly_utils import *\n",
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "from data.greater_than_dataset import GT_GROUND_TRUTH_HEADS\n",
    "from memory import get_gpu_memory\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import get_attn_head_roc\n",
    "\n",
    "\n",
    "# %%\n",
    "torch.set_grad_enabled(False)\n",
    "get_gpu_memory()\n",
    "# %%\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "dataset_gt = generate_greater_than_dataset(N=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# dataset_prompts = generate_greater_than_dataset(N=100)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def component_filter(component: str):\n",
    "    return component in [\n",
    "        CircuitComponent.Z_FEATURE,\n",
    "        CircuitComponent.MLP_FEATURE,\n",
    "        CircuitComponent.ATTN_HEAD,\n",
    "        CircuitComponent.UNEMBED,\n",
    "        # CircuitComponent.UNEMBED_AT_TOKEN,\n",
    "        CircuitComponent.EMBED,\n",
    "        CircuitComponent.POS_EMBED,\n",
    "        # CircuitComponent.BIAS_O,\n",
    "        CircuitComponent.Z_SAE_ERROR,\n",
    "        # CircuitComponent.Z_SAE_BIAS,\n",
    "        # CircuitComponent.TRANSCODER_ERROR,\n",
    "        # CircuitComponent.TRANSCODER_BIAS,\n",
    "    ]\n",
    "\n",
    "\n",
    "pass_based = True\n",
    "\n",
    "passes = 5\n",
    "node_contributors = 1\n",
    "first_pass_minimal = True\n",
    "\n",
    "sub_passes = 3\n",
    "do_sub_pass = True #False\n",
    "layer_thres = 9\n",
    "minimal = True\n",
    "\n",
    "\n",
    "num_greedy_passes = 20\n",
    "k = 1\n",
    "N = 30\n",
    "\n",
    "thres = 4\n",
    "\n",
    "\n",
    "# # Danny and Charlie... Charlie gave shit to Danny\n",
    "# # Danny and Charlie... Charlie gave shit to Charlie\n",
    "# # Danny and Charlie... Danny gave shit to Danny\n",
    "# #\n",
    "\n",
    "def strategy(cd: CircuitDiscovery):\n",
    "    if pass_based:\n",
    "        for _ in range(passes):\n",
    "            cd.add_greedy_pass(contributors_per_node=node_contributors, minimal=first_pass_minimal)\n",
    "\n",
    "            if do_sub_pass:\n",
    "                for _ in range(sub_passes):\n",
    "                    cd.add_greedy_pass_against_all_existing_nodes(contributors_per_node=node_contributors, skip_z_features=True, layer_threshold=layer_thres, minimal=minimal)\n",
    "    else:\n",
    "        for _ in range(num_greedy_passes):\n",
    "            cd.greedily_add_top_contributors(k=k, reciever_threshold=thres)\n",
    "\n",
    "\n",
    "\n",
    "task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = task_eval.get_attn_head_freqs_over_dataset(N=N, return_freqs=True)\n",
    "\n",
    "# %%\n",
    "ground = task_eval.get_faithfulness_curve_over_data(N=20, attn_head_freq_n=10, faithfulness_intervals=30, rand=False, ioi_ground=True, task='ioi')\n",
    "base = task_eval.get_faithfulness_curve_over_data(N=20, attn_head_freq_n=10, faithfulness_intervals=30, rand=False, ioi_ground=False, task='ioi')\n",
    "\n",
    "radd = []\n",
    "for _ in trange(20):\n",
    "    radd.append(task_eval.get_faithfulness_curve_over_data(N=20, attn_head_freq_n=10, faithfulness_intervals=30, rand=True, ioi_ground=False, visualize=False))\n",
    "\n",
    "\n",
    "# %%\n",
    "big_rad = {}\n",
    "for rad in radd:\n",
    "    for k, v in rad.items():\n",
    "        if k not in big_rad:\n",
    "            big_rad[k] = 0\n",
    "        big_rad[k] += v\n",
    "\n",
    "for k in big_rad:\n",
    "    big_rad[k] /= 20\n",
    "\n",
    "rad = big_rad\n",
    "\n",
    "# %%\n",
    "plt.plot([float(k) for k in ground.keys()], ground.values(), label=\"Ground Truth\")\n",
    "plt.plot([float(k) for k in rad.keys()], base.values(), label=\"Circuit Discovery\")\n",
    "plt.plot([float(k) for k in base.keys()], rad.values(), label=\"Random\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.2)\n",
    "plt.title(\"IOI Faithfulness (Mean Ablation)\")\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.margins(0)\n",
    "plt.ylabel(\"Normalized KL\")\n",
    "plt.xlabel(\"# Heads\")\n",
    "# ax.spines['left'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(ground.keys()), y=list(ground.values()),\n",
    "                         mode='lines+markers',\n",
    "                         name='Ground Truth',\n",
    "                         line=dict(color='blue', width=2),\n",
    "                         marker=dict(size=8, symbol='circle', color='blue')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(rad.keys()), y=list(base.values()),\n",
    "                         mode='lines+markers',\n",
    "                         name='Circuit Discovery',\n",
    "                         line=dict(color='red', width=2),\n",
    "                         marker=dict(size=8, symbol='square', color='red')))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(base.keys()), y=list(rad.values()),\n",
    "                         mode='lines+markers',\n",
    "                         name='Random',\n",
    "                         line=dict(color='green', width=2),\n",
    "                         marker=dict(size=8, symbol='triangle-up', color='green')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='IOI Faithfulness (Mean Ablation)',\n",
    "    xaxis=dict(title='# Heads', showgrid=False, zeroline=False, showline=True, linewidth=1, linecolor='black', mirror=True),\n",
    "    yaxis=dict(title='Normalized KL', showgrid=False, zeroline=False, showline=True, linewidth=1, linecolor='black', mirror=True),\n",
    "    font=dict(size=14),\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600,\n",
    "    legend=dict(x=0.7, y=0.9, borderwidth=1, bordercolor='black', bgcolor='rgba(255, 255, 255, 0.8)'),\n",
    "    plot_bgcolor='white',\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "# fig.update_xaxes(tickvals=list(ground.keys()))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import time\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task_evaluation import TaskEvaluation\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from data.greater_than_dataset import generate_greater_than_dataset\n",
    "from circuit_discovery import CircuitDiscovery, only_feature\n",
    "from circuit_lens import CircuitComponent\n",
    "from plotly_utils import *\n",
    "from data.ioi_dataset import IOI_GROUND_TRUTH_HEADS\n",
    "from data.greater_than_dataset import GT_GROUND_TRUTH_HEADS\n",
    "from memory import get_gpu_memory\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import get_attn_head_roc\n",
    "\n",
    "\n",
    "# %%\n",
    "torch.set_grad_enabled(False)\n",
    "# %%\n",
    "\n",
    "\n",
    "#dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "dataset_prompts = generate_greater_than_dataset(N=100)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def component_filter(component: str):\n",
    "    return component in [\n",
    "        CircuitComponent.Z_FEATURE,\n",
    "        CircuitComponent.MLP_FEATURE,\n",
    "        CircuitComponent.ATTN_HEAD,\n",
    "        CircuitComponent.UNEMBED,\n",
    "        # CircuitComponent.UNEMBED_AT_TOKEN,\n",
    "        CircuitComponent.EMBED,\n",
    "        CircuitComponent.POS_EMBED,\n",
    "        # CircuitComponent.BIAS_O,\n",
    "        CircuitComponent.Z_SAE_ERROR,\n",
    "        # CircuitComponent.Z_SAE_BIAS,\n",
    "        # CircuitComponent.TRANSCODER_ERROR,\n",
    "        # CircuitComponent.TRANSCODER_BIAS,\n",
    "    ]\n",
    "\n",
    "\n",
    "pass_based = True\n",
    "\n",
    "passes = 5\n",
    "node_contributors = 1\n",
    "first_pass_minimal = True\n",
    "\n",
    "sub_passes = 3\n",
    "do_sub_pass = False\n",
    "layer_thres = 9\n",
    "minimal = True\n",
    "\n",
    "\n",
    "num_greedy_passes = 20\n",
    "k = 1\n",
    "N = 30\n",
    "\n",
    "thres = 4\n",
    "\n",
    "def strategy(cd: CircuitDiscovery):\n",
    "    if pass_based:\n",
    "        for _ in range(passes):\n",
    "            cd.add_greedy_pass(contributors_per_node=node_contributors, minimal=first_pass_minimal)\n",
    "\n",
    "            if do_sub_pass:\n",
    "                for _ in range(sub_passes):\n",
    "                    cd.add_greedy_pass_against_all_existing_nodes(contributors_per_node=node_contributors, skip_z_features=True, layer_threshold=layer_thres, minimal=minimal)\n",
    "    else:\n",
    "        for _ in range(num_greedy_passes):\n",
    "            cd.greedily_add_top_contributors(k=k, reciever_threshold=thres)\n",
    "\n",
    "\n",
    "\n",
    "task_eval = TaskEvaluation(prompts=dataset_prompts, circuit_discovery_strategy=strategy, allowed_components_filter=component_filter)\n",
    "\n",
    "cd = task_eval.get_circuit_discovery_for_prompt(20)\n",
    "# f = task_eval.get_features_at_heads_over_dataset(N=30)\n",
    "N = 100\n",
    "\n",
    "attn_freqs = task_eval.get_attn_head_freqs_over_dataset(N=N, subtract_counter_factuals=False, return_freqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def get_attn_head_roc(ground_truth, data, task_name, visualize=True, additional_title=\"\"):\n",
    "    fp, tp, thresh = metrics.roc_curve(ground_truth.flatten(), data.flatten())\n",
    "    score = metrics.roc_auc_score(ground_truth.flatten(), data.flatten())\n",
    "\n",
    "    if visualize:\n",
    "        print(\"Score:\", score)\n",
    "\n",
    "        # Create the ROC curve with flat lines and vertical lines\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "\n",
    "        for i in range(len(fp)):\n",
    "            x_coords.append(fp[i])\n",
    "            y_coords.append(tp[i])\n",
    "\n",
    "            if i < len(fp) - 1:\n",
    "                x_coords.append(fp[i])\n",
    "                y_coords.append(tp[i+1])\n",
    "                x_coords.append(fp[i+1])\n",
    "                y_coords.append(tp[i+1])\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=x_coords, y=y_coords,\n",
    "                                 mode='lines',\n",
    "                                 name='ROC Curve',\n",
    "                                 line=dict(color='blue', width=2)))\n",
    "\n",
    "        fig.add_shape(type='line',\n",
    "                      x0=0, y0=0, x1=1, y1=1,\n",
    "                      line=dict(color='red', width=2, dash='dash'),\n",
    "                      name='Random Guess')\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"ROC Curve for {task_name} \" + additional_title,\n",
    "            xaxis=dict(title='False Positive Rate', showgrid=False, zeroline=False),\n",
    "            yaxis=dict(title='True Positive Rate', showgrid=False, zeroline=False),\n",
    "            font=dict(size=14),\n",
    "            template='plotly_white',\n",
    "            width=600,\n",
    "            height=600,\n",
    "            legend=dict(x=0.7, y=0.2, borderwidth=1, bordercolor='black', bgcolor='rgba(255, 255, 255, 0.8)'),\n",
    "            plot_bgcolor='white',\n",
    "            hovermode='closest'\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(range=[0, 1.01])\n",
    "        fig.update_yaxes(range=[0, 1.01])\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    return score, fp, tp, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOI_GROUND_TRUTH_DATA = torch.load(\"data/ioi_ground_truth.pt\")\n",
    "\n",
    "# IOI_GROUND_TRUTH_HEADS = torch.zeros(12, 12)\n",
    "\n",
    "# for layer, head in IOI_GROUND_TRUTH_DATA:\n",
    "#     IOI_GROUND_TRUTH_HEADS[layer, head] = 1\n",
    "\n",
    "# ground_truth = IOI_GROUND_TRUTH_HEADS.flatten()\n",
    "\n",
    "GT_GROUND_TRUTH_DATA = torch.load(\"data/gt_ground_truth.pt\")\n",
    "\n",
    "GT_GROUND_TRUTH_HEADS = torch.zeros(12, 12)\n",
    "\n",
    "for layer, head in GT_GROUND_TRUTH_DATA:\n",
    "    GT_GROUND_TRUTH_HEADS[layer, head] = 1\n",
    "\n",
    "ground_truth = GT_GROUND_TRUTH_HEADS.flatten()\n",
    "\n",
    "# fp, tp, thresh = get_attn_head_roc(ground_truth, a.flatten().softmax(dim=-1), \"IOI\", visualize=True, additional_title=\"(No Counterfactuals)\")\n",
    "score, _, _, _ = get_attn_head_roc(ground_truth, attn_freqs.flatten().softmax(dim=-1), \"GT\", visualize=True, additional_title=\"(No Counterfactuals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling IOI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we going to do this?\n",
    "* We will work from the bottom up (starting at bottom nodes of our graph and moving onto connected components).\n",
    "* Get feature families for each component. \n",
    "* Get max-activating examples for each feature family.\n",
    "* Get token contributions for these max-activating examples - combine this into one prompt.\n",
    "* Get the max-activating examples/tokens on IOI specific inputs, as well as token contributions.\n",
    "* Provide a description of the IOI task in the prompt. \n",
    "* Provide the feature interpretation of every component feeding in to our current component.\n",
    "* Get IOI-specific interpretation of component.\n",
    "\n",
    "Baseline:\n",
    "* One circuit at a time, same process. \n",
    "\n",
    "\n",
    "Extensions (not for now):\n",
    "* Feature co-occurrences - not only what our component is doing, but how it passes information between nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature families for each component\n",
    "\n",
    "from autointerpretability import *\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi', N=20)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_top_k_feature_tuples_for_component(co_occurrence_dict, component_str, k=5):\n",
    "    # Parse the component string to get the appropriate tuple key\n",
    "    if component_str.startswith(\"MLP\"):\n",
    "        layer = int(component_str[3:])\n",
    "        component = ('mlp_feature', layer)\n",
    "    elif component_str.startswith(\"L\") and \"H\" in component_str:\n",
    "        layer, head = map(int, component_str[1:].split(\"H\"))\n",
    "        component = ('attn_head', layer, head)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid component format: {component_str}\")\n",
    "\n",
    "    # Use a Counter to count the occurrences of each tuple\n",
    "    global_counter = Counter()\n",
    "\n",
    "    # Iterate through the co-occurrence dictionary\n",
    "    for comp_pair, co_occurrences in co_occurrence_dict.items():\n",
    "        comp1, comp2 = comp_pair\n",
    "\n",
    "        if comp1 == component or comp2 == component:\n",
    "            for feature_tuple in co_occurrences:\n",
    "                global_counter[(comp_pair, feature_tuple)] += 1\n",
    "\n",
    "    # Get the top-k tuples by count\n",
    "    top_k_tuples = global_counter.most_common(k)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    top_k_dict = defaultdict(dict)\n",
    "    \n",
    "    for (comp_pair, feature_tuple), count in top_k_tuples:\n",
    "        top_k_dict[comp_pair][feature_tuple] = count\n",
    "\n",
    "    return top_k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(cp.circuit_hypergraph['L2_H2']['features']))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, z_saes, transcoders = get_model_encoders('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from aug_interp_prompts import main_aug_interp_prompt, main_aug_interp_prompt_v2\n",
    "from openai_utils import gen_openai_completion, get_response\n",
    "from autointerpretability import *\n",
    "from discovery_strategies import (\n",
    "    create_filter,\n",
    "    create_simple_greedy_strategy,\n",
    "    create_top_contributor_strategy,\n",
    ")\n",
    "from max_act_analysis import MaxActAnalysis\n",
    "\n",
    "features = list(set(cp.circuit_hypergraph['L2_H2']['features']))\n",
    "\n",
    "#feature = 19042\n",
    "layer = 2\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.append(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.append(mini_examples_ioi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from typing import List, Tuple\n",
    "\n",
    "def main_aug_interp_prompt_ioi(\n",
    "    examples: List[str], examples_ioi: List[str], token_lr=(\"<<\", \">>\"), context_lr=(\"[[\", \"]]\")\n",
    "):\n",
    "    tl, tr = token_lr\n",
    "    cl, cr = context_lr\n",
    "\n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Indirect Object Identification (IOI). Here's how you will complete this task: #}\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to figure out what sort of behavior this neuron is responsible for -- namely, when this neuron fires, what kind of predictions does this neuron promote in the context of the specific task of Indirect Object Identification (IOI)? Here's how you'll complete the task:\n",
    "\n",
    "INPUT_DESCRIPTION: \n",
    "You will be given several examples of text that activate the neuron. First we'll provide the example text without any annotations, and then we'll provide the same text with annotations that show the specific tokens that caused the neuron to activate and context about why the neuron fired.\n",
    "\n",
    "The specific token that the neuron activates on will be the last token in the sequence, and will appear between {{tl}} and {{tr}} (like {{tl}}this{{tr}}).  \n",
    "\n",
    "Additionally, each sequence will have tokens enclosed between {{cl}} and {{cr}} (like {{cl}}this{{cr}}). From previous analysis, we know that these tokens form the context for why our neuron fires on the token enclosed in {{tl}} and {{tr}} (in addition to the value of the actual token itself). Note that we treat the group of tokens enclosed between {{cl}} and {{cr}} as the \"context\" for why the neuron fired.\n",
    "\n",
    "We will provide both general examples and specific examples related to the task of Indirect Object Identification (IOI).\n",
    "\n",
    "Task Description: A sentence containing indirect object identification (IOI) has an initial dependent clause, e.g. \"When Mary and John went to the store\", and a main clause, e.g. \"John gave a bottle of milk to Mary\". The initial clause introduces the indirect object (IO) \"Mary\" and the subject (S) \"John\". The main clause refers to the subject a second time, and in all our examples of IOI, the subject gives an object to the IO. The IOI task is to predict the final token in the sentence to be the IO. We use 'S1' and 'S2' to refer to the first and second occurrences of the subject, when we want to specify position.\n",
    "\n",
    "Given these examples, complete the following steps.\n",
    "\n",
    "OUTPUT_DESCRIPTION:\n",
    "\n",
    "Step 1: Based on the general examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 2: Based on the general examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 3: Write down several general shared features of the general text examples.\n",
    "Step 4: Based on the IOI examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 5: Based on the IOI examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 6: Write down several general shared features of the IOI text examples.\n",
    "Step 7: Based on the patterns you found between the activating token and the relevant context in both general and IOI examples, write down your best explanation for what this neuron is responsible for. Propose your explanation in the following form: \n",
    "[EXPLANATION]: <your explanation>\n",
    "\n",
    "Guidelines:\n",
    "- Try to produce a final explanation that's both concise and general to the examples provided.\n",
    "- Your explanation should be short: 1-2 sentences.\n",
    "- Specifically address the neuron's role in the context of the IOI task, explaining its specific function in relation to predicting the indirect object.\n",
    "\n",
    "INPUT:\n",
    "\n",
    "General Examples:\n",
    "{% for example in examples %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "IOI Task Examples:\n",
    "{% for example in examples_ioi %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "OUTPUT:\n",
    "                         \n",
    "Step 1:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"tl\": tl, \"tr\": tr, \"cl\": cl, \"cr\": cr, \"examples\": examples, \"examples_ioi\": examples_ioi}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = main_aug_interp_prompt_ioi(mini_examples_owt_overall, mini_examples_ioi_overall)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2h2_interp = \"Predicting conjunctions following the subject's name.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L0_H1']['features'])) if x!=-1]\n",
    "\n",
    "#feature = 19042\n",
    "layer = 0\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.append(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.append(mini_examples_ioi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of lists into a single list\n",
    "mini_examples_owt_overall = [item for sublist in mini_examples_owt_overall for item in sublist]\n",
    "mini_examples_ioi_overall = [item for sublist in mini_examples_ioi_overall for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = main_aug_interp_prompt_ioi(mini_examples_owt_overall, mini_examples_ioi_overall)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0h1_interp = \"Identifying the later appearance of the indirect object in a sentence structure where the indirect object is being given something.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L3_H0']['features'])) if x!=-1]\n",
    "\n",
    "# #feature = 19042\n",
    "layer = 3\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "\n",
    "p = main_aug_interp_prompt_ioi(mini_examples_owt_overall, mini_examples_ioi_overall)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3h0_interp = \"Aids in flagging when and where the subject or indirect object from a prior clause reappears in the text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L4_H11']['features'])) if x!=-1]\n",
    "print(features)\n",
    "\n",
    "# #feature = 19042\n",
    "layer = 4\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "\n",
    "p = main_aug_interp_prompt_ioi(mini_examples_owt_overall, mini_examples_ioi_overall)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4h11_interp = \"\"\" \n",
    "Predicts a determiner that is following a structure of \"<Name_1> and <Name_2>\", signalling the association between two entities or characters and promoting predictions in the context of an action or state involving them. In the context of the IOI task, this neuron helps identify and predict an interaction between the subject and the indirect object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def main_aug_interp_prompt_ioi(\n",
    "    examples: List[str], examples_ioi: List[str], \n",
    "    token_lr=(\"<<\", \">>\"), context_lr=(\"[[\", \"]]\")\n",
    "):\n",
    "    tl, tr = token_lr\n",
    "    cl, cr = context_lr\n",
    "\n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Indirect Object Identification (IOI). Here's how you will complete this task: #}\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to figure out what sort of behavior this neuron is responsible for -- namely, when this neuron fires, what kind of predictions does this neuron promote in the context of the specific task of Indirect Object Identification (IOI)? Here's how you'll complete the task:\n",
    "\n",
    "INPUT_DESCRIPTION: \n",
    "You will be given several examples of text that activate the neuron. First we'll provide the example text without any annotations, and then we'll provide the same text with annotations that show the specific tokens that caused the neuron to activate and context about why the neuron fired.\n",
    "\n",
    "The specific token that the neuron activates on will be the last token in the sequence, and will appear between {{tl}} and {{tr}} (like {{tl}}this{{tr}}).  \n",
    "\n",
    "Additionally, each sequence will have tokens enclosed between {{cl}} and {{cr}} (like {{cl}}this{{cr}}). From previous analysis, we know that these tokens form the context for why our neuron fires on the token enclosed in {{tl}} and {{tr}} (in addition to the value of the actual token itself). Note that we treat the group of tokens enclosed between {{cl}} and {{cr}} as the \"context\" for why the neuron fired.\n",
    "\n",
    "We will provide both general examples and specific examples related to the task of Indirect Object Identification (IOI).\n",
    "\n",
    "Task Description: A sentence containing indirect object identification (IOI) has an initial dependent clause, e.g. \"When Mary and John went to the store\", and a main clause, e.g. \"John gave a bottle of milk to Mary\". The initial clause introduces the indirect object (IO) \"Mary\" and the subject (S) \"John\". The main clause refers to the subject a second time, and in all our examples of IOI, the subject gives an object to the IO. The IOI task is to predict the final token in the sentence to be the IO. We use 'S1' and 'S2' to refer to the first and second occurrences of the subject, when we want to specify position.\n",
    "\n",
    "Given these examples, complete the following steps.\n",
    "\n",
    "OUTPUT_DESCRIPTION:\n",
    "\n",
    "Step 1: Based on the general examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 2: Based on the general examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 3: Write down several general shared features of the general text examples.\n",
    "Step 4: Based on the IOI examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 5: Based on the IOI examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 6: Write down several general shared features of the IOI text examples.\n",
    "Step 7: Based on the patterns you found between the activating token and the relevant context in both general and IOI examples, write down your best explanation for what this neuron is responsible for. Propose your explanation in the following form: \n",
    "[EXPLANATION]: <your explanation>\n",
    "\n",
    "Guidelines:\n",
    "- Try to produce a final explanation that's both concise and general to the examples provided.\n",
    "- Your explanation should be short: 1-2 sentences.\n",
    "- Specifically address the neuron's role in the context of the IOI task, explaining its specific function in relation to predicting the indirect object.\n",
    "- If provided, incorporate the interpretation of the previous neurons into your explanation, considering how the current neuron processes and uses the information from these previous neurons.\n",
    "\n",
    "INPUT:\n",
    "\n",
    "General Examples:\n",
    "{% for example in examples %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "IOI Task Examples:\n",
    "{% for example in examples_ioi %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "OUTPUT:\n",
    "                         \n",
    "Step 1:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"tl\": tl, \"tr\": tr, \"cl\": cl, \"cr\": cr, \"examples\": examples, \"examples_ioi\": examples_ioi, \"incoming_information\": incoming_information}\n",
    "    )\n",
    "\n",
    "def main_aug_interp_prompt_ioi_incoming(\n",
    "    examples: List[str], examples_ioi: List[str], \n",
    "    incoming_information: Optional[List[Tuple[str, str]]] = None, \n",
    "    token_lr=(\"<<\", \">>\"), context_lr=(\"[[\", \"]]\")\n",
    "):\n",
    "    tl, tr = token_lr\n",
    "    cl, cr = context_lr\n",
    "\n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Indirect Object Identification (IOI). Here's how you will complete this task: #}\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to figure out what sort of behavior this neuron is responsible for -- namely, when this neuron fires, what kind of predictions does this neuron promote in the context of the specific task of Indirect Object Identification (IOI)? Here's how you'll complete the task:\n",
    "\n",
    "INPUT_DESCRIPTION: \n",
    "You will be given several examples of text that activate the neuron. First we'll provide the example text without any annotations, and then we'll provide the same text with annotations that show the specific tokens that caused the neuron to activate and context about why the neuron fired.\n",
    "\n",
    "The specific token that the neuron activates on will be the last token in the sequence, and will appear between {{tl}} and {{tr}} (like {{tl}}this{{tr}}).  \n",
    "\n",
    "Additionally, each sequence will have tokens enclosed between {{cl}} and {{cr}} (like {{cl}}this{{cr}}). From previous analysis, we know that these tokens form the context for why our neuron fires on the token enclosed in {{tl}} and {{tr}} (in addition to the value of the actual token itself). Note that we treat the group of tokens enclosed between {{cl}} and {{cr}} as the \"context\" for why the neuron fired.\n",
    "\n",
    "We will provide both general examples and specific examples related to the task of Indirect Object Identification (IOI).\n",
    "\n",
    "Task Description: A sentence containing indirect object identification (IOI) has an initial dependent clause, e.g. \"When Mary and John went to the store\", and a main clause, e.g. \"John gave a bottle of milk to Mary\". The initial clause introduces the indirect object (IO) \"Mary\" and the subject (S) \"John\". The main clause refers to the subject a second time, and in all our examples of IOI, the subject gives an object to the IO. The IOI task is to predict the final token in the sentence to be the IO. We use 'S1' and 'S2' to refer to the first and second occurrences of the subject, when we want to specify position.\n",
    "\n",
    "Previous Neuron Information:\n",
    "You will also be provided with information about important previous neurons that feed into the current neuron. These neurons play a significant role in the IOI task and move information into the current neuron. The incoming information will be presented as a list of tuples, where each tuple contains the neuron's name and its interpretation in the context of the IOI task.\n",
    "\n",
    "{% for neuron in incoming_information %}\n",
    "Neuron {{neuron[0]}}:\n",
    "- Interpretation in IOI context: {{neuron[1]}}\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "Use this incoming information to help interpret the current neuron's role, considering how it processes and uses the information from these previous neurons.\n",
    "\n",
    "Given these examples, complete the following steps.\n",
    "\n",
    "OUTPUT_DESCRIPTION:\n",
    "\n",
    "Step 1: Based on the general examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 2: Based on the general examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 3: Write down several general shared features of the general text examples.\n",
    "Step 4: Based on the IOI examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 5: Based on the IOI examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 6: Write down several general shared features of the IOI text examples.\n",
    "Step 7: Based on the patterns you found between the activating token and the relevant context in both general and IOI examples, write down your best explanation for what this neuron is responsible for. Propose your explanation in the following form: \n",
    "[EXPLANATION]: <your explanation>\n",
    "\n",
    "Guidelines:\n",
    "- Try to produce a final explanation that's both concise and general to the examples provided.\n",
    "- Your explanation should be short: 1-2 sentences.\n",
    "- Specifically address the neuron's role in the context of the IOI task, explaining its specific function in relation to predicting the indirect object.\n",
    "- If provided, incorporate the interpretation of the previous neurons into your explanation, considering how the current neuron processes and uses the information from these previous neurons.\n",
    "\n",
    "INPUT:\n",
    "\n",
    "General Examples:\n",
    "{% for example in examples %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "IOI Task Examples:\n",
    "{% for example in examples_ioi %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "OUTPUT:\n",
    "                         \n",
    "Step 1:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"tl\": tl, \"tr\": tr, \"cl\": cl, \"cr\": cr, \"examples\": examples, \"examples_ioi\": examples_ioi, \"incoming_information\": incoming_information}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L5_H5']['features'])) if x!=-1]\n",
    "print(features)\n",
    "\n",
    "# #feature = 19042\n",
    "layer = 5\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "\n",
    "\n",
    "incoming_information = [\n",
    "    (\"L2H2\", l2h2_interp),\n",
    "    (\"L0H1\", l0h1_interp),\n",
    "    (\"L3H0\", l3h0_interp),\n",
    "    (\"L4H11\", l4h11_interp),\n",
    "]\n",
    "\n",
    "p = main_aug_interp_prompt_ioi_incoming(mini_examples_owt_overall, mini_examples_ioi_overall, incoming_information)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5h5_interp = \"This neuron predicts the appearance of named entities, specifically indirect objects, influenced by a previous introduction of the same entity or entities in the text (especially when entities were linked with a conjunction), crucial in identifying these entities when they reappear in a later context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L8_H6']['features'])) if x!=-1]\n",
    "print(features)\n",
    "\n",
    "# #feature = 19042\n",
    "layer = 8\n",
    "num_examples = 5000\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "\n",
    "\n",
    "incoming_information = [\n",
    "    # (\"L2H2\", l2h2_interp),\n",
    "    # (\"L0H1\", l0h1_interp),\n",
    "    # (\"L3H0\", l3h0_interp),\n",
    "    # (\"L4H11\", l4h11_interp),\n",
    "    (\"L5H5\", l5h5_interp),\n",
    "]\n",
    "\n",
    "p = main_aug_interp_prompt_ioi_incoming(mini_examples_owt_overall, mini_examples_ioi_overall, incoming_information)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8h6_interp = \"\"\" \n",
    "This neuron is responsible for detecting a linking or coordinating structure (\"and\", \"to\") between multiple named entities when those entities have already been introduced, as determined by L5H5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.circuit_hypergraph['L9_H9']['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L9_H9']['features'])) if x!=-1]\n",
    "print(features)\n",
    "\n",
    "# #feature = 19042\n",
    "layer = 9\n",
    "num_examples = 2500\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "    try:\n",
    "\n",
    "        analyze_owt = MaxActAnalysis(\n",
    "            \"attn\", \n",
    "            layer, \n",
    "            feature, \n",
    "            num_sequences=num_examples, \n",
    "            batch_size=128, \n",
    "            strategy=strategy\n",
    "        )\n",
    "        mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "        mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "        # For Dataset Prompt Tokens\n",
    "        analyze_prompts = MaxActAnalysis(\n",
    "            \"attn\", \n",
    "            layer, \n",
    "            feature, \n",
    "            num_sequences=num_examples, \n",
    "            batch_size=128, \n",
    "            strategy=strategy, \n",
    "            token_dataset=dataset_prompt_tokens\n",
    "        )\n",
    "        mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "        mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "    \n",
    "    except:\n",
    "        print(f\"Error with feature {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_information = [\n",
    "    # (\"L2H2\", l2h2_interp),\n",
    "    # (\"L0H1\", l0h1_interp),\n",
    "    # (\"L3H0\", l3h0_interp),\n",
    "    # (\"L4H11\", l4h11_interp),\n",
    "    #(\"L5H5\", l5h5_interp),\n",
    "    (\"L8H6\", l8h6_interp),\n",
    "]\n",
    "\n",
    "p = main_aug_interp_prompt_ioi_incoming(mini_examples_owt_overall, mini_examples_ioi_overall, incoming_information)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L10_H7']['features'])) if x!=-1]\n",
    "print(features)\n",
    "\n",
    "# #feature = 19042\n",
    "layer = 10\n",
    "num_examples = 2500\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "    try:\n",
    "\n",
    "        analyze_owt = MaxActAnalysis(\n",
    "            \"attn\", \n",
    "            layer, \n",
    "            feature, \n",
    "            num_sequences=num_examples, \n",
    "            batch_size=128, \n",
    "            strategy=strategy\n",
    "        )\n",
    "        mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "        mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "        # For Dataset Prompt Tokens\n",
    "        analyze_prompts = MaxActAnalysis(\n",
    "            \"attn\", \n",
    "            layer, \n",
    "            feature, \n",
    "            num_sequences=num_examples, \n",
    "            batch_size=128, \n",
    "            strategy=strategy, \n",
    "            token_dataset=dataset_prompt_tokens\n",
    "        )\n",
    "        mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "        mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "    \n",
    "    except:\n",
    "        print(f\"Error with feature {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_information = [\n",
    "    # (\"L2H2\", l2h2_interp),\n",
    "    # (\"L0H1\", l0h1_interp),\n",
    "    # (\"L3H0\", l3h0_interp),\n",
    "    # (\"L4H11\", l4h11_interp),\n",
    "    #(\"L5H5\", l5h5_interp),\n",
    "    (\"L8H6\", l8h6_interp),\n",
    "]\n",
    "\n",
    "p = main_aug_interp_prompt_ioi_incoming(mini_examples_owt_overall, mini_examples_ioi_overall, incoming_information)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New and improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from aug_interp_prompts import main_aug_interp_prompt, main_aug_interp_prompt_v2\n",
    "from openai_utils import gen_openai_completion, get_response\n",
    "from autointerpretability import *\n",
    "from discovery_strategies import (\n",
    "    create_filter,\n",
    "    create_simple_greedy_strategy,\n",
    "    create_top_contributor_strategy,\n",
    ")\n",
    "from max_act_analysis import MaxActAnalysis\n",
    "\n",
    "# Get feature families for each component\n",
    "\n",
    "from autointerpretability import *\n",
    "\n",
    "cp = get_circuit_prediction(task='ioi', N=20)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_top_k_feature_tuples_for_component(co_occurrence_dict, component_str, k=5):\n",
    "    # Parse the component string to get the appropriate tuple key\n",
    "    if component_str.startswith(\"MLP\"):\n",
    "        layer = int(component_str[3:])\n",
    "        component = ('mlp_feature', layer)\n",
    "    elif component_str.startswith(\"L\") and \"H\" in component_str:\n",
    "        layer, head = map(int, component_str[1:].split(\"H\"))\n",
    "        component = ('attn_head', layer, head)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid component format: {component_str}\")\n",
    "\n",
    "    # Use a Counter to count the occurrences of each tuple\n",
    "    global_counter = Counter()\n",
    "\n",
    "    # Iterate through the co-occurrence dictionary\n",
    "    for comp_pair, co_occurrences in co_occurrence_dict.items():\n",
    "        comp1, comp2 = comp_pair\n",
    "\n",
    "        if comp1 == component or comp2 == component:\n",
    "            for feature_tuple in co_occurrences:\n",
    "                global_counter[(comp_pair, feature_tuple)] += 1\n",
    "\n",
    "    # Get the top-k tuples by count\n",
    "    top_k_tuples = global_counter.most_common(k)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    top_k_dict = defaultdict(dict)\n",
    "    \n",
    "    for (comp_pair, feature_tuple), count in top_k_tuples:\n",
    "        top_k_dict[comp_pair][feature_tuple] = count\n",
    "\n",
    "    return top_k_dict\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model, z_saes, transcoders = get_model_encoders('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model, z_saes, transcoders = get_model_encoders('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "import numpy as np\n",
    "import einops\n",
    "from tabulate import tabulate\n",
    "from termcolor import colored\n",
    "\n",
    "def pretty_print_results(top_k_increases_indices, top_k_increases, top_k_decreases_indices, top_k_decreases, model, k, visualize):\n",
    "    increase_data = []\n",
    "    decrease_data = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        increase_token = model.to_string([top_k_increases_indices[i]])\n",
    "        increase_data.append([increase_token, f\"{top_k_increases[i]:.2f}\"])\n",
    "        \n",
    "        decrease_token = model.to_string([top_k_decreases_indices[i]])\n",
    "        decrease_data.append([decrease_token, f\"{top_k_decreases[i]:.2f}\"])\n",
    "    \n",
    "    increases_table = tabulate(increase_data, headers=[\"Token\", \"Increase\"], tablefmt=\"pretty\")\n",
    "    decreases_table = tabulate(decrease_data, headers=[\"Token\", \"Decrease\"], tablefmt=\"pretty\")\n",
    "    \n",
    "    if visualize:\n",
    "        print(colored(\"\\nTop k Increases:\", 'green'))\n",
    "        print(increases_table)\n",
    "        \n",
    "        print(colored(\"\\nTop k Decreases:\", 'red'))\n",
    "        print(decreases_table)\n",
    "    \n",
    "    return f\"Top k Increases:\\n{increases_table}\\n\\nTop k Decreases:\\n{decreases_table}\"\n",
    "\n",
    "def find_top_changes(prompt, model, layer, features, sae, k=10, visualize=True, max_z=True):\n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    \n",
    "    # Get the activations from the cache\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    z = cache[\"z\", layer, \"attn\"]\n",
    "    clean_z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "    z_hidden = sae.encode(clean_z)\n",
    "    \n",
    "    # Keep only the features in the second dimension\n",
    "    z_hidden = z_hidden[:, features]\n",
    "    \n",
    "    # Sum the feature activations together\n",
    "    if max_z:\n",
    "        z_hidden, _ = torch.max(z_hidden, dim=1) # z_hidden.sum(dim=1)  \n",
    "    else:\n",
    "        z_hidden = z_hidden.sum(dim=1)\n",
    "\n",
    "    # Make the first index 0\n",
    "    z_hidden[0] = 0.0\n",
    "    \n",
    "    # Get the index of the max activation\n",
    "    max_act_idx = z_hidden.argmax()\n",
    "\n",
    "    # If max act index is 0, return an error message\n",
    "    if max_act_idx == 0:\n",
    "        return None, \"\"\n",
    "    \n",
    "    # Cut the tokens at the max activation index\n",
    "    tokens = tokens[:, 1:max_act_idx + 1]\n",
    "    cut_prompt = model.to_string(tokens)[0]\n",
    "\n",
    "    # Get clean logits\n",
    "    clean_logits, clean_cache = model.run_with_cache(tokens)\n",
    "\n",
    "    # Define hook function to patch activations\n",
    "    def patch_z_vector(z, hook, layer, features):\n",
    "        clean_z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "        z_hidden = sae.encode(clean_z)\n",
    "        for feature in features:\n",
    "            z_hidden[:, feature] = 0.0\n",
    "        z_out = sae.decode(z_hidden)\n",
    "        z_out = einops.rearrange(z_out, \"(b s) (n d) -> b s n d\", b=z.shape[0], s=z.shape[1], n=z.shape[2], d=z.shape[3])\n",
    "        return z_out\n",
    "\n",
    "    # Apply the hook function\n",
    "    hook_fn = partial(patch_z_vector, layer=layer, features=features)\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[(utils.get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "        return_type=\"logits\"\n",
    "    )\n",
    "\n",
    "    clean_logits = clean_logits.squeeze()[-1, :]\n",
    "    patched_logits = patched_logits.squeeze()[-1, :]\n",
    "\n",
    "    difference = clean_logits - patched_logits\n",
    "\n",
    "    # Find indices of the top k increases\n",
    "    top_k_increases_indices = np.argpartition(difference, -k)[-k:]\n",
    "    top_k_increases = difference[top_k_increases_indices]\n",
    "\n",
    "    # Find indices of the top k decreases\n",
    "    top_k_decreases_indices = np.argpartition(difference, k)[:k]\n",
    "    top_k_decreases = difference[top_k_decreases_indices]\n",
    "\n",
    "    # Sort the top k increases and decreases\n",
    "    sorted_top_k_increases_indices = top_k_increases_indices[np.argsort(-top_k_increases)]\n",
    "    sorted_top_k_increases = difference[sorted_top_k_increases_indices]\n",
    "\n",
    "    sorted_top_k_decreases_indices = top_k_decreases_indices[np.argsort(top_k_decreases)]\n",
    "    sorted_top_k_decreases = difference[sorted_top_k_decreases_indices]\n",
    "\n",
    "    # Pretty print the results and return the tables as a string\n",
    "    result_string = pretty_print_results(sorted_top_k_increases_indices, sorted_top_k_increases, sorted_top_k_decreases_indices, sorted_top_k_decreases, model, k, visualize=visualize)\n",
    "    return result_string, cut_prompt\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Then, Joseph and Brandon had a lot of fun at the restaurant. Joseph gave a ring to\"\n",
    "layer = 9\n",
    "features = [3520]\n",
    "sae = z_saes[layer]\n",
    "result_string, cut_prompt = find_top_changes(prompt, model, layer, features, sae)\n",
    "print(cut_prompt)\n",
    "print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try it over a bunch of prompts\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=5)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "\n",
    "layer = 9\n",
    "features = [4729, 12471, 3520, 10391, 21753, 22975, 13173, 2581, 14056, 3481] #[16513, 2623]\n",
    "sae = z_saes[layer]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        result_string, cut_prompt = find_top_changes(prompt, model, layer, features, sae, k=5, visualize=False)\n",
    "        all_results.append(f\"Prompt = '{cut_prompt}'\\n\\n\"+result_string)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Combine all results into one string with \\n\\n\\n between each result\n",
    "all_results_string = \"\\n\\n\\n\".join(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_results_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def main_aug_interp_prompt_ioi(\n",
    "    examples: List[str], examples_ioi: List[str], \n",
    "    logit_increase_decrease: Optional[str] = None,\n",
    "    token_lr=(\"<<\", \">>\"), context_lr=(\"[[\", \"]]\")\n",
    "):\n",
    "    tl, tr = token_lr\n",
    "    cl, cr = context_lr\n",
    "\n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Indirect Object Identification (IOI). Here's how you will complete this task: #}\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to figure out what sort of behavior this neuron is responsible for -- namely, when this neuron fires, what kind of predictions does this neuron promote in the context of the specific task of Indirect Object Identification (IOI)? Here's how you'll complete the task:\n",
    "\n",
    "INPUT_DESCRIPTION:\n",
    "You will be given several examples of text that activate the neuron. First we'll provide the example text without any annotations, and then we'll provide the same text with annotations that show the specific tokens that caused the neuron to activate and context about why the neuron fired.\n",
    "\n",
    "The specific token that the neuron activates on will be the last token in the sequence, and will appear between {{tl}} and {{tr}} (like {{tl}}this{{tr}}).\n",
    "\n",
    "Additionally, each sequence will have tokens enclosed between {{cl}} and {{cr}} (like {{cl}}this{{cr}}). From previous analysis, we know that these tokens form the context for why our neuron fires on the token enclosed in {{tl}} and {{tr}} (in addition to the value of the actual token itself). Note that we treat the group of tokens enclosed between {{cl}} and {{cr}} as the \"context\" for why the neuron fired.\n",
    "\n",
    "We will provide both general examples and specific examples related to the task of Indirect Object Identification (IOI).\n",
    "\n",
    "Task Description: A sentence containing indirect object identification (IOI) has an initial dependent clause, e.g. \"When Mary and John went to the store\", and a main clause, e.g. \"John gave a bottle of milk to Mary\". The initial clause introduces the indirect object (IO) \"Mary\" and the subject (S) \"John\". The main clause refers to the subject a second time, and in all our examples of IOI, the subject gives an object to the IO. The IOI task is to predict the final token in the sentence to be the IO. We use 'S1' and 'S2' to refer to the first and second occurrences of the subject, when we want to specify position.\n",
    "\n",
    "Given these examples, complete the following steps.\n",
    "\n",
    "OUTPUT_DESCRIPTION:\n",
    "\n",
    "Step 1: Based on the general examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 2: Based on the general examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 3: Write down several general shared features of the general text examples.\n",
    "Step 4: Based on the IOI examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 5: Based on the IOI examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 6: Write down several general shared features of the IOI text examples.\n",
    "Step 7: Based on the logit increase/decrease information provided, analyze how the neuron's activation affects the probability of certain tokens being predicted next.\n",
    "Step 8: Based on the patterns you found between the activating token and the relevant context in both general and IOI examples, and the logit increase/decrease information, write down your best explanation for (1) which exact tokens the neuron fires on, and (2) how this firing influence the prediction of the NEXT token. Propose your explanation in the following form:\n",
    "[EXPLANATION]: <your explanation>\n",
    "\n",
    "Guidelines:\n",
    "- Try to produce a final explanation that's both concise and general to the examples provided.\n",
    "- Your explanation should be short: 1-2 sentences.\n",
    "- Specifically address the neuron's role in the context of the IOI task, explaining its specific function in relation to predicting the indirect object.\n",
    "- Use the logit increase/decrease information to explain the neuron's effect on the next token prediction.\n",
    "- When looking at the context tokens that contributed to the neuron activating on the activation token, think about how they relate to the activation token. Has the neuron seen these tokens before? How are they related to activation, and how are they related to the token that might come next?\n",
    "- Not all of the information will be useful! Sometimes, the neuron may just be looking at a specific previous token, or alternatively may not care about previous tokens and is just doing something to the prediction.\n",
    "- Your final explanation should be a combination of (1) what tokens the neuron specifically activates on, and (2) how this neuron then affects the next token.\n",
    "\n",
    "INPUT:\n",
    "\n",
    "General Examples for tokens the neuron activates on (<< >>), and the other tokens that provide context ([[ ]]):\n",
    "{% for example in examples %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "IOI Task Examples:\n",
    "{% for example in examples_ioi %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "Use these examples to determine what tokens the neuron fires/activates on, and the surrounding context (tokens) that also contribute to this.\n",
    "\n",
    "Logit Increase/Decrease Information:\n",
    "Below is information on how this particular neuron increases the probability of certain tokens (i.e. their logits) and decreases the probability of others, when predicting the next token in the prompt i.e. after the final word in the prompt provided. This information will help you discern the neuron's effect on the NEXT token to predict after the token it activates on, which it doesn't actually see in the examples.\n",
    "\n",
    "{% for x in logit_increase_decrease %}\n",
    "{{x}}\n",
    "{% endfor %}\n",
    "\n",
    "Use this information to help interpret the effect of the neuron's activation on the model's predictions. Be specific about whether it boosts the logits of the subject, the indirect object, or a different type of neuron, and similarly whether it decreases the logits of the subject, the indirect object, or a different type of neuron.\n",
    "\n",
    "OUTPUT:\n",
    "                         \n",
    "Step 1:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"tl\": tl, \"tr\": tr, \"cl\": cl, \"cr\": cr, \"examples\": examples, \"examples_ioi\": examples_ioi, \n",
    "         \"logit_increase_decrease\": logit_increase_decrease}\n",
    "    )\n",
    "\n",
    "# {# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model.\n",
    "#     # Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Indirect Object Identification (IOI).\n",
    "#     # Here's how you will complete this task:}\n",
    "\n",
    "\n",
    "def main_aug_interp_prompt_ioi_incoming(\n",
    "    examples: List[str], examples_ioi: List[str], \n",
    "    incoming_information: Optional[List[Tuple[str, str]]] = None, \n",
    "    logit_increase_decrease: Optional[str] = None,\n",
    "    token_lr=(\"<<\", \">>\"), context_lr=(\"[[\", \"]]\")\n",
    "):\n",
    "    tl, tr = token_lr\n",
    "    cl, cr = context_lr\n",
    "\n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Indirect Object Identification (IOI). Here's how you will complete this task: #}\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to figure out what sort of behavior this neuron is responsible for -- namely, when this neuron fires, what kind of predictions does this neuron promote in the context of the specific task of Indirect Object Identification (IOI)? Here's how you'll complete the task:\n",
    "\n",
    "INPUT_DESCRIPTION:\n",
    "You will be given several examples of text that activate the neuron. First we'll provide the example text without any annotations, and then we'll provide the same text with annotations that show the specific tokens that caused the neuron to activate and context about why the neuron fired.\n",
    "\n",
    "The specific token that the neuron activates on will be the last token in the sequence, and will appear between {{tl}} and {{tr}} (like {{tl}}this{{tr}}).\n",
    "\n",
    "Additionally, each sequence will have tokens enclosed between {{cl}} and {{cr}} (like {{cl}}this{{cr}}). From previous analysis, we know that these tokens form the context for why our neuron fires on the token enclosed in {{tl}} and {{tr}} (in addition to the value of the actual token itself). Note that we treat the group of tokens enclosed between {{cl}} and {{cr}} as the \"context\" for why the neuron fired.\n",
    "\n",
    "We will provide both general examples and specific examples related to the task of Indirect Object Identification (IOI).\n",
    "\n",
    "Task Description: A sentence containing indirect object identification (IOI) has an initial dependent clause, e.g. \"When Mary and John went to the store\", and a main clause, e.g. \"John gave a bottle of milk to Mary\". The initial clause introduces the indirect object (IO) \"Mary\" and the subject (S) \"John\". The main clause refers to the subject a second time, and in all our examples of IOI, the subject gives an object to the IO. The IOI task is to predict the final token in the sentence to be the IO. We use 'S1' and 'S2' to refer to the first and second occurrences of the subject, when we want to specify position.\n",
    "\n",
    "Previous Neuron Information:\n",
    "You will also be provided with information about important previous neurons that feed into the current neuron. These neurons play a significant role in the IOI task and move information into the current neuron. The incoming information will be presented as a list of tuples, where each tuple contains the neuron's name and its interpretation in the context of the IOI task.\n",
    "\n",
    "{% for neuron in incoming_information %}\n",
    "Neuron {{neuron[0]}}:\n",
    "- Interpretation in IOI context: {{neuron[1]}}\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "Use this incoming information to help interpret the current neuron's role, considering how it processes and uses the information from these previous neurons.\n",
    "\n",
    "Given these examples, complete the following steps.\n",
    "\n",
    "OUTPUT_DESCRIPTION:\n",
    "\n",
    "Step 1: Based on the general examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 2: Based on the general examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 3: Write down several general shared features of the general text examples.\n",
    "Step 4: Based on the IOI examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 5: Based on the IOI examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 6: Write down several general shared features of the IOI text examples.\n",
    "Step 7: Based on the logit increase/decrease information provided, analyze how the neuron's activation affects the probability of certain tokens being predicted next.\n",
    "Step 8: Based on the patterns you found between the activating token and the relevant context in both general and IOI examples, and the logit increase/decrease information, write down your best explanation for (1) which exact tokens the neuron fires on, and (2) how this firing influence the prediction of the NEXT token. Propose your explanation in the following form:\n",
    "[EXPLANATION]: <your explanation>\n",
    "\n",
    "Guidelines:\n",
    "- Try to produce a final explanation that's both concise and general to the examples provided.\n",
    "- Your explanation should be short: 1-2 sentences.\n",
    "- Specifically address the neuron's role in the context of the IOI task, explaining its specific function in relation to predicting the indirect object.\n",
    "- If provided, incorporate the interpretation of the previous neurons into your explanation, considering how the current neuron processes and uses the information from these previous neurons.\n",
    "- Use the logit increase/decrease information to explain the neuron's effect on the next token prediction.\n",
    "- When looking at the context tokens that contributed to the neuron activating on the activation token, think about how they relate to the activation token. Has the neuron seen these tokens before? How are they related to activation, and how are they related to the token that might come next?\n",
    "- Not all of the information will be useful! Sometimes, the neuron may just be looking at a specific previous token, or alternatively may not care about previous tokens and is just doing something to the prediction.\n",
    "- Your final explanation should be a combination of (1) what tokens the neuron specifically activates on, and (2) how this neuron then affects the next token.\n",
    "\n",
    "INPUT:\n",
    "\n",
    "General Examples for tokens the neuron activates on (<< >>), and the other tokens that provide context ([[ ]]):\n",
    "{% for example in examples %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "IOI Task Examples:\n",
    "{% for example in examples_ioi %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "Use these examples to determine what tokens the neuron fires/activates on, and the surrounding context (tokens) that also contribute to this.\n",
    "\n",
    "Logit Increase/Decrease Information:\n",
    "Below is information on how this particular neuron increases the probability of certain tokens (i.e. their logits) and decreases the probability of others, when predicting the next token in the prompt i.e. after the final word in the prompt provided. This information will help you discern the neuron's effect on the NEXT token to predict after the token it activates on, which it doesn't actually see in the examples.\n",
    "\n",
    "{% for x in logit_increase_decrease %}\n",
    "{{x}}\n",
    "{% endfor %}\n",
    "\n",
    "Use this information to help interpret the effect of the neuron's activation on the model's predictions. Be specific about whether it boosts the logits of the subject, the indirect object, or a different type of neuron, and similarly whether it decreases the logits of the subject, the indirect object, or a different type of neuron.\n",
    "\n",
    "OUTPUT:\n",
    "                         \n",
    "Step 1:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"tl\": tl, \"tr\": tr, \"cl\": cl, \"cr\": cr, \"examples\": examples, \"examples_ioi\": examples_ioi, \n",
    "         \"incoming_information\": incoming_information, \"logit_increase_decrease\": logit_increase_decrease}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try it over a bunch of prompts\n",
    "layer = 8\n",
    "features = [16513, 10461]\n",
    "sae = z_saes[layer]\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=50)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    try:\n",
    "        result_string, cut_prompt = find_top_changes(prompt, model, layer, features, sae, k=8, visualize=False, max_z=True)\n",
    "        all_results.append(f\"Prompt = '{cut_prompt}'\\n\\n\"+result_string)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Combine all results into one string with \\n\\n\\n between each result\n",
    "all_results_string = \"\\n\\n\\n\".join(all_results)\n",
    "print(\"Successfully generated logit increase/decrease info.\")\n",
    "\n",
    "print(all_results_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(set(cp.circuit_hypergraph['L8_H6']['features'])) if x!=-1]\n",
    "print(features)\n",
    "\n",
    "num_examples = 2500\n",
    "layer = 8\n",
    "# features = [16513, 2623]\n",
    "sae = z_saes[layer]\n",
    "\n",
    "strategy = create_simple_greedy_strategy(\n",
    "    passes=1,\n",
    "    node_contributors=1,\n",
    "    minimal=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Now let's try it over a bunch of prompts\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=8)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    result_string, cut_prompt = find_top_changes(prompt, model, layer, features, sae, k=8, visualize=False)\n",
    "    all_results.append(f\"Prompt = '{cut_prompt}'\\n\\n\"+result_string)\n",
    "\n",
    "# Combine all results into one string with \\n\\n\\n between each result\n",
    "all_results_string = \"\\n\\n\\n\".join(all_results)\n",
    "print(\"Successfully generated logit increase/decrease info.\")\n",
    "\n",
    "print(all_results_string)\n",
    "\n",
    "dataset_prompts = gen_templated_prompts(template_idex=1, N=500)\n",
    "prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "tokens = model.to_tokens(prompts)  # Assuming `model` is already defined\n",
    "dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "mini_examples_owt_overall = []\n",
    "mini_examples_ioi_overall = []\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    analyze_owt = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy\n",
    "    )\n",
    "    mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "\n",
    "    # For Dataset Prompt Tokens\n",
    "    analyze_prompts = MaxActAnalysis(\n",
    "        \"attn\", \n",
    "        layer, \n",
    "        feature, \n",
    "        num_sequences=num_examples, \n",
    "        batch_size=128, \n",
    "        strategy=strategy, \n",
    "        token_dataset=dataset_prompt_tokens\n",
    "    )\n",
    "    mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "    mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "\n",
    "incoming_information = [\n",
    "    # (\"L2H2\", l2h2_interp),\n",
    "    # (\"L0H1\", l0h1_interp),\n",
    "    # (\"L3H0\", l3h0_interp),\n",
    "    # (\"L4H11\", l4h11_interp),\n",
    "    (\"L5H5\", l5h5_interp),\n",
    "]\n",
    "\n",
    "p = main_aug_interp_prompt_ioi_incoming(mini_examples_owt_overall, mini_examples_ioi_overall, incoming_information, [all_results_string])\n",
    "# print(p)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5h5_interp = \"\"\" \n",
    "Activates on the appearance of named entities, specifically indirect objects, influenced by a previous introduction of the same entity or entities in the text, and identifies the latter of these entities when they reappear together in a later context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_information = [\n",
    "    # (\"L2H2\", l2h2_interp),\n",
    "    # (\"L0H1\", l0h1_interp),\n",
    "    # (\"L3H0\", l3h0_interp),\n",
    "    # (\"L4H11\", l4h11_interp),\n",
    "    (\"L5H5\", l5h5_interp),\n",
    "]\n",
    "\n",
    "p = main_aug_interp_prompt_ioi_incoming(mini_examples_owt_overall, mini_examples_ioi_overall, incoming_information, [all_results_string])\n",
    "# print(p)\n",
    "interp = get_response(p)\n",
    "print(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# Function to generate logit increase/decrease information\n",
    "def generate_logit_info(prompts: List[str], model, layer: int, features: List[int], sae) -> str:\n",
    "    all_results = []\n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            result_string, cut_prompt = find_top_changes(prompt, model, layer, features, sae, k=8, visualize=False)\n",
    "            # Surround the last word in cut prompt with << and >> for the logit info\n",
    "            cut_prompt = cut_prompt.split(\" \")\n",
    "            cut_prompt[-1] = f\"<<{cut_prompt[-1]}>>\"\n",
    "            cut_prompt = \" \".join(cut_prompt)\n",
    "            if result_string is not None:\n",
    "                all_results.append(f\"Prompt = '{cut_prompt}'\\n\\n\" + result_string)\n",
    "\n",
    "        except:\n",
    "            print(f\"Error for prompt {prompt}\")\n",
    "            continue\n",
    "            \n",
    "    all_results = all_results[:8]\n",
    "    return \"\\n\\n\\n\".join(all_results)\n",
    "\n",
    "# Function to generate mini examples for both general and IOI tasks\n",
    "def generate_mini_examples(features: List[int], layer: int, num_examples: int, dataset_prompt_tokens, strategy) -> Tuple[List[str], List[str]]:\n",
    "    mini_examples_owt_overall = []\n",
    "    mini_examples_ioi_overall = []\n",
    "\n",
    "    for feature in features:\n",
    "        try:\n",
    "            analyze_owt = MaxActAnalysis(\"attn\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy)\n",
    "            mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "            mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "        except:\n",
    "            print(f\"Error for OWT tokens with feature {feature}\")\n",
    "\n",
    "        try: \n",
    "            analyze_prompts = MaxActAnalysis(\"attn\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy, token_dataset=dataset_prompt_tokens)\n",
    "            mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "            mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "        except:\n",
    "            print(f\"Error for IOI tokens with feature {feature}\")\n",
    "\n",
    "    return mini_examples_owt_overall, mini_examples_ioi_overall\n",
    "\n",
    "# Function to interpret the neuron using the provided prompt\n",
    "def interpret_neuron(mini_examples_owt: List[str], mini_examples_ioi: List[str], incoming_information: Optional[List[Tuple[str, str]]], logit_info: str) -> str:\n",
    "    if len(incoming_information) > 0:\n",
    "        print(\"Using incoming information.\")\n",
    "        prompt = main_aug_interp_prompt_ioi_incoming(mini_examples_owt, mini_examples_ioi, incoming_information, [logit_info])\n",
    "    else:\n",
    "        print(\"Not using incoming information.\")\n",
    "        prompt = main_aug_interp_prompt_ioi(mini_examples_owt, mini_examples_ioi, [logit_info])\n",
    "    return get_response(prompt), prompt\n",
    "\n",
    "# Function to get the top 5 (or less) occurring features\n",
    "def get_top_features(component: str, cp) -> List[int]:\n",
    "    features = [x for x in list(cp.circuit_hypergraph[component]['features']) if x != -1]\n",
    "    feature_counts = Counter(features)\n",
    "    top_features = [feature for feature, _ in feature_counts.most_common(10)]\n",
    "    return top_features\n",
    "\n",
    "# Higher-level function to process components based on the graph notation\n",
    "def process_components(graph: Dict[str, List[str]], model, z_saes, strategy, num_examples: int = 2500):\n",
    "    # Initialize data structure to manage incoming information\n",
    "    component_interpretations = {}\n",
    "\n",
    "    num_examples = 2500\n",
    "\n",
    "    # Generate prompts\n",
    "    dataset_prompts = gen_templated_prompts(template_idex=1, N=50)\n",
    "    prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "\n",
    "    # Process each component\n",
    "    for component, next_components in graph.items():\n",
    "        layer, head = component.split(\"_\")\n",
    "        layer = int(layer[1:])\n",
    "        head = int(head[1:])\n",
    "        features = get_top_features(component, cp)\n",
    "        print(f\"Doing {component} with features {features} for layer {layer} head {head}\")\n",
    "\n",
    "        sae = z_saes[layer]\n",
    "\n",
    "        # Generate logit increase/decrease information\n",
    "        logit_info = generate_logit_info(prompts, model, layer, features, sae)\n",
    "        print(logit_info)\n",
    "        print(f\"Successfully generated logit increase/decrease info for {component}.\")\n",
    "\n",
    "        # Generate mini examples\n",
    "        dataset_prompts_full = gen_templated_prompts(template_idex=1, N=500)\n",
    "        full_prompts = [x['text'] + x['correct'] for x in dataset_prompts_full]\n",
    "        tokens = model.to_tokens(full_prompts)\n",
    "        dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "        mini_examples_owt, mini_examples_ioi = generate_mini_examples(features, layer, num_examples, dataset_prompt_tokens, strategy)\n",
    "\n",
    "        # Prepare incoming information\n",
    "        incoming_information = [] #[(prev_component, component_interpretations[prev_component]) for prev_component in graph if component in graph[prev_component]]\n",
    "        print(f\"Incoming information for {component}:\\n{incoming_information}\\n\")\n",
    "\n",
    "        # Interpret the neuron\n",
    "        interpretation, p = interpret_neuron(mini_examples_owt, mini_examples_ioi, incoming_information, logit_info)\n",
    "        print(f\"Interpretation for {component}:\\n{interpretation}\\n\")\n",
    "        print(f\"Prompt for {component}:\\n{p}\\n\")\n",
    "        component_interpretations[component] = {\"interp\": interpretation, \"prompt\": p}\n",
    "\n",
    "        # Update incoming information for future components\n",
    "        for next_component in next_components:\n",
    "            if next_component not in graph:\n",
    "                graph[next_component] = []\n",
    "            graph[next_component].append(component)\n",
    "\n",
    "    return component_interpretations\n",
    "\n",
    "# Example graph notation\n",
    "graph_notation = {\n",
    "    # \"L2_H2\": [\"L5_H5\"],\n",
    "    # \"L4_H11\": [\"L5_H5\"],\n",
    "    # \"L0_H1\": [\"L5_H5\"],\n",
    "    # \"L3_H0\": [\"L5_H5\"],\n",
    "    # \"L5_H5\": [\"L8_H6\"],\n",
    "    # \"L8_H6\": [\"L10_H7\", \"L9_H9\"],\n",
    "    \"L9_H9\": [],\n",
    "    # \"L10_H7\": []\n",
    "}\n",
    "\n",
    "# # Example usage\n",
    "# model = ...  # Your model here\n",
    "# sae = z_saes[layer]\n",
    "model, z_saes, _ = get_model_encoders('cpu')\n",
    "strategy = create_simple_greedy_strategy(passes=1, node_contributors=1, minimal=True)\n",
    "component_interpretations_l9h9 = process_components(graph_notation, model, z_saes, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(component_interpretations_l9h9['L9_H9']['interp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_interpretations['L10_H7'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_interpretations['L3_H0']['interp'] = \"\"\" \n",
    "[EXPLANATION]: Activates when the subject or indirect object from a prior clause reappears in the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in component_interpretations.items():\n",
    "    print(f\"Component: {k}\")\n",
    "    # print(f\"Interpretation: {v['interp']}\")\n",
    "    if 'EXPLANATION' in v['interp']:\n",
    "        print(f\"Interpretation: {v['interp'].split('[EXPLANATION]: ')[-1].strip()}\")\n",
    "    elif 'Explanation' in v['interp']:\n",
    "        print(f\"Interpretation: {v['interp'].split('[Explanation]:')[-1].strip()}\")\n",
    "    #print(f\"Interpretation: {v['interp'].split('[EXPLANATION]: ')[-1].strip()}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one of the prompts\n",
    "p = component_interpretations['L8_H6']['prompt']\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greater-than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from aug_interp_prompts import main_aug_interp_prompt, main_aug_interp_prompt_v2\n",
    "from openai_utils import gen_openai_completion, get_response\n",
    "from autointerpretability import *\n",
    "from discovery_strategies import (\n",
    "    create_filter,\n",
    "    create_simple_greedy_strategy,\n",
    "    create_top_contributor_strategy,\n",
    ")\n",
    "from max_act_analysis import MaxActAnalysis\n",
    "\n",
    "from autointerpretability import *\n",
    "\n",
    "cp = get_circuit_prediction(task='gt', N=50)\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_top_k_feature_tuples_for_component(co_occurrence_dict, component_str, k=5):\n",
    "    # Parse the component string to get the appropriate tuple key\n",
    "    if component_str.startswith(\"MLP\"):\n",
    "        layer = int(component_str[3:])\n",
    "        component = ('mlp_feature', layer)\n",
    "    elif component_str.startswith(\"L\") and \"H\" in component_str:\n",
    "        layer, head = map(int, component_str[1:].split(\"H\"))\n",
    "        component = ('attn_head', layer, head)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid component format: {component_str}\")\n",
    "\n",
    "    # Use a Counter to count the occurrences of each tuple\n",
    "    global_counter = Counter()\n",
    "\n",
    "    # Iterate through the co-occurrence dictionary\n",
    "    for comp_pair, co_occurrences in co_occurrence_dict.items():\n",
    "        comp1, comp2 = comp_pair\n",
    "\n",
    "        if comp1 == component or comp2 == component:\n",
    "            for feature_tuple in co_occurrences:\n",
    "                global_counter[(comp_pair, feature_tuple)] += 1\n",
    "\n",
    "    # Get the top-k tuples by count\n",
    "    top_k_tuples = global_counter.most_common(k)\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    top_k_dict = defaultdict(dict)\n",
    "    \n",
    "    for (comp_pair, feature_tuple), count in top_k_tuples:\n",
    "        top_k_dict[comp_pair][feature_tuple] = count\n",
    "\n",
    "    return top_k_dict\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model, z_saes, transcoders = get_model_encoders('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through circuit hypergraph and print out frequencies of all keys with non-zero freq\n",
    "hypergraph = cp.circuit_hypergraph\n",
    "\n",
    "components = []\n",
    "threshold = 0.3\n",
    "\n",
    "for key, value in hypergraph.items():\n",
    "    if value['freq'] > 0.0:\n",
    "        print(key, value['freq'], len([x for x in list(set(value['features'])) if x!=-1]))\n",
    "    \n",
    "    if value['freq'] > threshold and len([x for x in list(set(value['features'])) if x!=-1]) > 0:\n",
    "        components.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from typing import List, Optional\n",
    "\n",
    "def main_aug_interp_prompt_gt(\n",
    "    examples: List[str], examples_gt: List[str], \n",
    "    logit_increase_decrease: Optional[str] = None,\n",
    "    token_lr=(\"<<\", \">>\"), context_lr=(\"[[\", \"]]\")\n",
    "):\n",
    "    tl, tr = token_lr\n",
    "    cl, cr = context_lr\n",
    "\n",
    "    template = Template(\n",
    "        \"\"\"\n",
    "{# You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior in the context of a specific task: Greater-Than Prediction (GT). Here's how you will complete this task: #}\n",
    "\n",
    "You are a meticulous AI researcher conducting an important investigation into a certain neuron in a language model. This language model is trained to predict the text that will follow a given input. Your task is to figure out what sort of behavior this neuron is responsible for -- namely, when this neuron fires, what kind of predictions does this neuron promote in the context of the specific task of Greater-Than Prediction (GT)? Here's how you'll complete the task:\n",
    "\n",
    "INPUT_DESCRIPTION:\n",
    "You will be given several examples of text that activate the neuron. First we'll provide the example text without any annotations, and then we'll provide the same text with annotations that show the specific tokens that caused the neuron to activate and context about why the neuron fired.\n",
    "\n",
    "The specific token that the neuron activates on will be the last token in the sequence, and will appear between {{tl}} and {{tr}} (like {{tl}}this{{tr}}).\n",
    "\n",
    "Additionally, each sequence will have tokens enclosed between {{cl}} and {{cr}} (like {{cl}}this{{cr}}). From previous analysis, we know that these tokens form the context for why our neuron fires on the token enclosed in {{tl}} and {{tr}} (in addition to the value of the actual token itself). Note that we treat the group of tokens enclosed between {{cl}} and {{cr}} as the \"context\" for why the neuron fired.\n",
    "\n",
    "We will provide both general examples and specific examples related to the task of Greater-Than Prediction (GT).\n",
    "\n",
    "Task Description: The greater-than task involves predicting a year that is greater than a given year in the context of sentences framed like The <noun> lasted from the year XXYY to the year XX. The initial part of the sentence introduces a time span, and the model's task is to assign higher probabilities to years that are greater than YY. Each example is designed to have at least one correct and one incorrect validly tokenized answer.\n",
    "\n",
    "Given these examples, complete the following steps.\n",
    "\n",
    "OUTPUT_DESCRIPTION:\n",
    "\n",
    "Step 1: Based on the general examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 2: Based on the general examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 3: Write down several general shared features of the general text examples.\n",
    "Step 4: Based on the GT examples provided, write down observed patterns between the tokens that caused the neuron to activate (just the tokens enclosed in {{tl}} and {{tr}}).\n",
    "Step 5: Based on the GT examples provided, write down patterns you see in the context for why the neuron fired. (Remember, the \"context\" for an example is the group of tokens enclosed in {{cl}} and {{cr}}). Include any patterns in the relationships between different tokens in the context, and any patterns in the relationship between the context and the rest of the text.\n",
    "Step 6: Write down several general shared features of the GT text examples.\n",
    "Step 7: Based on the logit increase/decrease information provided, analyze how the neuron's activation affects the probability of certain tokens being predicted next.\n",
    "Step 8: Based on the patterns you found between the activating token and the relevant context in both general and GT examples, and the logit increase/decrease information, write down your best explanation for (1) which exact tokens the neuron fires on, and (2) how this firing influences the prediction of the NEXT token. Propose your explanation in the following form:\n",
    "[EXPLANATION]: <your explanation>\n",
    "\n",
    "Guidelines:\n",
    "- Try to produce a final explanation that's both concise and general to the examples provided.\n",
    "- Your explanation should be short: 1-2 sentences.\n",
    "- Specifically address the neuron's role in the context of the GT task, explaining its specific function in relation to predicting the greater-than year.\n",
    "- Use the logit increase/decrease information to explain the neuron's effect on the next token prediction.\n",
    "- When looking at the context tokens that contributed to the neuron activating on the activation token, think about how they relate to the activation token. Has the neuron seen these tokens before? How are they related to activation, and how are they related to the token that might come next?\n",
    "- Not all of the information will be useful! Sometimes, the neuron may just be looking at a specific previous token, or alternatively may not care about previous tokens and is just doing something to the prediction.\n",
    "- Your final explanation should be a combination of (1) what tokens the neuron specifically activates on, and (2) how this neuron then affects the next token.\n",
    "\n",
    "INPUT:\n",
    "\n",
    "General Examples for tokens the neuron activates on (<< >>), and the other tokens that provide context ([[ ]]):\n",
    "{% for example in examples %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "GT Task Examples:\n",
    "{% for example in examples_gt %}                         \n",
    "EXAMPLE {{loop.index + 1}}:\n",
    "- Base Text -\n",
    "=================================================\n",
    "{{example[0]}}\n",
    "=================================================\n",
    "\n",
    "- Annotated Text -\n",
    "=================================================\n",
    "{{example[1]}}\n",
    "=================================================\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "Use these examples to determine what tokens the neuron fires/activates on, and the surrounding context (tokens) that also contribute to this.\n",
    "\n",
    "Logit Increase/Decrease Information:\n",
    "Below is information on how this particular neuron increases the probability of certain tokens (i.e. their logits) and decreases the probability of others, when predicting the next token in the prompt i.e. after the final word in the prompt provided. This information will help you discern the neuron's effect on the NEXT token to predict after the token it activates on, which it doesn't actually see in the examples.\n",
    "\n",
    "{% for x in logit_increase_decrease %}\n",
    "{{x}}\n",
    "{% endfor %}\n",
    "\n",
    "Use this information to help interpret the effect of the neuron's activation on the model's predictions. Be specific about whether it boosts the logits of the greater-than year, the context tokens, or a different type of token, and similarly whether it decreases the logits of the greater-than year, the context tokens, or a different type of token.\n",
    "\n",
    "OUTPUT:\n",
    "                         \n",
    "Step 1:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    return template.render(\n",
    "        {\"tl\": tl, \"tr\": tr, \"cl\": cl, \"cr\": cr, \"examples\": examples, \"examples_gt\": examples_gt, \n",
    "         \"logit_increase_decrease\": logit_increase_decrease}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "import numpy as np\n",
    "import einops\n",
    "from tabulate import tabulate\n",
    "from termcolor import colored\n",
    "\n",
    "def pretty_print_results(top_k_increases_indices, top_k_increases, top_k_decreases_indices, top_k_decreases, model, k, visualize):\n",
    "    increase_data = []\n",
    "    decrease_data = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        increase_token = model.to_string([top_k_increases_indices[i]])\n",
    "        increase_data.append([increase_token, f\"{top_k_increases[i]:.2f}\"])\n",
    "        \n",
    "        decrease_token = model.to_string([top_k_decreases_indices[i]])\n",
    "        decrease_data.append([decrease_token, f\"{top_k_decreases[i]:.2f}\"])\n",
    "    \n",
    "    increases_table = tabulate(increase_data, headers=[\"Token\", \"Increase\"], tablefmt=\"pretty\")\n",
    "    decreases_table = tabulate(decrease_data, headers=[\"Token\", \"Decrease\"], tablefmt=\"pretty\")\n",
    "    \n",
    "    if visualize:\n",
    "        print(colored(\"\\nTop k Increases:\", 'green'))\n",
    "        print(increases_table)\n",
    "        \n",
    "        print(colored(\"\\nTop k Decreases:\", 'red'))\n",
    "        print(decreases_table)\n",
    "    \n",
    "    return f\"Top k Increases:\\n{increases_table}\\n\\nTop k Decreases:\\n{decreases_table}\"\n",
    "\n",
    "def find_top_changes(prompt, model, layer, features, sae, k=10, visualize=True, max_z=True):\n",
    "    # Tokenize the prompt\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    \n",
    "    # Get the activations from the cache\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    z = cache[\"z\", layer, \"attn\"]\n",
    "    clean_z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "    z_hidden = sae.encode(clean_z)\n",
    "    \n",
    "    # Keep only the features in the second dimension\n",
    "    z_hidden = z_hidden[:, features]\n",
    "    \n",
    "    # Sum the feature activations together\n",
    "    if max_z:\n",
    "        z_hidden, _ = torch.max(z_hidden, dim=1) # z_hidden.sum(dim=1)  \n",
    "    else:\n",
    "        z_hidden = z_hidden.sum(dim=1)\n",
    "\n",
    "    # Make the first index 0\n",
    "    #z_hidden[0] = 0.0\n",
    "    \n",
    "    # Get the index of the max activation\n",
    "    max_act_idx = z_hidden.argmax()\n",
    "\n",
    "    # If max act index is 0, return an error message\n",
    "    if max_act_idx == 0:\n",
    "        return None, \"\"\n",
    "    \n",
    "    # Cut the tokens at the max activation index\n",
    "    tokens = tokens[:, 1:max_act_idx + 1]\n",
    "    cut_prompt = model.to_string(tokens)[0]\n",
    "\n",
    "    # Get clean logits\n",
    "    clean_logits, _ = model.run_with_cache(tokens)\n",
    "\n",
    "    # Define hook function to patch activations\n",
    "    def patch_z_vector(z, hook, layer, features):\n",
    "        clean_z = einops.rearrange(z, \"b s n d -> (b s) (n d)\")\n",
    "        z_hidden = sae.encode(clean_z)\n",
    "        for feature in features:\n",
    "            z_hidden[:, feature] = 0.0\n",
    "        z_out = sae.decode(z_hidden)\n",
    "        z_out = einops.rearrange(z_out, \"(b s) (n d) -> b s n d\", b=z.shape[0], s=z.shape[1], n=z.shape[2], d=z.shape[3])\n",
    "        return z_out\n",
    "\n",
    "    # Apply the hook function\n",
    "    hook_fn = partial(patch_z_vector, layer=layer, features=features)\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[(utils.get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "        return_type=\"logits\"\n",
    "    )\n",
    "\n",
    "    clean_logits = clean_logits.squeeze()[-1, :]\n",
    "    patched_logits = patched_logits.squeeze()[-1, :]\n",
    "\n",
    "    difference = clean_logits - patched_logits\n",
    "\n",
    "    # Find indices of the top k increases\n",
    "    top_k_increases_indices = np.argpartition(difference, -k)[-k:]\n",
    "    top_k_increases = difference[top_k_increases_indices]\n",
    "\n",
    "    # Find indices of the top k decreases\n",
    "    top_k_decreases_indices = np.argpartition(difference, k)[:k]\n",
    "    top_k_decreases = difference[top_k_decreases_indices]\n",
    "\n",
    "    # Sort the top k increases and decreases\n",
    "    sorted_top_k_increases_indices = top_k_increases_indices[np.argsort(-top_k_increases)]\n",
    "    sorted_top_k_increases = difference[sorted_top_k_increases_indices]\n",
    "\n",
    "    sorted_top_k_decreases_indices = top_k_decreases_indices[np.argsort(top_k_decreases)]\n",
    "    sorted_top_k_decreases = difference[sorted_top_k_decreases_indices]\n",
    "\n",
    "    # Pretty print the results and return the tables as a string\n",
    "    result_string = pretty_print_results(sorted_top_k_increases_indices, sorted_top_k_increases, sorted_top_k_decreases_indices, sorted_top_k_decreases, model, k, visualize=visualize)\n",
    "    return result_string, cut_prompt\n",
    "\n",
    "# Example usage\n",
    "prompt = \"The war lasted from the year 1746 to the year 17\"\n",
    "layer = 9\n",
    "features = [5463]\n",
    "# sae = z_saes[layer]\n",
    "transcoder = transcoders[layer]\n",
    "result_string, cut_prompt = find_top_changes(prompt, model, layer, features, transcoder)\n",
    "print(cut_prompt)\n",
    "print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in list(set(cp.circuit_hypergraph[\"MLP9\"]['features'])) if x != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts = generate_greater_than_dataset(N=50)\n",
    "prompts = [x['text'].split('<|endoftext|>')[-1] + x['correct'] for x in dataset_prompts]\n",
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "from data.greater_than_dataset import (\n",
    "    generate_greater_than_dataset,\n",
    "    GT_GROUND_TRUTH_HEADS,\n",
    ")\n",
    "\n",
    "# Function to generate logit increase/decrease information\n",
    "def generate_logit_info(prompts: List[str], model, layer: int, features: List[int], sae) -> str:\n",
    "    all_results = []\n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            result_string, cut_prompt = find_top_changes(prompt, model, layer, features, sae, k=8, visualize=False)\n",
    "            # Surround the last word in cut prompt with << and >> for the logit info\n",
    "            cut_prompt = cut_prompt.split(\" \")\n",
    "            cut_prompt[-1] = f\"<<{cut_prompt[-1]}>>\"\n",
    "            cut_prompt = \" \".join(cut_prompt)\n",
    "            if result_string is not None:\n",
    "                all_results.append(f\"Prompt = '{cut_prompt}'\\n\\n\" + result_string)\n",
    "\n",
    "        except:\n",
    "            print(f\"Error for prompt {prompt}\")\n",
    "            continue\n",
    "            \n",
    "    all_results = all_results[:10]\n",
    "    return \"\\n\\n\\n\".join(all_results)\n",
    "\n",
    "# Function to generate mini examples for both general and IOI tasks\n",
    "def generate_mini_examples(component_type: str, features: List[int], layer: int, num_examples: int, dataset_prompt_tokens, strategy) -> Tuple[List[str], List[str]]:\n",
    "    mini_examples_owt_overall = []\n",
    "    mini_examples_ioi_overall = []\n",
    "\n",
    "    for feature in features:\n",
    "        try:\n",
    "            if component_type == \"attn\":\n",
    "                analyze_owt = MaxActAnalysis(\"attn\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy)\n",
    "            else:\n",
    "                analyze_owt = MaxActAnalysis(\"mlp\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy)\n",
    "            mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)\n",
    "            mini_examples_owt_overall.extend(mini_examples_owt)\n",
    "        except:\n",
    "            print(f\"Error for OWT tokens with feature {feature}\")\n",
    "\n",
    "        try: \n",
    "            if component_type == \"attn\":\n",
    "                analyze_prompts = MaxActAnalysis(\"attn\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy, token_dataset=dataset_prompt_tokens)\n",
    "            else:\n",
    "                analyze_prompts = MaxActAnalysis(\"mlp\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy, token_dataset=dataset_prompt_tokens)\n",
    "            \n",
    "            mini_examples_ioi = analyze_prompts.get_context_referenced_prompts_for_range(0, 5)\n",
    "            mini_examples_ioi_overall.extend(mini_examples_ioi)\n",
    "        except:\n",
    "            print(f\"Error for IOI tokens with feature {feature}\")\n",
    "\n",
    "    return mini_examples_owt_overall, mini_examples_ioi_overall\n",
    "\n",
    "# Function to interpret the neuron using the provided prompt\n",
    "def interpret_neuron(mini_examples_owt: List[str], mini_examples_ioi: List[str], incoming_information: Optional[List[Tuple[str, str]]], logit_info: str) -> str:\n",
    "    if len(incoming_information) > 0:\n",
    "        print(\"Using incoming information.\")\n",
    "        prompt = main_aug_interp_prompt_ioi_incoming(mini_examples_owt, mini_examples_ioi, incoming_information, [logit_info])\n",
    "    else:\n",
    "        print(\"Not using incoming information.\")\n",
    "        prompt = main_aug_interp_prompt_gt(mini_examples_owt, mini_examples_ioi, [logit_info])\n",
    "    return get_response(prompt), prompt\n",
    "\n",
    "# Function to get the top 5 (or less) occurring features\n",
    "def get_top_features(component: str, cp) -> List[int]:\n",
    "    features = [x for x in list(cp.circuit_hypergraph[component]['features']) if x != -1]\n",
    "    feature_counts = Counter(features)\n",
    "    top_features = [feature for feature, _ in feature_counts.most_common(10)]\n",
    "    return top_features\n",
    "\n",
    "# Higher-level function to process components based on the graph notation\n",
    "def process_components(graph: Dict[str, List[str]], model, z_saes, trancoders, strategy, num_examples: int = 2500, task='gt'):\n",
    "    # Initialize data structure to manage incoming information\n",
    "    component_interpretations = {}\n",
    "\n",
    "    num_examples = 2500\n",
    "\n",
    "    # Generate prompts\n",
    "    if task == 'ioi':\n",
    "        dataset_prompts = gen_templated_prompts(template_idex=1, N=250)\n",
    "        prompts = [x['text'] + x['correct'] for x in dataset_prompts]\n",
    "    elif task == 'gt':\n",
    "        dataset_prompts = generate_greater_than_dataset(N=250)\n",
    "        prompts = [x['text'].split('<|endoftext|>')[-1] for x in dataset_prompts]# + x['correct'] for x in dataset_prompts]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task: {task}\")\n",
    "\n",
    "    # Process each component\n",
    "    for component, next_components in graph.items():\n",
    "        component_type = \"attn\" if component[0]=='L' else \"mlp\"\n",
    "        if component_type == \"attn\":\n",
    "            layer, head = component.split(\"_\")\n",
    "            layer = int(layer[1:])\n",
    "            head = int(head[1:])\n",
    "            features = get_top_features(component, cp)\n",
    "            print(f\"Doing {component} with features {features} for layer {layer} head {head}\")\n",
    "        else:\n",
    "            layer = int(component[3:])\n",
    "            features = get_top_features(component, cp)\n",
    "            print(f\"Doing {component} with features {features} for layer {layer}\")\n",
    "\n",
    "        if component_type == \"attn\":\n",
    "            print(f\"Doing {component} with features {features} for layer {layer} head {head} with ZSAEs\")\n",
    "            sae = z_saes[layer]\n",
    "        else:\n",
    "            print(f\"Doing {component} with features {features} for layer {layer} with transcoders\")\n",
    "            sae = transcoders[layer]\n",
    "\n",
    "        # Generate logit increase/decrease information\n",
    "        logit_info = generate_logit_info(prompts, model, layer, features, sae)\n",
    "        print(logit_info)\n",
    "        print(f\"Successfully generated logit increase/decrease info for {component}.\")\n",
    "\n",
    "        # Generate mini examples\n",
    "        if task == 'ioi':\n",
    "            dataset_prompts_full = gen_templated_prompts(template_idex=1, N=250)\n",
    "            full_prompts = [x['text'] + x['correct'] for x in dataset_prompts_full]\n",
    "        elif task == 'gt':\n",
    "            dataset_prompts_full = generate_greater_than_dataset(N=250)\n",
    "            full_prompts = [x['text'] + x['correct'] for x in dataset_prompts_full]\n",
    "        tokens = model.to_tokens(full_prompts)\n",
    "        dataset_prompt_tokens = torch.tensor(tokens)\n",
    "\n",
    "        mini_examples_owt, mini_examples_ioi = generate_mini_examples(component_type, features, layer, num_examples, dataset_prompt_tokens, strategy)\n",
    "\n",
    "        # Prepare incoming information\n",
    "        incoming_information = [] #[(prev_component, component_interpretations[prev_component]) for prev_component in graph if component in graph[prev_component]]\n",
    "        print(f\"Incoming information for {component}:\\n{incoming_information}\\n\")\n",
    "\n",
    "        # Interpret the neuron\n",
    "        interpretation, p = interpret_neuron(mini_examples_owt, mini_examples_ioi, incoming_information, logit_info)\n",
    "        print(f\"Interpretation for {component}:\\n{interpretation}\\n\")\n",
    "        print(f\"Prompt for {component}:\\n{p}\\n\")\n",
    "        component_interpretations[component] = {\"interp\": interpretation, \"prompt\": p}\n",
    "\n",
    "        # Update incoming information for future components\n",
    "        for next_component in next_components:\n",
    "            if next_component not in graph:\n",
    "                graph[next_component] = []\n",
    "            graph[next_component].append(component)\n",
    "\n",
    "    return component_interpretations\n",
    "\n",
    "# Example graph notation\n",
    "graph_notation = {\n",
    "    # 'L0_H1',\n",
    "    # 'L0_H3',\n",
    "    # 'MLP0',\n",
    "    # 'L1_H0',\n",
    "    # 'L1_H10',\n",
    "    # 'MLP1',\n",
    "    # 'MLP2',\n",
    "    # 'MLP3',\n",
    "    # 'MLP4',\n",
    "    # 'L5_H5',\n",
    "    # 'L8_H7',\n",
    "    'MLP4': [],\n",
    "    # 'MLP9',\n",
    "    # 'MLP10',\n",
    "    # 'MLP11',\n",
    "}\n",
    "\n",
    "# # Example usage\n",
    "# model = ...  # Your model here\n",
    "# sae = z_saes[layer]\n",
    "model, z_saes, transcoders = get_model_encoders('cpu')\n",
    "strategy = create_simple_greedy_strategy(passes=1, node_contributors=1, minimal=True)\n",
    "component_interpretations_mlp1 = process_components(graph_notation, model, z_saes, transcoders, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(component_interpretations_mlp1['MLP4']['interp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(component_interpretations_mlp1['MLP9']['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.ioi_dataset import gen_templated_prompts\n",
    "from aug_interp_prompts import main_aug_interp_prompt, main_aug_interp_prompt_v2\n",
    "from openai_utils import gen_openai_completion, get_response\n",
    "from autointerpretability import *\n",
    "from discovery_strategies import (\n",
    "    create_filter,\n",
    "    create_simple_greedy_strategy,\n",
    "    create_top_contributor_strategy,\n",
    ")\n",
    "from max_act_analysis import MaxActAnalysis\n",
    "\n",
    "layer = 9\n",
    "feature = 12072#, 5463, 15687\n",
    "num_examples=1500\n",
    "strategy = create_simple_greedy_strategy(passes=1, node_contributors=1, minimal=True)\n",
    "analyze_owt = MaxActAnalysis(\"mlp\", layer, feature, num_sequences=num_examples, batch_size=128, strategy=strategy)\n",
    "mini_examples_owt = analyze_owt.get_context_referenced_prompts_for_range(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in component_interpretations_l9h9.items():\n",
    "    print(f\"Component: {k}\")\n",
    "    print(f\"Interpretation: {v['interp'].split('[EXPLANATION]: ')[-1].strip()}\")  \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
