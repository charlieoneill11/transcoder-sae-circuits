{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating combo SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max activating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity of SAE sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "import einops\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from gated_sae import GatedSAE\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def get_activation_store_and_model():\n",
    "\n",
    "    # let's start with a layer 8 SAE.\n",
    "    hook_point = \"blocks.8.hook_resid_pre\"\n",
    "\n",
    "    # if the SAEs were stored with precomputed feature sparsities,\n",
    "    #  those will be return in a dictionary as well.\n",
    "    saes, sparsities = get_gpt2_res_jb_saes(hook_point)\n",
    "\n",
    "    sparse_autoencoder = saes[hook_point]\n",
    "    device = 'cpu'\n",
    "    sparse_autoencoder.to(device)\n",
    "    sparse_autoencoder.cfg.device = device\n",
    "\n",
    "    sparse_autoencoder.cfg.hook_point = \"blocks.9.attn.hook_z\"\n",
    "    sparse_autoencoder.cfg.store_batch_size = 4\n",
    "\n",
    "    print(sparse_autoencoder.cfg.store_batch_size)\n",
    "\n",
    "    loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "\n",
    "    # don't overwrite the sparse autoencoder with the loader's sae (newly initialized)\n",
    "    model, _, activation_store = loader.load_sae_training_group_session()\n",
    "    \n",
    "    return model, activation_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "layer = 9\n",
    "repo_id = 'charlieoneill/regular-sae'\n",
    "\n",
    "\n",
    "# Load big SAE\n",
    "filename = f'sae_layer_{layer}_32.pt'\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "big_sae = GatedSAE(768, 32*768, l1_coefficient=2)\n",
    "big_sae.load_state_dict(torch.load(file_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Load little SAE\n",
    "filename = f'sae_layer_{layer}_16.pt'\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "little_sae = GatedSAE(768, 16*768, l1_coefficient=2)\n",
    "little_sae.load_state_dict(torch.load(file_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Load error SAE\n",
    "filename = f'sae_layer_{layer}.pt'\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "error_sae = GatedSAE(768, 16*768, l1_coefficient=2)\n",
    "error_sae.load_state_dict(torch.load(file_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, activation_store = get_activation_store_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = activation_store.get_batch_tokens()\n",
    "\n",
    "_, cache = model.run_with_cache(batch)\n",
    "\n",
    "z_acts = cache['z', layer, 'attn']\n",
    "print(z_acts.shape)\n",
    "z_acts = einops.rearrange(z_acts, 'b h l d -> (b h) (l d)')\n",
    "print(z_acts.shape)\n",
    "\n",
    "sae_out, _, mse_loss = big_sae(z_acts, z_acts)\n",
    "print(sae_out.shape)\n",
    "print(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sort of evaluation code\n",
    "\n",
    "# I think we should have individual metric functions, then a function to apply them over a batch, then a function to apply them over a dataset\n",
    "\n",
    "def mse_loss(x, y):\n",
    "    \"\"\"\n",
    "    L2 loss of reconstruction.\n",
    "    \"\"\"\n",
    "    per_item_loss = torch.nn.functional.mse_loss(x, y, reduction='none')\n",
    "    return per_item_loss.sum(dim=-1).mean()\n",
    "\n",
    "def l0_loss(z):\n",
    "    \"\"\"\n",
    "    L0 loss of reconstruction.\n",
    "    \"\"\"\n",
    "    return (z != 0).float().sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_kl_divergence(clean_logits, patched_logits):\n",
    "    # Ensure the inputs are log probabilities\n",
    "    clean_log_probs = F.log_softmax(clean_logits, dim=-1)\n",
    "    patched_log_probs = F.log_softmax(patched_logits, dim=-1)\n",
    "    \n",
    "    # Convert patched_logits to probabilities\n",
    "    patched_probs = torch.exp(patched_log_probs)\n",
    "    \n",
    "    # Calculate KL divergence for each element in the batch and sequence\n",
    "    kl_div = F.kl_div(clean_log_probs, patched_probs, reduction='none')\n",
    "    \n",
    "    # Average over the vocabulary size (last dimension)\n",
    "    kl_div = kl_div.sum(dim=-1)\n",
    "    \n",
    "    # Average over the batch and sequence length\n",
    "    kl_div = kl_div.mean(dim=0).mean(dim=0)\n",
    "    \n",
    "    return kl_div.item()\n",
    "\n",
    "def attention_head_z_patching_hook(attention_head_z, hook: HookPoint, layer: int, sae: GatedSAE, gated_sae: GatedSAE):\n",
    "    z_acts = einops.rearrange(attention_head_z, \"b s h d -> (b s) (h d)\")\n",
    "    if sae is not None:\n",
    "        # Get the reconstructions from the SAE\n",
    "        z_reconstruct, _, _ = sae(z_acts, z_acts)\n",
    "    else:\n",
    "        z_reconstruct = torch.zeros_like(z_acts)\n",
    "    if gated_sae is not None:\n",
    "        # Get the error\n",
    "        error = z_acts - z_reconstruct\n",
    "        # Get the predicted error\n",
    "        predicted_error, _, _ = gated_sae(z_acts, error)\n",
    "        # Add the predicted error to the z_reconstruct\n",
    "        z_reconstruct = z_reconstruct + predicted_error\n",
    "    # Rearrange back into original shape\n",
    "    z_reconstruct = einops.rearrange(z_reconstruct, \"(b s) (h d) -> b s h d\", b=attention_head_z.shape[0], s=attention_head_z.shape[1], h=attention_head_z.shape[2], d=attention_head_z.shape[3])\n",
    "    attention_head_z = z_reconstruct\n",
    "    return attention_head_z\n",
    "\n",
    "\n",
    "def kl_divergence_and_loss_difference(sae, gated_sae, batch, layer):\n",
    "    clean_logits, clean_loss = model(batch, return_type=\"both\")\n",
    "    hook_fn = partial(attention_head_z_patching_hook, layer=layer, sae=sae, gated_sae=gated_sae)\n",
    "    patched_logits, patched_loss = model.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[(get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "        return_type=\"both\"\n",
    "    )\n",
    "    return calculate_kl_divergence(clean_logits, patched_logits), patched_loss - clean_loss\n",
    "\n",
    "# Apply our metrics over a single batch\n",
    "kl_divergence, loss_difference = kl_divergence_and_loss_difference(big_sae, None, batch, layer)\n",
    "z_hat, _, _ = big_sae(z_acts, z_acts)\n",
    "z = big_sae.encoder(z_acts)\n",
    "mse = mse_loss(z_hat, z_acts)\n",
    "l0 = l0_loss(z)\n",
    "print(f\"KL Divergence: {kl_divergence}\")\n",
    "print(f\"MSE Loss: {mse}\")\n",
    "print(f\"L0 Loss: {l0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a function that gets the metrics for each model in a single batch\n",
    "\n",
    "def metrics_from_batch(big_sae, little_sae, error_sae, batch, layer):\n",
    "\n",
    "    # Get z_acts from batch\n",
    "    _, cache = model.run_with_cache(batch)\n",
    "    z_acts = cache['z', layer, 'attn']\n",
    "    z_acts = einops.rearrange(z_acts, 'b h l d -> (b h) (l d)')\n",
    "\n",
    "    # Just Big SAE\n",
    "    kl_divergence, loss_difference = kl_divergence_and_loss_difference(big_sae, None, batch, layer)\n",
    "    z_hat, _, _ = big_sae(z_acts, z_acts)\n",
    "    z = big_sae.encoder(z_acts)\n",
    "    mse = mse_loss(z_hat, z_acts)\n",
    "    l0 = l0_loss(z)\n",
    "\n",
    "    # Just little SAE\n",
    "    kl_divergence_little, loss_difference_little = kl_divergence_and_loss_difference(little_sae, None, batch, layer)\n",
    "    z_hat_little, _, _ = little_sae(z_acts, z_acts)\n",
    "    z_little = little_sae.encoder(z_acts)\n",
    "    mse_little = mse_loss(z_hat_little, z_acts)\n",
    "    l0_little = l0_loss(z_little)\n",
    "\n",
    "    # Combo SAE = Little SAE + Error SAE\n",
    "    kl_divergence_combo, loss_difference_combo = kl_divergence_and_loss_difference(little_sae, error_sae, batch, layer)\n",
    "    z_hat_combo, _, _ = little_sae(z_acts, z_acts)\n",
    "    predicted_error, _, _ = error_sae(z_acts, z_acts - z_hat_combo)\n",
    "    error_z = error_sae.encoder(z_acts)\n",
    "    z_hat_combo = z_hat_combo + predicted_error\n",
    "    z_combo = little_sae.encoder(z_acts) + error_z\n",
    "    mse_combo = mse_loss(z_hat_combo, z_acts)\n",
    "    l0_combo = l0_loss(z_combo)\n",
    "\n",
    "    # Create dict\n",
    "    batch_dict = {\n",
    "        \"big_sae\": {\n",
    "            \"kl_divergence\": kl_divergence,\n",
    "            \"loss_difference\": loss_difference.item(), # \"loss_difference\" is the difference in loss between the patched and clean models\n",
    "            \"mse\": mse.item(),\n",
    "            \"l0\": l0.item()\n",
    "        },\n",
    "        \"little_sae\": {\n",
    "            \"kl_divergence\": kl_divergence_little,\n",
    "            \"loss_difference\": loss_difference_little.item(),\n",
    "            \"mse\": mse_little.item(),\n",
    "            \"l0\": l0_little.item()\n",
    "        },\n",
    "        \"combo_sae\": {\n",
    "            \"kl_divergence\": kl_divergence_combo,\n",
    "            \"loss_difference\": loss_difference_combo.item(),\n",
    "            \"mse\": mse_combo.item(),\n",
    "            \"l0\": l0_combo.item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return batch_dict\n",
    "\n",
    "batch = activation_store.get_batch_tokens()\n",
    "\n",
    "batch_dict = metrics_from_batch(big_sae, little_sae, error_sae, batch, layer)\n",
    "\n",
    "# Print nicely\n",
    "for model_name, model_dict in batch_dict.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric_name, metric_value in model_dict.items():\n",
    "        print(f\"\\t{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Now a function to apply over n batches and return the average\n",
    "def metrics_from_batches(activation_store, big_sae, little_sae, error_sae, batch, layer, n_batches):\n",
    "    batch_dicts = []\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch = activation_store.get_batch_tokens()\n",
    "        batch_dict = metrics_from_batch(big_sae, little_sae, error_sae, batch, layer)\n",
    "        batch_dicts.append(batch_dict)\n",
    "\n",
    "    average_dict = {}\n",
    "    std_dict = {}\n",
    "    for model_name in batch_dicts[0].keys():\n",
    "        average_dict[model_name] = {}\n",
    "        std_dict[model_name] = {}\n",
    "        for metric_name in batch_dicts[0][model_name].keys():\n",
    "            metric_values = [batch_dict[model_name][metric_name] for batch_dict in batch_dicts]\n",
    "            average_dict[model_name][metric_name] = sum(metric_values) / n_batches\n",
    "            std_dict[model_name][metric_name] = np.std(metric_values) / 2\n",
    "\n",
    "            # If metric is kl_divergence, scale by 1000\n",
    "            if metric_name == \"kl_divergence\" or metric_name == \"loss_difference\":\n",
    "                average_dict[model_name][metric_name] *= 1000\n",
    "                std_dict[model_name][metric_name] *= 1000\n",
    "\n",
    "    return average_dict, std_dict\n",
    "\n",
    "average_dict, std_dict = metrics_from_batches(activation_store, big_sae, little_sae, error_sae, batch, layer, 50)\n",
    "\n",
    "# Print nicely\n",
    "for model_name, model_dict in average_dict.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric_name, metric_value in model_dict.items():\n",
    "        print(f\"\\t{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a grouped bar chat - groups are metrics, bars are models\n",
    "# Create traces for each model\n",
    "traces = []\n",
    "for model_name, model_dict in average_dict.items():\n",
    "    trace = go.Bar(\n",
    "        name=model_name,\n",
    "        x=list(model_dict.keys()),\n",
    "        y=list(model_dict.values()),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=[std_dict[model_name][metric_name] for metric_name in model_dict.keys()],\n",
    "            visible=True\n",
    "        )\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title=\"Big vs Combo metrics (same L0)\",\n",
    "    xaxis=dict(title=\"Metrics\"),\n",
    "    barmode=\"group\",\n",
    "    width=800\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto curve data (from HuggingFace)\n",
    "l0s = [10, 11, 12, 13, 14, 15, 16]\n",
    "l2s = [30.01, 29.02, 28.28, 27.99, 26.05, 25.80, 25.43]\n",
    "big_sae = (12.38, 28.24)\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(x=l0s, y=l2s, title=\"Combo SAE Pareto Curve (L0 vs MSE)\", width=800)\n",
    "\n",
    "# Add a red cross marker for big_sae\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[big_sae[0]],\n",
    "    y=[big_sae[1]],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color='red',\n",
    "        size=15,\n",
    "        symbol='cross'\n",
    "    ),\n",
    "    name='Big SAE'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='L0',\n",
    "    yaxis_title='MSE',\n",
    "    legend_title='Models',\n",
    "    font=dict(size=14)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training combo SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller regular SAE, hidden size 16_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller error SAE, hidden size 16_000\n",
    "layer = 9\n",
    "model_type = 'gated'\n",
    "n_epochs = 100\n",
    "l1_coefficient = 3e-4\n",
    "batch_size = 2048\n",
    "lr = 0.001\n",
    "projection_up = 16\n",
    "repo_name = \"error-saes\"\n",
    "\n",
    "error_sae = main(layer, model_type, n_epochs, l1_coefficient, projection_up, batch_size, lr, repo_name, return_model=True, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
