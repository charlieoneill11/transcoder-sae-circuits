{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating combo SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max activating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity of SAE sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Define parameters\n",
    "layer = 9\n",
    "repo_id = 'charlieoneill/regular-sae'\n",
    "filename = f'sae_layer_{layer}_32.pt'\n",
    "\n",
    "# Load from HuggingFace\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "big_sae = GatedSAE(768, 32*768, l1_coefficient=2)\n",
    "big_sae.load_state_dict(torch.load(file_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "/Users/charlesoneill/miniconda3/envs/anu/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/anu/lib/python3.12/site-packages/datasets/load.py:1486: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "# let's start with a layer 8 SAE.\n",
    "hook_point = \"blocks.8.hook_resid_pre\"\n",
    "\n",
    "# if the SAEs were stored with precomputed feature sparsities,\n",
    "#  those will be return in a dictionary as well.\n",
    "saes, sparsities = get_gpt2_res_jb_saes(hook_point)\n",
    "\n",
    "sparse_autoencoder = saes[hook_point]\n",
    "device = 'cpu'\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "\n",
    "sparse_autoencoder.cfg.hook_point = \"blocks.9.attn.hook_z\"\n",
    "sparse_autoencoder.cfg.store_batch_size = 4\n",
    "\n",
    "print(sparse_autoencoder.cfg.store_batch_size)\n",
    "\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "\n",
    "# don't overwrite the sparse autoencoder with the loader's sae (newly initialized)\n",
    "model, _, activation_store = loader.load_sae_training_group_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 12, 64])\n",
      "torch.Size([512, 768])\n",
      "torch.Size([512, 768])\n",
      "tensor(30.9679, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "batch = activation_store.get_batch_tokens()\n",
    "\n",
    "_, cache = model.run_with_cache(batch)\n",
    "\n",
    "z_acts = cache['z', layer, 'attn']\n",
    "print(z_acts.shape)\n",
    "z_acts = einops.rearrange(z_acts, 'b h l d -> (b h) (l d)')\n",
    "print(z_acts.shape)\n",
    "\n",
    "sae_out, _, mse_loss = big_sae(z_acts, z_acts)\n",
    "print(sae_out.shape)\n",
    "print(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sort of evaluation code\n",
    "\n",
    "# I think we should have individual metric functions, then a function to apply them over a batch, then a function to apply them over a dataset\n",
    "\n",
    "def mse_loss(x, y):\n",
    "    \"\"\"\n",
    "    L2 loss of reconstruction.\n",
    "    \"\"\"\n",
    "    per_item_loss = torch.nn.functional.mse_loss(x, y, reduction='none')\n",
    "    return per_item_loss.sum(dim=-1).mean()\n",
    "\n",
    "def l0_loss(z):\n",
    "    \"\"\"\n",
    "    L0 loss of reconstruction.\n",
    "    \"\"\"\n",
    "    return (z != 0).float().sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.033432040363550186\n",
      "MSE Loss: 30.967899322509766\n",
      "L0 Loss: 12.9921875\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_kl_divergence(clean_logits, patched_logits):\n",
    "    # Ensure the inputs are log probabilities\n",
    "    clean_log_probs = F.log_softmax(clean_logits, dim=-1)\n",
    "    patched_log_probs = F.log_softmax(patched_logits, dim=-1)\n",
    "    \n",
    "    # Convert patched_logits to probabilities\n",
    "    patched_probs = torch.exp(patched_log_probs)\n",
    "    \n",
    "    # Calculate KL divergence for each element in the batch and sequence\n",
    "    kl_div = F.kl_div(clean_log_probs, patched_probs, reduction='none')\n",
    "    \n",
    "    # Average over the vocabulary size (last dimension)\n",
    "    kl_div = kl_div.sum(dim=-1)\n",
    "    \n",
    "    # Average over the batch and sequence length\n",
    "    kl_div = kl_div.mean(dim=0).mean(dim=0)\n",
    "    \n",
    "    return kl_div.item()\n",
    "\n",
    "def attention_head_z_patching_hook(attention_head_z, hook: HookPoint, layer: int, sae: GatedSAE, gated_sae: GatedSAE):\n",
    "    z_acts = einops.rearrange(attention_head_z, \"b s h d -> (b s) (h d)\")\n",
    "    if sae is not None:\n",
    "        # Get the reconstructions from the SAE\n",
    "        z_reconstruct, _, _ = sae(z_acts, z_acts)\n",
    "    else:\n",
    "        z_reconstruct = torch.zeros_like(z_acts)\n",
    "    if gated_sae is not None:\n",
    "        # Get the error\n",
    "        error = z_acts - z_reconstruct\n",
    "        # Get the predicted error\n",
    "        predicted_error, _, _ = gated_sae(z_acts, error)\n",
    "        # Add the predicted error to the z_reconstruct\n",
    "        z_reconstruct = z_reconstruct + predicted_error\n",
    "    # Rearrange back into original shape\n",
    "    z_reconstruct = einops.rearrange(z_reconstruct, \"(b s) (h d) -> b s h d\", b=attention_head_z.shape[0], s=attention_head_z.shape[1], h=attention_head_z.shape[2], d=attention_head_z.shape[3])\n",
    "    attention_head_z = z_reconstruct\n",
    "    return attention_head_z\n",
    "\n",
    "\n",
    "def kl_divergence_and_loss_difference(sae, gated_sae, batch, layer):\n",
    "    clean_logits, clean_loss = model(batch, return_type=\"both\")\n",
    "    hook_fn = partial(attention_head_z_patching_hook, layer=layer, sae=sae, gated_sae=gated_sae)\n",
    "    patched_logits, patched_loss = model.run_with_hooks(\n",
    "        batch,\n",
    "        fwd_hooks=[(get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "        return_type=\"both\"\n",
    "    )\n",
    "    return calculate_kl_divergence(clean_logits, patched_logits), patched_loss - clean_loss\n",
    "\n",
    "# Apply our metrics over a single batch\n",
    "kl_divergence, loss_difference = kl_divergence_and_loss_difference(big_sae, None, batch, layer)\n",
    "z_hat, _, _ = big_sae(z_acts, z_acts)\n",
    "z = big_sae.encoder(z_acts)\n",
    "mse = mse_loss(z_hat, z_acts)\n",
    "l0 = l0_loss(z)\n",
    "print(f\"KL Divergence: {kl_divergence}\")\n",
    "print(f\"MSE Loss: {mse}\")\n",
    "print(f\"L0 Loss: {l0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training combo SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller regular SAE, hidden size 16_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller error SAE, hidden size 16_000\n",
    "layer = 9\n",
    "model_type = 'gated'\n",
    "n_epochs = 100\n",
    "l1_coefficient = 3e-4\n",
    "batch_size = 2048\n",
    "lr = 0.001\n",
    "projection_up = 16\n",
    "repo_name = \"error-saes\"\n",
    "\n",
    "error_sae = main(layer, model_type, n_epochs, l1_coefficient, projection_up, batch_size, lr, repo_name, return_model=True, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
